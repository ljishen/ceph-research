

[1;7;39;49m[2021-05-15T16:00:15,122763877-07:00][RUNNING][ROUND 1/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:00:15,127716503-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T16:00:15,143468164-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40233\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.51819\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b92b0e27-906a-4910-b34c-5fda33080a30\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid b92b0e27-906a-4910-b34c-5fda33080a30\nlast_changed 2021-05-15T16:00:24.839457-0700\ncreated 2021-05-15T16:00:24.839457-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40233/0,v1:10.10.1.2:40234/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.51819 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 a14713a5-78c5-4070-9d3d-76d4aa5fccea\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 5055ee63-9a0b-4e2e-858c-cdbb5f37378a\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 61519e98-865c-47db-93cd-a40fb17cdd66\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42233\n  w/ user/pass: admin / 3817637c-c73a-43fc-a8da-f7fa1a8ee4b2\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:00:43 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40233
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.51819
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b92b0e27-906a-4910-b34c-5fda33080a30
setting min_mon_release = octopus
epoch 0
fsid b92b0e27-906a-4910-b34c-5fda33080a30
last_changed 2021-05-15T16:00:24.839457-0700
created 2021-05-15T16:00:24.839457-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40233/0,v1:10.10.1.2:40234/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.51819 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 a14713a5-78c5-4070-9d3d-76d4aa5fccea
0
start osd.0
add osd1 5055ee63-9a0b-4e2e-858c-cdbb5f37378a
1
start osd.1
add osd2 61519e98-865c-47db-93cd-a40fb17cdd66
2
start osd.2


restful urls: https://10.10.1.2:42233
  w/ user/pass: admin / 3817637c-c73a-43fc-a8da-f7fa1a8ee4b2


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T16:00:17.079-0700 7fad57e1b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:00:17.079-0700 7fad57e1b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:00:17.095-0700 7fe5e22971c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:00:17.095-0700 7fe5e22971c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40233,v1:10.10.1.2:40234] --print /tmp/ceph_monmap.51819 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.51819 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.51819 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42233 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5644a0012000 @  0x7f7fb68c3680 0x7f7fb68e4824 0x7f7fb707f187 0x7f7fb7087355 0x7f7fb707f708 0x7f7fb707f877 0x7f7fb7080c24 0x7f7fb7098ec1 0x7f7fb700b5f3 0x7f7fb706ce97 0x7f7fb7074b1a 0x7f7fb6777d84 0x7f7fb6893609 0x7f7fb6467293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.Z5BRdwRRBV 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a14713a5-78c5-4070-9d3d-76d4aa5fccea -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCTUqBgDsbjDBAA0WY53iJ4ps+wnjhHWaFiVw== --osd-uuid a14713a5-78c5-4070-9d3d-76d4aa5fccea 
2021-05-15T16:00:35.843-0700 7fb53815ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:00:35.863-0700 7fb53815ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:00:35.863-0700 7fb53815ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x558f7e544000 @  0x7fb538b27680 0x7fb538b48824 0x558f739e7447 0x558f739ef4b5 0x558f739e79c8 0x558f739e7b37 0x558f739e8ee4 0x558f737b9ca1 0x558f73991423 0x558f737aa2a7 0x558f737af53a 0x7fb53867ad84 0x7fb5387ff609 0x7fb538368293
2021-05-15T16:00:36.171-0700 7fb53815ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5055ee63-9a0b-4e2e-858c-cdbb5f37378a -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x56384299c000 @  0x7f536216f680 0x7f5362190824 0x563837d14447 0x563837d1c4b5 0x563837d149c8 0x563837d14b37 0x563837d15ee4 0x563837ae6ca1 0x563837cbe423 0x563837ad72a7 0x563837adc53a 0x7f5361cc2d84 0x7f5361e47609 0x7f53619b0293
2021-05-15T16:00:36.783-0700 7f53617a6f00 -1 Falling back to public interface
2021-05-15T16:00:37.035-0700 7f53617a6f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCUUqBgA1m+HBAAEyKrELpOSLn11ktsFQhXZA== --osd-uuid 5055ee63-9a0b-4e2e-858c-cdbb5f37378a 
2021-05-15T16:00:37.159-0700 7f5c6d0f0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:00:37.183-0700 7f5c6d0f0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:00:37.183-0700 7f5c6d0f0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56376b83e000 @  0x7f5c6dab9680 0x7f5c6dada824 0x563761079447 0x5637610814b5 0x5637610799c8 0x563761079b37 0x56376107aee4 0x563760e4bca1 0x563761023423 0x563760e3c2a7 0x563760e4153a 0x7f5c6d60cd84 0x7f5c6d791609 0x7f5c6d2fa293
2021-05-15T16:00:37.491-0700 7f5c6d0f0f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 61519e98-865c-47db-93cd-a40fb17cdd66 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5652a9be0000 @  0x7f43a0bb1680 0x7f43a0bd2824 0x56529ecf4447 0x56529ecfc4b5 0x56529ecf49c8 0x56529ecf4b37 0x56529ecf5ee4 0x56529eac6ca1 0x56529ec9e423 0x56529eab72a7 0x56529eabc53a 0x7f43a0704d84 0x7f43a0889609 0x7f43a03f2293
2021-05-15T16:00:38.183-0700 7f43a01e8f00 -1 Falling back to public interface
2021-05-15T16:00:38.431-0700 7f43a01e8f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCVUqBgLm/0MxAAiwxWD9pXATMA82ksYEqRGw== --osd-uuid 61519e98-865c-47db-93cd-a40fb17cdd66 
2021-05-15T16:00:38.543-0700 7f4db8722f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:00:38.563-0700 7f4db8722f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:00:38.563-0700 7f4db8722f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x560097538000 @  0x7f4db90eb680 0x7f4db910c824 0x56008b927447 0x56008b92f4b5 0x56008b9279c8 0x56008b927b37 0x56008b928ee4 0x56008b6f9ca1 0x56008b8d1423 0x56008b6ea2a7 0x56008b6ef53a 0x7f4db8c3ed84 0x7f4db8dc3609 0x7f4db892c293
2021-05-15T16:00:38.859-0700 7f4db8722f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5592083aa000 @  0x7fdc491d1680 0x7fdc491f2824 0x5591fd1a0447 0x5591fd1a84b5 0x5591fd1a09c8 0x5591fd1a0b37 0x5591fd1a1ee4 0x5591fcf72ca1 0x5591fd14a423 0x5591fcf632a7 0x5591fcf6853a 0x7fdc48d24d84 0x7fdc48ea9609 0x7fdc48a12293
2021-05-15T16:00:39.499-0700 7fdc48808f00 -1 Falling back to public interface
2021-05-15T16:00:39.767-0700 7fdc48808f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T16:00:43,870837535-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T16:00:43,887440779-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T16:00:43,968610647-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:00:43,974672087-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T16:00:47,878417761-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:00:47,884792545-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T16:00:51,798287045-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:00:51,804827673-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T16:00:55,663155486-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:00:55,669606477-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T16:01:03,446342182-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:01:03,452736345-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T16:01:07,854720345-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:01:07,861168574-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T16:01:12,698885472-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:01:12,706762356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T16:01:17,103473847-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:01:17,109943302-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:01:21,562341842-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:01:21,568777603-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:01:25,815756457-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:01:25,821989977-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T16:01:30,544208484-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:01:30,550729244-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T16:01:34,289669658-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:01:34,295852876-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.08   90      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   89      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   80      up          osd.2  
                       TOTAL  300 GiB  173 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.08  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T16:01:38,020207748-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:02:01,981208601-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:02:10,966476620-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:02:20,400389073-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:02:29,465116406-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:02:38,498888213-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:02:38,510449951-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:02:47,466557290-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:02:47,477528960-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:02:56,820971127-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:02:56,832193626-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:03:05,875319100-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:03:05,886660877-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:03:14,952069269-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:03:14,963688248-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:03:14,972097994-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:03:14,976943369-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:03:14,988737869-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=477151
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T16:03:14,998644706-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-15T16:03:15,037628161-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:03:15,044064839-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:03:16.517+0000 ffffba161010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:03:17.309+0000 ffffba161010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:03:17.309+0000 ffffba161010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:03:17.332489+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-15T23:03:17.332544+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-15T23:03:17.334882+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:03:17.334882+0000     0       0         0         0         0         0           -           0
2021-05-15T23:03:18.335041+0000     1     255     19017     18762   73.2843   73.2891  0.00137814   0.0128996
2021-05-15T23:03:19.335204+0000     2     255     37303     37048   72.3511   71.4297  0.00179059   0.0134292
2021-05-15T23:03:20.335357+0000     3     255     55653     55398   72.1237   71.6797  0.00197736   0.0136118
2021-05-15T23:03:21.335512+0000     4     255     73783     73528   71.7951   70.8203  0.00167162   0.0137398
2021-05-15T23:03:22.335666+0000     5     255     92274     92019   71.8799   72.2305 0.000998836   0.0137505
2021-05-15T23:03:23.335820+0000     6     255    110446    110191   71.7288   70.9844  0.00125846   0.0138129
2021-05-15T23:03:24.335989+0000     7     255    128287    128032   71.4361   69.6914  0.00376605   0.0139304
2021-05-15T23:03:25.336158+0000     8     255    146825    146570   71.5568   72.4141   0.0012373   0.0138681
2021-05-15T23:03:26.336314+0000     9     256    164381    164125   71.2242   68.5742  0.00118931   0.0139836
2021-05-15T23:03:27.336501+0000    10     255    180730    180475   70.4873   63.8672   0.0895953   0.0141267
2021-05-15T23:03:28.336659+0000    11     256    195989    195733   69.4968   59.6016  0.00360664   0.0143298
2021-05-15T23:03:29.336807+0000    12     255    212850    212595   69.1936   65.8672  0.00143506   0.0143776
2021-05-15T23:03:30.336968+0000    13     255    229662    229407   68.9218   65.6719   0.0119827   0.0144927
2021-05-15T23:03:31.337121+0000    14     255    248161    247906   69.1596   72.2617  0.00567058   0.0144078
2021-05-15T23:03:32.337282+0000    15     256    265145    264889   68.9709   66.3398   0.0011062   0.0144136
2021-05-15T23:03:33.337434+0000    16     255    281098    280843   68.5547   62.3203   0.0015321   0.0145343
2021-05-15T23:03:34.337600+0000    17     255    296983    296728   68.1715   62.0508  0.00201599   0.0146396
2021-05-15T23:03:35.337855+0000    18     255    313588    313333   67.9867   64.8633   0.0893322   0.0146738
2021-05-15T23:03:36.338007+0000    19     255    327644    327389   67.2979   54.9062  0.00702374   0.0148298
2021-05-15T23:03:37.338156+0000 min lat: 0.000585791 max lat: 0.213732 avg lat: 0.0148139
2021-05-15T23:03:37.338156+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:03:37.338156+0000    20     214    344932    344718    67.317   67.6914    0.107599   0.0148139
2021-05-15T23:03:38.338411+0000 Total time run:         20.0821
Total writes made:      344932
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     67.0942
Stddev Bandwidth:       4.9694
Max bandwidth (MB/sec): 73.2891
Min bandwidth (MB/sec): 54.9062
Average IOPS:           17176
Stddev IOPS:            1272.17
Max IOPS:               18762
Min IOPS:               14056
Average Latency(s):     0.0148616
Stddev Latency(s):      0.0334303
Max latency(s):         0.213732
Min latency(s):         0.000585791

[1;32mlocalhost.localdomain	[2021-05-15T16:03:38,922878646-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 477151


[1;33mlocalhost.localdomain	[2021-05-15T16:03:38,928518411-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:04:02,952014843-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 344.93k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:04:02,963677770-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:04:11,995327900-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 344.93k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:04:12,007173142-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:04:21,270143388-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 344.93k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:04:21,283563788-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:04:30,373872615-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 344.93k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:04:30,385844708-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:04:39,723186409-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 344.93k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:04:39,734953304-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:04:39,743476336-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:04:39,748547501-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:04:39,759945895-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=480697
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T16:04:39,771321602-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-15T16:04:39,809524149-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:04:39,815795168-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c47d2e68-65ab-4449-ab0b-33c4bd9822fa', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c47d2e68-65ab-4449-ab0b-33c4bd9822fa --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pTcelg:/tmp/ceph-asok.pTcelg -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:04:41.283+0000 ffffb7ad7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:04:42.415+0000 ffffb7ad7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:04:42.419+0000 ffffb7ad7010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:04:42.442336+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:04:42.442336+0000     0       0         0         0         0         0           -           0
2021-05-15T23:04:43.442485+0000     1     255     30940     30685   119.829   119.863  0.00765096  0.00828971
2021-05-15T23:04:44.442745+0000     2     255     61259     61004   119.116   118.434  0.00734627   0.0083646
2021-05-15T23:04:45.442926+0000     3     256     93779     93523   121.745   127.027  0.00532743  0.00805732
2021-05-15T23:04:46.443084+0000     4     255    124858    124603   121.656   121.406  0.00757641  0.00820171
2021-05-15T23:04:47.443249+0000     5     255    160458    160203   125.132   139.062  0.00673122  0.00797635
2021-05-15T23:04:48.443402+0000     6     255    188090    187835   122.264   107.938  0.00706665  0.00816485
2021-05-15T23:04:49.443585+0000     7     255    218089    217834   121.535   117.184  0.00780546  0.00821471
2021-05-15T23:04:50.443752+0000     8     255    245899    245644    119.92   108.633  0.00796032  0.00832613
2021-05-15T23:04:51.443917+0000     9     255    278999    278744    120.96   129.297  0.00712438  0.00825584
2021-05-15T23:04:52.444105+0000    10     255    313334    313079   122.273   134.121  0.00816334  0.00816724
2021-05-15T23:04:53.444403+0000 Total time run:       10.8549
Total reads made:     344932
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   124.127
Average IOPS:         31776
Stddev IOPS:          2606.46
Max IOPS:             35600
Min IOPS:             27632
Average Latency(s):   0.00804776
Max latency(s):       0.107794
Min latency(s):       0.000264407

[1;32mlocalhost.localdomain	[2021-05-15T16:04:54,066061430-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 480697


[1;33mlocalhost.localdomain	[2021-05-15T16:04:54,073135825-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:05:18,116856082-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 344.93k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:05:18,130663710-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:05:27,270584152-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 344.93k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:05:27,284752769-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:05:36,313523426-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 344.93k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:05:36,327454688-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:05:45,381756694-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 344.93k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:05:45,396487906-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:05:54,595095823-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 344.93k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:05:54,609343057-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:05:54,620173013-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T16:05:54,624081224-07:00][RUNNING][ROUND 2/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:05:54,630260322-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T16:05:54,646375377-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40189\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.54438\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 20828e9f-f4ab-4c6c-96a6-8ee665acbf4b\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 20828e9f-f4ab-4c6c-96a6-8ee665acbf4b\nlast_changed 2021-05-15T16:06:20.636039-0700\ncreated 2021-05-15T16:06:20.636039-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40189/0,v1:10.10.1.2:40190/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.54438 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 766c9bc3-2739-444d-b15b-961aca6e11f9\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 5d61340e-329c-4b19-9d44-a80d0acda95f\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 d2fe8c29-4003-4d11-8d53-36e36b2373b3\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42189\n  w/ user/pass: admin / 0551a4e5-5940-40fb-b9b5-f960739c0092\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:06:41 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40189
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.54438
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 20828e9f-f4ab-4c6c-96a6-8ee665acbf4b
setting min_mon_release = octopus
epoch 0
fsid 20828e9f-f4ab-4c6c-96a6-8ee665acbf4b
last_changed 2021-05-15T16:06:20.636039-0700
created 2021-05-15T16:06:20.636039-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40189/0,v1:10.10.1.2:40190/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.54438 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 766c9bc3-2739-444d-b15b-961aca6e11f9
0
start osd.0
add osd1 5d61340e-329c-4b19-9d44-a80d0acda95f
1
start osd.1
add osd2 d2fe8c29-4003-4d11-8d53-36e36b2373b3
2
start osd.2


restful urls: https://10.10.1.2:42189
  w/ user/pass: admin / 0551a4e5-5940-40fb-b9b5-f960739c0092


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T16:05:56.578-0700 7fcecdeb21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:05:56.578-0700 7fcecdeb21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:05:56.598-0700 7f2d760a91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:05:56.598-0700 7f2d760a91c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40189,v1:10.10.1.2:40190] --print /tmp/ceph_monmap.54438 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.54438 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.54438 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42189 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55b99688a000 @  0x7f7129388680 0x7f71293a9824 0x7f7129b44187 0x7f7129b4c355 0x7f7129b44708 0x7f7129b44877 0x7f7129b45c24 0x7f7129b5dec1 0x7f7129ad05f3 0x7f7129b31e97 0x7f7129b39b1a 0x7f712923cd84 0x7f7129358609 0x7f7128f2c293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.KcNXVGZLtT 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 766c9bc3-2739-444d-b15b-961aca6e11f9 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD4U6Bgvxt5IBAAmcx5keFPLqhuTMCGp5altg== --osd-uuid 766c9bc3-2739-444d-b15b-961aca6e11f9 
2021-05-15T16:06:33.178-0700 7fd93cd13f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:06:33.198-0700 7fd93cd13f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:06:33.198-0700 7fd93cd13f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56450be76000 @  0x7fd93d6dc680 0x7fd93d6fd824 0x564500d99447 0x564500da14b5 0x564500d999c8 0x564500d99b37 0x564500d9aee4 0x564500b6bca1 0x564500d43423 0x564500b5c2a7 0x564500b6153a 0x7fd93d22fd84 0x7fd93d3b4609 0x7fd93cf1d293
2021-05-15T16:06:33.510-0700 7fd93cd13f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5d61340e-329c-4b19-9d44-a80d0acda95f -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5644696d4000 @  0x7f98827ea680 0x7f988280b824 0x56445d99e447 0x56445d9a64b5 0x56445d99e9c8 0x56445d99eb37 0x56445d99fee4 0x56445d770ca1 0x56445d948423 0x56445d7612a7 0x56445d76653a 0x7f988233dd84 0x7f98824c2609 0x7f988202b293
2021-05-15T16:06:34.126-0700 7f9881e21f00 -1 Falling back to public interface
2021-05-15T16:06:34.382-0700 7f9881e21f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD5U6Bg0CNLMRAAg/uReyyS8LgaqfoxlzUNDw== --osd-uuid 5d61340e-329c-4b19-9d44-a80d0acda95f 
2021-05-15T16:06:34.510-0700 7f2174bb2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:06:34.530-0700 7f2174bb2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:06:34.530-0700 7f2174bb2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x565219fee000 @  0x7f217557b680 0x7f217559c824 0x56520efaa447 0x56520efb24b5 0x56520efaa9c8 0x56520efaab37 0x56520efabee4 0x56520ed7cca1 0x56520ef54423 0x56520ed6d2a7 0x56520ed7253a 0x7f21750ced84 0x7f2175253609 0x7f2174dbc293
2021-05-15T16:06:34.854-0700 7f2174bb2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d2fe8c29-4003-4d11-8d53-36e36b2373b3 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x558cef4d8000 @  0x7fd40aed3680 0x7fd40aef4824 0x558ce4eeb447 0x558ce4ef34b5 0x558ce4eeb9c8 0x558ce4eebb37 0x558ce4eecee4 0x558ce4cbdca1 0x558ce4e95423 0x558ce4cae2a7 0x558ce4cb353a 0x7fd40aa26d84 0x7fd40abab609 0x7fd40a714293
2021-05-15T16:06:35.538-0700 7fd40a50af00 -1 Falling back to public interface
2021-05-15T16:06:35.798-0700 7fd40a50af00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD7U6BgdNGJDBAAojReLPzC7Jhz70TjOwo9RQ== --osd-uuid d2fe8c29-4003-4d11-8d53-36e36b2373b3 
2021-05-15T16:06:35.902-0700 7f2664db5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:06:35.922-0700 7f2664db5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:06:35.922-0700 7f2664db5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x560125ae0000 @  0x7f266577e680 0x7f266579f824 0x56011b03c447 0x56011b0444b5 0x56011b03c9c8 0x56011b03cb37 0x56011b03dee4 0x56011ae0eca1 0x56011afe6423 0x56011adff2a7 0x56011ae0453a 0x7f26652d1d84 0x7f2665456609 0x7f2664fbf293
2021-05-15T16:06:36.230-0700 7f2664db5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x564cfd99c000 @  0x7f20ad0d7680 0x7f20ad0f8824 0x564cf2eb1447 0x564cf2eb94b5 0x564cf2eb19c8 0x564cf2eb1b37 0x564cf2eb2ee4 0x564cf2c83ca1 0x564cf2e5b423 0x564cf2c742a7 0x564cf2c7953a 0x7f20acc2ad84 0x7f20acdaf609 0x7f20ac918293
2021-05-15T16:06:36.914-0700 7f20ac70ef00 -1 Falling back to public interface
2021-05-15T16:06:37.186-0700 7f20ac70ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T16:06:41,214558468-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T16:06:41,233370706-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T16:06:41,314057047-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:06:41,320392160-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T16:06:45,246769151-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:06:45,252955471-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T16:06:49,271728065-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:06:49,278399034-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T16:06:53,255566569-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:06:53,262139834-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T16:07:00,970822662-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:07:00,977152900-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T16:07:04,990642250-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:07:04,997158121-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T16:07:09,758280650-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:07:09,765539124-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T16:07:13,696651140-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:07:13,702991543-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:07:18,366988682-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:07:18,372994350-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:07:22,828608412-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:07:22,835018843-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T16:07:27,471317559-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:07:27,477957475-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T16:07:31,172139594-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:07:31,178411726-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.94   89      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  1.01   93      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.06   77      up          osd.2  
                       TOTAL  300 GiB  177 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.94/1.06  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T16:07:34,811847711-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:07:58,769956427-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:08:07,876259753-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 51s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:08:17,114891837-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (30s)
      [========....................] (remaining: 72s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:08:26,190604707-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 92s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:08:35,183774153-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   peering
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 44s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:08:44,254094055-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:08:44,268000727-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:08:53,364312610-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:08:53,378360394-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:09:02,333130308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:09:02,347140141-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:09:11,470360576-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:09:11,484337276-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:09:20,600411205-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:09:20,616280856-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:09:20,629536121-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:09:20,637742804-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:09:20,656552935-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=494126
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T16:09:20,669916974-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-15T16:09:20,715189175-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:09:20,722201182-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:09:22.207+0000 ffffbc71d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:09:23.007+0000 ffffbc71d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:09:23.007+0000 ffffbc71d010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:09:23.028243+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-15T23:09:23.028299+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-15T23:09:23.030624+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:09:23.030624+0000     0       0         0         0         0         0           -           0
2021-05-15T23:09:24.030781+0000     1     255     19894     19639   76.6974   76.7148  0.00366764   0.0124932
2021-05-15T23:09:25.030941+0000     2     255     39470     39215    76.577   76.4688  0.00127527   0.0126881
2021-05-15T23:09:26.031098+0000     3     255     59533     59278   77.1709   78.3711  0.00230391   0.0127584
2021-05-15T23:09:27.031247+0000     4     255     79543     79288   77.4163   78.1641  0.00144158   0.0127428
2021-05-15T23:09:28.031401+0000     5     255     98852     98597   77.0159   75.4258  0.00259773   0.0128606
2021-05-15T23:09:29.031560+0000     6     255    118968    118713   77.2742   78.5781  0.00171498   0.0128313
2021-05-15T23:09:30.031714+0000     7     256    138555    138299    77.163   76.5078  0.00142718   0.0128477
2021-05-15T23:09:31.031873+0000     8     255    157499    157244   76.7666   74.0039   0.0468201   0.0129961
2021-05-15T23:09:32.032030+0000     9     255    177462    177207   76.9002   77.9805  0.00111924   0.0129412
2021-05-15T23:09:33.032183+0000    10     255    196925    196670   76.8117   76.0273  0.00297519   0.0129725
2021-05-15T23:09:34.032335+0000    11     255    216191    215936   76.6694   75.2578  0.00404911   0.0130003
2021-05-15T23:09:35.032492+0000    12     255    235492    235237   76.5622   75.3945   0.0118578   0.0130079
2021-05-15T23:09:36.032652+0000    13     255    254914    254659   76.5078   75.8672     0.10572   0.0130227
2021-05-15T23:09:37.032827+0000    14     255    274340    274085   76.4622   75.8828  0.00477879   0.0130442
2021-05-15T23:09:38.032998+0000    15     255    292956    292701   76.2118   72.7188  0.00196085   0.0130736
2021-05-15T23:09:39.033150+0000    16     255    311739    311484   76.0336   73.3711  0.00148455   0.0131035
2021-05-15T23:09:40.033308+0000    17     255    330739    330484   75.9261   74.2188  0.00264034   0.0131247
2021-05-15T23:09:41.033476+0000    18     255    347604    347349   75.3673   65.8789  0.00530068   0.0132372
2021-05-15T23:09:42.033632+0000    19     255    364442    364187   74.8618   65.7734  0.00262027   0.0133376
2021-05-15T23:09:43.033794+0000 min lat: 0.000630506 max lat: 0.168882 avg lat: 0.0133562
2021-05-15T23:09:43.033794+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:09:43.033794+0000    20     208    382482    382274   74.6508   70.6523    0.123328   0.0133562
2021-05-15T23:09:44.034022+0000 Total time run:         20.079
Total writes made:      382482
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     74.4096
Stddev Bandwidth:       3.61165
Max bandwidth (MB/sec): 78.5781
Min bandwidth (MB/sec): 65.7734
Average IOPS:           19048
Stddev IOPS:            924.583
Max IOPS:               20116
Min IOPS:               16838
Average Latency(s):     0.0134013
Stddev Latency(s):      0.0290916
Max latency(s):         0.168882
Min latency(s):         0.000630506

[1;32mlocalhost.localdomain	[2021-05-15T16:09:44,673863465-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 494126


[1;33mlocalhost.localdomain	[2021-05-15T16:09:44,681355979-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:10:08,713989043-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 382.48k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:10:08,728282646-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:10:18,094467068-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 382.48k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:10:18,109961538-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:10:27,023661109-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 382.48k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:10:27,038205787-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:10:36,126385504-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 382.48k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:10:36,140917972-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:10:45,484312036-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 382.48k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:10:45,498458693-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:10:45,509923188-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:10:45,516671021-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:10:45,531396491-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=497643
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T16:10:45,543456827-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-15T16:10:45,581811879-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:10:45,588238694-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d642fa7-d15e-48fa-8019-a0e676e6c099', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d642fa7-d15e-48fa-8019-a0e676e6c099 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FgMjlp:/tmp/ceph-asok.FgMjlp -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:10:47.044+0000 ffff7fd1a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:10:47.832+0000 ffff7fd1a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:10:47.836+0000 ffff7fd1a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:10:47.858620+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:10:47.858620+0000     0      22        22         0         0         0           -           0
2021-05-15T23:10:48.858768+0000     1     255     34025     33770   131.803   131.914  0.00357482  0.00752881
2021-05-15T23:10:49.858944+0000     2     255     64204     63949   124.837   117.887  0.00964052   0.0079741
2021-05-15T23:10:50.859119+0000     3     255     90371     90116   117.292   102.215  0.00751362  0.00850088
2021-05-15T23:10:51.859300+0000     4     255    121853    121598   118.707   122.977   0.0156071  0.00839966
2021-05-15T23:10:52.859475+0000     5     255    152503    152248   118.907   119.727  0.00718202  0.00839297
2021-05-15T23:10:53.859649+0000     6     255    183392    183137   119.196    120.66  0.00669105  0.00837484
2021-05-15T23:10:54.859818+0000     7     255    216385    216130   120.576   128.879  0.00752212  0.00827971
2021-05-15T23:10:55.859999+0000     8     255    249292    249037   121.569   128.543   0.0037347  0.00821141
2021-05-15T23:10:56.860163+0000     9     256    281290    281034   121.946   124.988  0.00344099  0.00818594
2021-05-15T23:10:57.860334+0000    10     255    311952    311697   121.727   119.777  0.00749832  0.00820354
2021-05-15T23:10:58.860498+0000    11     255    343877    343622   121.996   124.707 0.000628756  0.00818109
2021-05-15T23:10:59.860680+0000    12     255    379120    378865     123.3   137.668  0.00746068  0.00809971
2021-05-15T23:11:00.860915+0000 Total time run:       12.0991
Total reads made:     382482
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   123.486
Average IOPS:         31612
Stddev IOPS:          2250.65
Max IOPS:             35243
Min IOPS:             26167
Average Latency(s):   0.0080896
Max latency(s):       0.115249
Min latency(s):       0.000254794

[1;32mlocalhost.localdomain	[2021-05-15T16:11:01,526485389-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 497643


[1;33mlocalhost.localdomain	[2021-05-15T16:11:01,533312506-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:11:25,545864287-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 382.48k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:11:25,560314289-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:11:34,693236916-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 382.48k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:11:34,707632196-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:11:43,785568159-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 382.48k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:11:43,799945280-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:11:52,831245352-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 382.48k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:11:52,845718831-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:12:02,037527126-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 382.48k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:12:02,051709606-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:12:02,062711838-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T16:12:02,066843423-07:00][RUNNING][ROUND 3/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:12:02,073301045-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T16:12:02,089038062-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40601\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.55547\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e171fc26-a612-4871-8650-58b9b622a78e\nsetting min_mon_release = octopus\nepoch 0\nfsid e171fc26-a612-4871-8650-58b9b622a78e\nlast_changed 2021-05-15T16:12:27.516538-0700\ncreated 2021-05-15T16:12:27.516538-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40601/0,v1:10.10.1.2:40602/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.55547 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 04502799-5025-4c31-9967-0ab8e7fe3b0f\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 dc38e91f-a73e-4768-b250-8c0244d435f5\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 309603d1-0629-4136-aca3-c3708bab7b30\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42601\n  w/ user/pass: admin / cc5ebf7f-63ef-4910-8ba0-a297c88c8c4b\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:12:46 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40601
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.55547
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e171fc26-a612-4871-8650-58b9b622a78e
setting min_mon_release = octopus
epoch 0
fsid e171fc26-a612-4871-8650-58b9b622a78e
last_changed 2021-05-15T16:12:27.516538-0700
created 2021-05-15T16:12:27.516538-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40601/0,v1:10.10.1.2:40602/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.55547 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 04502799-5025-4c31-9967-0ab8e7fe3b0f
0
start osd.0
add osd1 dc38e91f-a73e-4768-b250-8c0244d435f5
1
start osd.1
add osd2 309603d1-0629-4136-aca3-c3708bab7b30
2
start osd.2


restful urls: https://10.10.1.2:42601
  w/ user/pass: admin / cc5ebf7f-63ef-4910-8ba0-a297c88c8c4b


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T16:12:04.017-0700 7fe4d313f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:12:04.017-0700 7fe4d313f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:12:04.037-0700 7f21c67131c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:12:04.037-0700 7f21c67131c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40601,v1:10.10.1.2:40602] --print /tmp/ceph_monmap.55547 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.55547 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.55547 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42601 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55f19f39c000 @  0x7f79c7215680 0x7f79c7236824 0x7f79c79d1187 0x7f79c79d9355 0x7f79c79d1708 0x7f79c79d1877 0x7f79c79d2c24 0x7f79c79eaec1 0x7f79c795d5f3 0x7f79c79bee97 0x7f79c79c6b1a 0x7f79c70c9d84 0x7f79c71e5609 0x7f79c6db9293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.LmDcTBqwv7 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 04502799-5025-4c31-9967-0ab8e7fe3b0f -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBmVaBg+AwRAxAA/0Z/hHFsj7AJJAYZtg0ybA== --osd-uuid 04502799-5025-4c31-9967-0ab8e7fe3b0f 
2021-05-15T16:12:38.693-0700 7f29743a9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:12:38.713-0700 7f29743a9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:12:38.713-0700 7f29743a9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55eba37f2000 @  0x7f2974d72680 0x7f2974d93824 0x55eb990ef447 0x55eb990f74b5 0x55eb990ef9c8 0x55eb990efb37 0x55eb990f0ee4 0x55eb98ec1ca1 0x55eb99099423 0x55eb98eb22a7 0x55eb98eb753a 0x7f29748c5d84 0x7f2974a4a609 0x7f29745b3293
2021-05-15T16:12:39.021-0700 7f29743a9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new dc38e91f-a73e-4768-b250-8c0244d435f5 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55ee211c4000 @  0x7fe32297a680 0x7fe32299b824 0x55ee153b3447 0x55ee153bb4b5 0x55ee153b39c8 0x55ee153b3b37 0x55ee153b4ee4 0x55ee15185ca1 0x55ee1535d423 0x55ee151762a7 0x55ee1517b53a 0x7fe3224cdd84 0x7fe322652609 0x7fe3221bb293
2021-05-15T16:12:39.657-0700 7fe321fb1f00 -1 Falling back to public interface
2021-05-15T16:12:39.913-0700 7fe321fb1f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBnVaBgyvTZExAAAvOShu5J9aHe+OIFn1aRgw== --osd-uuid dc38e91f-a73e-4768-b250-8c0244d435f5 
2021-05-15T16:12:40.005-0700 7f1e45761f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:12:40.025-0700 7f1e45761f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:12:40.025-0700 7f1e45761f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b3ffa40000 @  0x7f1e4612a680 0x7f1e4614b824 0x55b3f5229447 0x55b3f52314b5 0x55b3f52299c8 0x55b3f5229b37 0x55b3f522aee4 0x55b3f4ffbca1 0x55b3f51d3423 0x55b3f4fec2a7 0x55b3f4ff153a 0x7f1e45c7dd84 0x7f1e45e02609 0x7f1e4596b293
2021-05-15T16:12:40.333-0700 7f1e45761f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 309603d1-0629-4136-aca3-c3708bab7b30 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5574091fa000 @  0x7f196e5fc680 0x7f196e61d824 0x5573fef91447 0x5573fef994b5 0x5573fef919c8 0x5573fef91b37 0x5573fef92ee4 0x5573fed63ca1 0x5573fef3b423 0x5573fed542a7 0x5573fed5953a 0x7f196e14fd84 0x7f196e2d4609 0x7f196de3d293
2021-05-15T16:12:41.001-0700 7f196dc33f00 -1 Falling back to public interface
2021-05-15T16:12:41.257-0700 7f196dc33f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBoVaBgMPGOJxAAS6EJPu1Okp8yaWcoMWKLmA== --osd-uuid 309603d1-0629-4136-aca3-c3708bab7b30 
2021-05-15T16:12:41.317-0700 7f9fdce9cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:12:41.337-0700 7f9fdce9cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:12:41.337-0700 7f9fdce9cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55700ad26000 @  0x7f9fdd865680 0x7f9fdd886824 0x556fff878447 0x556fff8804b5 0x556fff8789c8 0x556fff878b37 0x556fff879ee4 0x556fff64aca1 0x556fff822423 0x556fff63b2a7 0x556fff64053a 0x7f9fdd3b8d84 0x7f9fdd53d609 0x7f9fdd0a6293
2021-05-15T16:12:41.653-0700 7f9fdce9cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x564e364a6000 @  0x7f1718cbc680 0x7f1718cdd824 0x564e2b4a9447 0x564e2b4b14b5 0x564e2b4a99c8 0x564e2b4a9b37 0x564e2b4aaee4 0x564e2b27bca1 0x564e2b453423 0x564e2b26c2a7 0x564e2b27153a 0x7f171880fd84 0x7f1718994609 0x7f17184fd293
2021-05-15T16:12:42.281-0700 7f17182f3f00 -1 Falling back to public interface
2021-05-15T16:12:42.541-0700 7f17182f3f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T16:12:46,689038065-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T16:12:46,707623811-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T16:12:46,789434744-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:12:46,795956660-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T16:12:50,791481985-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:12:50,797926454-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T16:12:54,507713821-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:12:54,514178871-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T16:12:58,429699579-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:12:58,435926594-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T16:13:06,452921990-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:13:06,459825077-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T16:13:10,563783917-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:13:10,570213144-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T16:13:15,269141327-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:13:15,275251610-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T16:13:20,357095233-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:13:20,363352853-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:13:25,250166432-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:13:25,256657804-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:13:29,739752808-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:13:29,746167754-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T16:13:34,331193724-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:13:34,337398646-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T16:13:38,207770668-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:13:38,214060037-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.06   90      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.94   89      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  1.01   80      up          osd.2  
                       TOTAL  300 GiB  177 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.94/1.06  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T16:13:41,924021334-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:14:05,836124552-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:14:14,869785801-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:14:24,067195859-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:14:33,290672828-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:14:42,296775180-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   245 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:14:42,310933171-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:14:51,343896256-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:14:51,358221590-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:15:00,399487157-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:15:00,413995046-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:15:09,417120854-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:15:09,431446080-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:15:18,482474740-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:15:18,496731800-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:15:18,507785348-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:15:18,514352045-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:15:18,529029549-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=510644
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T16:15:18,541371985-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-15T16:15:18,580927607-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:15:18,587423013-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:15:20.204+0000 ffffa1892010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:15:21.028+0000 ffffa1892010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:15:21.028+0000 ffffa1892010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:15:21.045667+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-15T23:15:21.045714+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-15T23:15:21.047855+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:15:21.047855+0000     0       0         0         0         0         0           -           0
2021-05-15T23:15:22.048018+0000     1     255     20082     19827   77.4447   77.4492   0.0596481   0.0125506
2021-05-15T23:15:23.048218+0000     2     255     40232     39977     78.07   78.7109  0.00465803   0.0126569
2021-05-15T23:15:24.048375+0000     3     255     60435     60180   78.3485    78.918  0.00931931   0.0126761
2021-05-15T23:15:25.048535+0000     4     255     80306     80051   78.1636   77.6211   0.0131396   0.0127519
2021-05-15T23:15:26.048680+0000     5     255    100983    100728   78.6824   80.7695   0.0271229   0.0126412
2021-05-15T23:15:27.048845+0000     6     255    120881    120626    78.521   77.7266  0.00501918   0.0126891
2021-05-15T23:15:28.049010+0000     7     255    140917    140662   78.4826   78.2656  0.00544433    0.012692
2021-05-15T23:15:29.049174+0000     8     255    161041    160786   78.4969   78.6094  0.00571059   0.0126838
2021-05-15T23:15:30.049338+0000     9     255    180310    180055   78.1369   75.2695  0.00187899   0.0127203
2021-05-15T23:15:31.049474+0000    10     255    199769    199514   77.9234   76.0117   0.0125012   0.0128152
2021-05-15T23:15:32.049628+0000    11     255    221463    221208   78.5421   84.7422  0.00211965   0.0126943
2021-05-15T23:15:33.049780+0000    12     255    241526    241271   78.5268   78.3711    0.093854   0.0126929
2021-05-15T23:15:34.049945+0000    13     255    261452    261197   78.4727   77.8359  0.00309864   0.0127008
2021-05-15T23:15:35.050103+0000    14     255    278208    277953    77.542   65.4531  0.00172814   0.0128731
2021-05-15T23:15:36.050265+0000    15     255    296077    295822   77.0251   69.8008   0.0029196   0.0129516
2021-05-15T23:15:37.050411+0000    16     255    313462    313207   76.4548   67.9102  0.00140834   0.0130383
2021-05-15T23:15:38.050559+0000    17     255    329892    329637   75.7322   64.1797  0.00142961   0.0131716
2021-05-15T23:15:39.050720+0000    18     255    346114    345859   75.0447   63.3672  0.00321664   0.0133071
2021-05-15T23:15:40.050869+0000    19     255    363185    362930   74.6041   66.6836   0.0814515   0.0133718
2021-05-15T23:15:41.051022+0000 min lat: 0.000645239 max lat: 0.203707 avg lat: 0.0134828
2021-05-15T23:15:41.051022+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:15:41.051022+0000    20     137    379166    379029   74.0178   62.8867   0.0137418   0.0134828
2021-05-15T23:15:42.051286+0000 Total time run:         20.0373
Total writes made:      379166
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     73.918
Stddev Bandwidth:       6.64791
Max bandwidth (MB/sec): 84.7422
Min bandwidth (MB/sec): 62.8867
Average IOPS:           18922
Stddev IOPS:            1701.87
Max IOPS:               21694
Min IOPS:               16099
Average Latency(s):     0.0135027
Stddev Latency(s):      0.022253
Max latency(s):         0.203707
Min latency(s):         0.000645239

[1;32mlocalhost.localdomain	[2021-05-15T16:15:42,653913709-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 510644


[1;33mlocalhost.localdomain	[2021-05-15T16:15:42,661005870-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:16:06,948453308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 379.17k objects, 1.4 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:16:06,962710912-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:16:16,183845427-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 379.17k objects, 1.4 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:16:16,198835154-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:16:25,257744840-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 379.17k objects, 1.4 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:16:25,272159084-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:16:34,501228437-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 379.17k objects, 1.4 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:16:34,515666019-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:16:43,436992880-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 379.17k objects, 1.4 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:16:43,451535156-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:16:43,462920337-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:16:43,469829836-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:16:43,485078798-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=514172
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T16:16:43,498118228-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-15T16:16:43,536603390-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:16:43,543041620-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '32d1dfd1-bdcf-47f8-a622-c9ead9a36a23', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 32d1dfd1-bdcf-47f8-a622-c9ead9a36a23 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qjc611:/tmp/ceph-asok.qjc611 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:16:45.229+0000 ffff99565010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:16:46.013+0000 ffff99565010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:16:46.017+0000 ffff99565010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:16:46.042053+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:16:46.042053+0000     0       0         0         0         0         0           -           0
2021-05-15T23:16:47.042236+0000     1     255     28451     28196   110.106   110.141  0.00718253  0.00902024
2021-05-15T23:16:48.042399+0000     2     255     59712     59457   116.099   122.113   0.0077341   0.0085793
2021-05-15T23:16:49.042577+0000     3     255     84478     84223   109.641   96.7422  0.00698041   0.0090958
2021-05-15T23:16:50.042746+0000     4     255    120358    120103   117.264   140.156  0.00709127   0.0085078
2021-05-15T23:16:51.042913+0000     5     255    149361    149106   116.466   113.293  0.00779366  0.00856889
2021-05-15T23:16:52.043115+0000     6     256    178324    178068   115.907   113.133  0.00892604  0.00861092
2021-05-15T23:16:53.043282+0000     7     256    209582    209326   116.789   122.102 0.000296311  0.00852892
2021-05-15T23:16:54.043452+0000     8     255    239988    239733   117.035   118.777  0.00741573   0.0085314
2021-05-15T23:16:55.043662+0000     9     256    267432    267176   115.939   107.199  0.00849734  0.00861245
2021-05-15T23:16:56.043828+0000    10     255    299548    299293   116.889   125.457  0.00718876  0.00854358
2021-05-15T23:16:57.043993+0000    11     255    330500    330245   117.252   120.906   0.0119623  0.00851671
2021-05-15T23:16:58.044159+0000    12     255    362348    362093   117.847   124.406  0.00769679  0.00847485
2021-05-15T23:16:59.044502+0000 Total time run:       12.636
Total reads made:     379166
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   117.214
Average IOPS:         30006
Stddev IOPS:          2789.75
Max IOPS:             35880
Min IOPS:             24766
Average Latency(s):   0.00852213
Max latency(s):       0.17524
Min latency(s):       0.00025966

[1;32mlocalhost.localdomain	[2021-05-15T16:16:59,794920412-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 514172


[1;33mlocalhost.localdomain	[2021-05-15T16:16:59,802114620-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:17:23,826087631-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 379.17k objects, 1.4 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:17:23,840915016-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:17:32,847300627-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 379.17k objects, 1.4 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:17:32,861959200-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:17:41,997768138-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 379.17k objects, 1.4 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:17:42,014082481-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:17:51,199535608-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 379.17k objects, 1.4 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:17:51,213842697-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:18:00,137404536-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 379.17k objects, 1.4 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:18:00,152061444-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:18:00,163592275-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T16:18:00,167949185-07:00][RUNNING][ROUND 4/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:18:00,174711699-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T16:18:00,191504124-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40016\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.56649\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 5e94a076-2ab1-4604-8f57-5196ae97081f\nsetting min_mon_release = octopus\nepoch 0\nfsid 5e94a076-2ab1-4604-8f57-5196ae97081f\nlast_changed 2021-05-15T16:18:25.982940-0700\ncreated 2021-05-15T16:18:25.982940-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40016/0,v1:10.10.1.2:40017/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.56649 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 69818d28-7469-49e7-9bbb-70d2ad0c12d4\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 d654109f-9c81-480c-b6e7-16309639eedc\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 485abe54-5a5b-433d-9e41-f06446d4a38b\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42016\n  w/ user/pass: admin / 2df8ae07-6637-4b44-9ccb-5e9cfb132295\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:18:45 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40016
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.56649
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 5e94a076-2ab1-4604-8f57-5196ae97081f
setting min_mon_release = octopus
epoch 0
fsid 5e94a076-2ab1-4604-8f57-5196ae97081f
last_changed 2021-05-15T16:18:25.982940-0700
created 2021-05-15T16:18:25.982940-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40016/0,v1:10.10.1.2:40017/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.56649 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 69818d28-7469-49e7-9bbb-70d2ad0c12d4
0
start osd.0
add osd1 d654109f-9c81-480c-b6e7-16309639eedc
1
start osd.1
add osd2 485abe54-5a5b-433d-9e41-f06446d4a38b
2
start osd.2


restful urls: https://10.10.1.2:42016
  w/ user/pass: admin / 2df8ae07-6637-4b44-9ccb-5e9cfb132295


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T16:18:02.080-0700 7f2e392981c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:18:02.080-0700 7f2e392981c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:18:02.100-0700 7f31751601c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:18:02.100-0700 7f31751601c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40016,v1:10.10.1.2:40017] --print /tmp/ceph_monmap.56649 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.56649 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.56649 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42016 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x557b4a986000 @  0x7f96e7408680 0x7f96e7429824 0x7f96e7bc4187 0x7f96e7bcc355 0x7f96e7bc4708 0x7f96e7bc4877 0x7f96e7bc5c24 0x7f96e7bddec1 0x7f96e7b505f3 0x7f96e7bb1e97 0x7f96e7bb9b1a 0x7f96e72bcd84 0x7f96e73d8609 0x7f96e6fac293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.8J2NzojXDf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 69818d28-7469-49e7-9bbb-70d2ad0c12d4 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDMVqBgsY1AExAAAWxYYjwJ8HoYgbBEYPKBUA== --osd-uuid 69818d28-7469-49e7-9bbb-70d2ad0c12d4 
2021-05-15T16:18:36.972-0700 7fd1bbf46f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:18:36.992-0700 7fd1bbf46f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:18:36.992-0700 7fd1bbf46f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x559bb30e0000 @  0x7fd1bc90f680 0x7fd1bc930824 0x559ba9042447 0x559ba904a4b5 0x559ba90429c8 0x559ba9042b37 0x559ba9043ee4 0x559ba8e14ca1 0x559ba8fec423 0x559ba8e052a7 0x559ba8e0a53a 0x7fd1bc462d84 0x7fd1bc5e7609 0x7fd1bc150293
2021-05-15T16:18:37.300-0700 7fd1bbf46f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d654109f-9c81-480c-b6e7-16309639eedc -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5566819ac000 @  0x7f6aa0e9a680 0x7f6aa0ebb824 0x5566766b6447 0x5566766be4b5 0x5566766b69c8 0x5566766b6b37 0x5566766b7ee4 0x556676488ca1 0x556676660423 0x5566764792a7 0x55667647e53a 0x7f6aa09edd84 0x7f6aa0b72609 0x7f6aa06db293
2021-05-15T16:18:37.908-0700 7f6aa04d1f00 -1 Falling back to public interface
2021-05-15T16:18:38.164-0700 7f6aa04d1f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDNVqBgI1N5JBAAYCrlIefFJQmZPo6wnD6QaQ== --osd-uuid d654109f-9c81-480c-b6e7-16309639eedc 
2021-05-15T16:18:38.288-0700 7f83a0bb3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:18:38.308-0700 7f83a0bb3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:18:38.308-0700 7f83a0bb3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5603ad3e4000 @  0x7f83a157c680 0x7f83a159d824 0x5603a18b3447 0x5603a18bb4b5 0x5603a18b39c8 0x5603a18b3b37 0x5603a18b4ee4 0x5603a1685ca1 0x5603a185d423 0x5603a16762a7 0x5603a167b53a 0x7f83a10cfd84 0x7f83a1254609 0x7f83a0dbd293
2021-05-15T16:18:38.620-0700 7f83a0bb3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 485abe54-5a5b-433d-9e41-f06446d4a38b -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55d450ea6000 @  0x7f31f7c75680 0x7f31f7c96824 0x55d4457ce447 0x55d4457d64b5 0x55d4457ce9c8 0x55d4457ceb37 0x55d4457cfee4 0x55d4455a0ca1 0x55d445778423 0x55d4455912a7 0x55d44559653a 0x7f31f77c8d84 0x7f31f794d609 0x7f31f74b6293
2021-05-15T16:18:39.304-0700 7f31f72acf00 -1 Falling back to public interface
2021-05-15T16:18:39.556-0700 7f31f72acf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDOVqBgqzZVOhAAwcSKfiRlBkedf338cNwgRg== --osd-uuid 485abe54-5a5b-433d-9e41-f06446d4a38b 
2021-05-15T16:18:39.636-0700 7fdae2d5cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:18:39.656-0700 7fdae2d5cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:18:39.656-0700 7fdae2d5cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55d89cb84000 @  0x7fdae3725680 0x7fdae3746824 0x55d89168d447 0x55d8916954b5 0x55d89168d9c8 0x55d89168db37 0x55d89168eee4 0x55d89145fca1 0x55d891637423 0x55d8914502a7 0x55d89145553a 0x7fdae3278d84 0x7fdae33fd609 0x7fdae2f66293
2021-05-15T16:18:39.972-0700 7fdae2d5cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55f3e68e4000 @  0x7f5a8d940680 0x7f5a8d961824 0x55f3dac62447 0x55f3dac6a4b5 0x55f3dac629c8 0x55f3dac62b37 0x55f3dac63ee4 0x55f3daa34ca1 0x55f3dac0c423 0x55f3daa252a7 0x55f3daa2a53a 0x7f5a8d493d84 0x7f5a8d618609 0x7f5a8d181293
2021-05-15T16:18:40.548-0700 7f5a8cf77f00 -1 Falling back to public interface
2021-05-15T16:18:40.808-0700 7f5a8cf77f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T16:18:45,106259807-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T16:18:45,125297569-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T16:18:45,204668499-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:18:45,210818249-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T16:18:49,317643245-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:18:49,325551682-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T16:18:53,278819641-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:18:53,285137671-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T16:18:57,166879446-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:18:57,173283078-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T16:19:04,766555514-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:19:04,772939762-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T16:19:09,002465381-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:19:09,009025620-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T16:19:13,677814430-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:19:13,684340208-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T16:19:18,164634478-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:19:18,171145808-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:19:22,751240573-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:19:22,757783137-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:19:27,694231866-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:19:27,700536493-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T16:19:31,912339382-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:19:31,918745693-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T16:19:35,609350640-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:19:35,615681565-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  180 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  180 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   66 KiB   0 B   0 B   0 B  100 GiB     0  1.10   88      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.92   91      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  0.98   80      up          osd.2  
                       TOTAL  300 GiB  181 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.92/1.10  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T16:19:39,547805454-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:20:03,815811732-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:20:12,839470124-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:20:22,061870391-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:20:31,180536991-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:20:40,358856859-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   peering
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:20:40,373704143-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:20:49,565561159-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:20:49,580227558-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:20:58,552825537-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:20:58,567459484-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:21:07,820571694-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:21:07,835548604-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:21:16,918698639-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:21:16,933403512-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:21:16,945087611-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:21:16,951912141-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:21:16,966881248-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=527168
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T16:21:16,979626785-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-15T16:21:17,018906048-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:21:17,025261279-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:21:18.791+0000 ffff902ee010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:21:19.579+0000 ffff902ee010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:21:19.579+0000 ffff902ee010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:21:19.599507+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-15T23:21:19.599552+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-15T23:21:19.601851+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:21:19.601851+0000     0       0         0         0         0         0           -           0
2021-05-15T23:21:20.602026+0000     1     255     19541     19286   75.3192   75.3359   0.0023008   0.0125273
2021-05-15T23:21:21.602185+0000     2     255     37626     37371   72.9763   70.6445   0.0035555    0.013392
2021-05-15T23:21:22.602342+0000     3     255     56388     56133   73.0767   73.2891  0.00101519   0.0134325
2021-05-15T23:21:23.602516+0000     4     255     74537     74282   72.5281   70.8945  0.00185986   0.0135847
2021-05-15T23:21:24.602673+0000     5     255     92712     92457   72.2195   70.9961  0.00234306   0.0136973
2021-05-15T23:21:25.602840+0000     6     256    110901    110645   72.0221   71.0469  0.00134324    0.013757
2021-05-15T23:21:26.603004+0000     7     255    129441    129186    72.078   72.4258  0.00220377   0.0137578
2021-05-15T23:21:27.603166+0000     8     255    147587    147332   71.9272   70.8828  0.00120619   0.0138007
2021-05-15T23:21:28.603329+0000     9     255    165424    165169   71.6758   69.6758  0.00162026   0.0138794
2021-05-15T23:21:29.603495+0000    10     255    183398    183143   71.5281   70.2109  0.00134459   0.0138991
2021-05-15T23:21:30.603658+0000    11     255    201475    201220   71.4439   70.6133  0.00143798   0.0139242
2021-05-15T23:21:31.603820+0000    12     255    219316    219061   71.2969   69.6914  0.00210385   0.0139781
2021-05-15T23:21:32.603979+0000    13     255    238125    237870   71.4634   73.4727  0.00120488   0.0139483
2021-05-15T23:21:33.604143+0000    14     255    255110    254855   71.0972   66.3477   0.0170033    0.014033
2021-05-15T23:21:34.604297+0000    15     255    271735    271480   70.6862   64.9414  0.00144388   0.0141185
2021-05-15T23:21:35.604459+0000    16     255    288796    288541   70.4329   66.6445  0.00446261   0.0141716
2021-05-15T23:21:36.604640+0000    17     255    305380    305125   70.0998   64.7812    0.031477   0.0142429
2021-05-15T23:21:37.604799+0000    18     255    320769    320514   69.5444   60.1133  0.00178691   0.0143334
2021-05-15T23:21:38.604955+0000    19     255    335850    335595   68.9842   58.9102  0.00250879   0.0144726
2021-05-15T23:21:39.605112+0000 min lat: 0.000646896 max lat: 0.200532 avg lat: 0.0145295
2021-05-15T23:21:39.605112+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:21:39.605112+0000    20     136    352080    351944   68.7277   63.8633  0.00793247   0.0145295
2021-05-15T23:21:40.605358+0000 Total time run:         20.0298
Total writes made:      352080
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     68.6633
Stddev Bandwidth:       4.3887
Max bandwidth (MB/sec): 75.3359
Min bandwidth (MB/sec): 58.9102
Average IOPS:           17577
Stddev IOPS:            1123.51
Max IOPS:               19286
Min IOPS:               15081
Average Latency(s):     0.014541
Stddev Latency(s):      0.0307337
Max latency(s):         0.200532
Min latency(s):         0.000646896

[1;32mlocalhost.localdomain	[2021-05-15T16:21:41,223062499-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 527168


[1;33mlocalhost.localdomain	[2021-05-15T16:21:41,230728128-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:22:05,436065432-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.08k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:22:05,450929142-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:22:14,770328308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.08k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:22:14,785290409-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:22:24,260195595-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.08k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:22:24,274472824-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:22:33,328885010-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.08k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:22:33,343887607-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:22:42,427375601-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.08k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:22:42,442371778-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:22:42,453980746-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:22:42,460665043-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:22:42,475963381-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=530795
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T16:22:42,488980049-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-15T16:22:42,527101339-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:22:42,533384561-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '442c4e0a-a411-49f7-ab3d-ba07f23edd8d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 442c4e0a-a411-49f7-ab3d-ba07f23edd8d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MdRhnd:/tmp/ceph-asok.MdRhnd -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:22:44.051+0000 ffffa61e8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:22:44.876+0000 ffffa61e8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:22:44.876+0000 ffffa61e8010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:22:44.898591+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:22:44.898591+0000     0       0         0         0         0         0           -           0
2021-05-15T23:22:45.898753+0000     1     255     25361     25106   98.0398   98.0703   0.0025519   0.0100827
2021-05-15T23:22:46.898925+0000     2     255     60060     59805   116.778   135.543   0.0208322  0.00852329
2021-05-15T23:22:47.899101+0000     3     255     92395     92140   119.948   126.309  0.00706034  0.00831387
2021-05-15T23:22:48.899296+0000     4     255    123983    123728   120.802   123.391  0.00472519  0.00825722
2021-05-15T23:22:49.899476+0000     5     255    158521    158266    123.62   134.914  0.00730851  0.00807295
2021-05-15T23:22:50.899640+0000     6     256    189337    189081   123.075   120.371 0.000330591  0.00804567
2021-05-15T23:22:51.899803+0000     7     255    222768    222513   124.146   130.594  0.00756046  0.00804153
2021-05-15T23:22:52.900018+0000     8     255    248764    248509   121.318   101.547  0.00324124  0.00822779
2021-05-15T23:22:53.900187+0000     9     255    279622    279367    121.23   120.539  0.00678149    0.008237
2021-05-15T23:22:54.900374+0000    10     255    307615    307360   120.039   109.348   0.0705488  0.00830633
2021-05-15T23:22:55.900544+0000    11     256    340040    339784   120.639   126.656  0.00337166  0.00827715
2021-05-15T23:22:56.900776+0000 Total time run:       11.415
Total reads made:     352080
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   120.483
Average IOPS:         30843
Stddev IOPS:          3240.55
Max IOPS:             34699
Min IOPS:             25106
Average Latency(s):   0.00829065
Max latency(s):       0.149736
Min latency(s):       0.00025373

[1;32mlocalhost.localdomain	[2021-05-15T16:22:57,534660662-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 530795


[1;33mlocalhost.localdomain	[2021-05-15T16:22:57,542104249-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:23:21,804877667-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.08k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:23:21,819697853-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:23:30,850665660-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.08k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:23:30,865776891-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:23:39,914447922-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.08k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:23:39,929302232-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:23:49,286655733-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.08k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:23:49,303340535-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:23:58,220589397-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.08k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:23:58,235617402-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:23:58,247373246-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T16:23:58,251721759-07:00][RUNNING][ROUND 5/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:23:58,258579970-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T16:23:58,274722870-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40429\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.57767\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a1ffb2b8-bbf8-4fff-a455-072b10f8f1f5\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid a1ffb2b8-bbf8-4fff-a455-072b10f8f1f5\nlast_changed 2021-05-15T16:24:15.815507-0700\ncreated 2021-05-15T16:24:15.815507-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40429/0,v1:10.10.1.2:40430/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.57767 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 724c8e80-22f8-4a70-b18f-7d10f74fda14\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 df199938-cf96-4a23-b268-67b11a07af18\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 835cac6c-8640-4ad3-ac7c-ef5deab30ac6\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42429\n  w/ user/pass: admin / 543f1046-a6dc-403f-89b8-787c353081cc\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:24:37 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40429
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.57767
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a1ffb2b8-bbf8-4fff-a455-072b10f8f1f5
setting min_mon_release = octopus
epoch 0
fsid a1ffb2b8-bbf8-4fff-a455-072b10f8f1f5
last_changed 2021-05-15T16:24:15.815507-0700
created 2021-05-15T16:24:15.815507-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40429/0,v1:10.10.1.2:40430/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.57767 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 724c8e80-22f8-4a70-b18f-7d10f74fda14
0
start osd.0
add osd1 df199938-cf96-4a23-b268-67b11a07af18
1
start osd.1
add osd2 835cac6c-8640-4ad3-ac7c-ef5deab30ac6
2
start osd.2


restful urls: https://10.10.1.2:42429
  w/ user/pass: admin / 543f1046-a6dc-403f-89b8-787c353081cc


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T16:24:00.187-0700 7f93440f01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:24:00.187-0700 7f93440f01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:24:00.203-0700 7fa0737fe1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:24:00.203-0700 7fa0737fe1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40429,v1:10.10.1.2:40430] --print /tmp/ceph_monmap.57767 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.57767 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.57767 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42429 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55d5de556000 @  0x7f3b6ccff680 0x7f3b6cd20824 0x7f3b6d4bb187 0x7f3b6d4c3355 0x7f3b6d4bb708 0x7f3b6d4bb877 0x7f3b6d4bcc24 0x7f3b6d4d4ec1 0x7f3b6d4475f3 0x7f3b6d4a8e97 0x7f3b6d4b0b1a 0x7f3b6cbb3d84 0x7f3b6cccf609 0x7f3b6c8a3293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.EPjOUqzDHX 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 724c8e80-22f8-4a70-b18f-7d10f74fda14 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAsWKBg2UaHGhAAzrIsXj/Numvmj4bw+OOUoA== --osd-uuid 724c8e80-22f8-4a70-b18f-7d10f74fda14 
2021-05-15T16:24:29.083-0700 7fa19fda7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:24:29.103-0700 7fa19fda7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:24:29.103-0700 7fa19fda7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55aead23a000 @  0x7fa1a0770680 0x7fa1a0791824 0x55aea137c447 0x55aea13844b5 0x55aea137c9c8 0x55aea137cb37 0x55aea137dee4 0x55aea114eca1 0x55aea1326423 0x55aea113f2a7 0x55aea114453a 0x7fa1a02c3d84 0x7fa1a0448609 0x7fa19ffb1293
2021-05-15T16:24:29.463-0700 7fa19fda7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new df199938-cf96-4a23-b268-67b11a07af18 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55f14a464000 @  0x7f408196a680 0x7f408198b824 0x55f13ff83447 0x55f13ff8b4b5 0x55f13ff839c8 0x55f13ff83b37 0x55f13ff84ee4 0x55f13fd55ca1 0x55f13ff2d423 0x55f13fd462a7 0x55f13fd4b53a 0x7f40814bdd84 0x7f4081642609 0x7f40811ab293
2021-05-15T16:24:30.063-0700 7f4080fa1f00 -1 Falling back to public interface
2021-05-15T16:24:30.315-0700 7f4080fa1f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAtWKBgJ1NALhAAHNLkyp8EUBUX4I4lyhI3kA== --osd-uuid df199938-cf96-4a23-b268-67b11a07af18 
2021-05-15T16:24:30.435-0700 7f490444bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:24:30.455-0700 7f490444bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:24:30.455-0700 7f490444bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55d46d0c4000 @  0x7f4904e14680 0x7f4904e35824 0x55d46297f447 0x55d4629874b5 0x55d46297f9c8 0x55d46297fb37 0x55d462980ee4 0x55d462751ca1 0x55d462929423 0x55d4627422a7 0x55d46274753a 0x7f4904967d84 0x7f4904aec609 0x7f4904655293
2021-05-15T16:24:30.771-0700 7f490444bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 835cac6c-8640-4ad3-ac7c-ef5deab30ac6 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55f1b17a0000 @  0x7f79aaad6680 0x7f79aaaf7824 0x55f1a7304447 0x55f1a730c4b5 0x55f1a73049c8 0x55f1a7304b37 0x55f1a7305ee4 0x55f1a70d6ca1 0x55f1a72ae423 0x55f1a70c72a7 0x55f1a70cc53a 0x7f79aa629d84 0x7f79aa7ae609 0x7f79aa317293
2021-05-15T16:24:31.431-0700 7f79aa10df00 -1 Falling back to public interface
2021-05-15T16:24:31.687-0700 7f79aa10df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAvWKBgKq3mBxAAs4RlXq4/rlW8u16cKt31/Q== --osd-uuid 835cac6c-8640-4ad3-ac7c-ef5deab30ac6 
2021-05-15T16:24:31.807-0700 7f0e461aaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:24:31.831-0700 7f0e461aaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:24:31.831-0700 7f0e461aaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x556e75ed8000 @  0x7f0e46b73680 0x7f0e46b94824 0x556e6ad1a447 0x556e6ad224b5 0x556e6ad1a9c8 0x556e6ad1ab37 0x556e6ad1bee4 0x556e6aaecca1 0x556e6acc4423 0x556e6aadd2a7 0x556e6aae253a 0x7f0e466c6d84 0x7f0e4684b609 0x7f0e463b4293
2021-05-15T16:24:32.143-0700 7f0e461aaf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55fab136a000 @  0x7f3042b4c680 0x7f3042b6d824 0x55faa5c69447 0x55faa5c714b5 0x55faa5c699c8 0x55faa5c69b37 0x55faa5c6aee4 0x55faa5a3bca1 0x55faa5c13423 0x55faa5a2c2a7 0x55faa5a3153a 0x7f304269fd84 0x7f3042824609 0x7f304238d293
2021-05-15T16:24:32.771-0700 7f3042183f00 -1 Falling back to public interface
2021-05-15T16:24:33.031-0700 7f3042183f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T16:24:37,187617249-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T16:24:37,207542587-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T16:24:37,289184301-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:24:37,295440187-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T16:24:41,171564046-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:24:41,177957387-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T16:24:45,016544686-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:24:45,022760676-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T16:24:49,027927213-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:24:49,034675576-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T16:24:57,098547753-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:24:57,104936690-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T16:25:01,174595847-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:25:01,181724676-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T16:25:05,499770140-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:25:05,506204307-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T16:25:09,674272494-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:25:09,680367341-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:25:14,811335084-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:25:14,817736076-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:25:19,469973972-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:25:19,476183481-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T16:25:23,680695744-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:25:23,687092246-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T16:25:27,729632299-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:25:27,735880001-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  187 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  187 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   66 KiB   0 B   0 B   0 B  100 GiB     0  1.06   90      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  0.95   89      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  0.99   80      up          osd.2  
                       TOTAL  300 GiB  188 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.06  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T16:25:31,472784972-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:25:55,470787095-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:26:04,586645640-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 47s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:26:13,881115427-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (30s)
      [========....................] (remaining: 66s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:26:22,912037556-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 80s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:26:31,940337535-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   peering
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 39s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:26:41,268807468-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:26:41,287967623-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:26:50,326354744-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:26:50,340951004-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:26:59,408953297-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:26:59,423483628-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:27:08,550327964-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:27:08,565200378-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:27:17,709628362-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:27:17,724401587-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:27:17,735924958-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:27:17,742899439-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:27:17,758433285-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=543890
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T16:27:17,771107393-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-15T16:27:17,810121095-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:27:17,816275019-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:27:19.415+0000 ffff84260010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:27:20.203+0000 ffff84260010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:27:20.203+0000 ffff84260010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:27:20.224239+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-15T23:27:20.224283+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-15T23:27:20.226520+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:27:20.226520+0000     0       0         0         0         0         0           -           0
2021-05-15T23:27:21.226672+0000     1     255     20031     19776   77.2449     77.25  0.00229038   0.0122896
2021-05-15T23:27:22.226828+0000     2     255     39012     38757   75.6889   74.1445  0.00567711   0.0130748
2021-05-15T23:27:23.226982+0000     3     255     58261     58006   75.5192   75.1914    0.067058   0.0131115
2021-05-15T23:27:24.227137+0000     4     255     77747     77492   75.6657   76.1172  0.00268145   0.0130985
2021-05-15T23:27:25.227301+0000     5     255     96961     96706   75.5411   75.0547  0.00553444   0.0131042
2021-05-15T23:27:26.227458+0000     6     255    116108    115853   75.4144    74.793   0.0716836    0.013188
2021-05-15T23:27:27.227611+0000     7     255    135372    135117   75.3893     75.25  0.00240472   0.0132018
2021-05-15T23:27:28.227764+0000     8     255    153546    153291   74.8383   70.9922   0.0607372    0.013316
2021-05-15T23:27:29.227925+0000     9     256    171668    171412   74.3867   70.7852   0.0774125   0.0132989
2021-05-15T23:27:30.228085+0000    10     255    190004    189749   74.1097   71.6289  0.00160349    0.013455
2021-05-15T23:27:31.228239+0000    11     255    208838    208583   74.0597   73.5703   0.0070796   0.0134676
2021-05-15T23:27:32.228405+0000    12     256    225513    225257   73.3149   65.1328  0.00741796   0.0136081
2021-05-15T23:27:33.228591+0000    13     255    244401    244146     73.35   73.7852  0.00151146   0.0135878
2021-05-15T23:27:34.228768+0000    14     256    261968    261712    73.011   68.6172   0.0019917     0.01363
2021-05-15T23:27:35.228922+0000    15     256    279037    278781    72.588   66.6758  0.00167323   0.0137218
2021-05-15T23:27:36.229081+0000    16     256    295526    295270   72.0763   64.4102 0.000714773   0.0137951
2021-05-15T23:27:37.229263+0000    17     256    313030    312774   71.8578    68.375   0.0176888   0.0138938
2021-05-15T23:27:38.229430+0000    18     255    328824    328569   71.2929   61.6992  0.00216214   0.0139874
2021-05-15T23:27:39.229605+0000    19     255    346214    345959   71.1152   67.9297  0.00306101   0.0140336
2021-05-15T23:27:40.229754+0000 min lat: 0.000643344 max lat: 0.190844 avg lat: 0.0140452
2021-05-15T23:27:40.229754+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:27:40.229754+0000    20     217    363943    363726   71.0291   69.4023  0.00860022   0.0140452
2021-05-15T23:27:41.229994+0000 Total time run:         20.0779
Total writes made:      363943
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     70.8069
Stddev Bandwidth:       4.37678
Max bandwidth (MB/sec): 77.25
Min bandwidth (MB/sec): 61.6992
Average IOPS:           18126
Stddev IOPS:            1120.46
Max IOPS:               19776
Min IOPS:               15795
Average Latency(s):     0.0140835
Stddev Latency(s):      0.025001
Max latency(s):         0.190844
Min latency(s):         0.000643344

[1;32mlocalhost.localdomain	[2021-05-15T16:27:41,862255055-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 543890


[1;33mlocalhost.localdomain	[2021-05-15T16:27:41,870142244-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:28:05,955498827-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 363.94k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:28:05,970846646-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:28:15,169300495-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 363.94k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:28:15,184191581-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:28:24,221877050-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 363.94k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:28:24,236781272-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:28:33,526151921-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 363.94k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:28:33,541167756-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:28:42,935798948-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 363.94k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:28:42,952333538-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:28:42,965687391-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:28:42,974472449-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:28:42,992343127-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=547516
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T16:28:43,006684318-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-15T16:28:43,046117115-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:28:43,052554171-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '78bd2514-b535-4a2c-8be3-e7f7568eeb12', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 78bd2514-b535-4a2c-8be3-e7f7568eeb12 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fwu9yt:/tmp/ceph-asok.fwu9yt -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:28:44.650+0000 ffffa0eb8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:28:45.687+0000 ffffa0eb8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:28:45.687+0000 ffffa0eb8010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:28:45.713232+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:28:45.713232+0000     0       0         0         0         0         0           -           0
2021-05-15T23:28:46.713393+0000     1     255     32524     32269   126.017   126.051  0.00668748  0.00788425
2021-05-15T23:28:47.713588+0000     2     255     67637     67382   131.575    137.16 0.000893211  0.00756117
2021-05-15T23:28:48.713784+0000     3     255    100230     99975   130.147   127.316  0.00279635  0.00765269
2021-05-15T23:28:49.713987+0000     4     255    131181    130926    127.83   120.902   0.0079985  0.00780386
2021-05-15T23:28:50.714148+0000     5     255    163703    163448   127.668   127.039   0.0070402  0.00781727
2021-05-15T23:28:51.714315+0000     6     255    194398    194143    126.37   119.902  0.00619112  0.00789811
2021-05-15T23:28:52.714484+0000     7     255    224043    223788   124.857   115.801  0.00341964  0.00799471
2021-05-15T23:28:53.714655+0000     8     255    255431    255176   124.574   122.609  0.00551514  0.00801456
2021-05-15T23:28:54.714812+0000     9     255    285101    284846   123.608   115.898  0.00759185   0.0080782
2021-05-15T23:28:55.714980+0000    10     256    315547    315291   123.138   118.926 0.000783901  0.00810642
2021-05-15T23:28:56.715145+0000    11     255    345576    345321   122.606   117.305  0.00741045  0.00814538
2021-05-15T23:28:57.715383+0000 Total time run:       11.5881
Total reads made:     363943
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   122.682
Average IOPS:         31406
Stddev IOPS:          1636.57
Max IOPS:             35113
Min IOPS:             29645
Average Latency(s):   0.00814276
Max latency(s):       0.101097
Min latency(s):       0.000248758

[1;32mlocalhost.localdomain	[2021-05-15T16:28:58,337026990-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 547516


[1;33mlocalhost.localdomain	[2021-05-15T16:28:58,344442318-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:29:22,417109701-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 363.94k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:29:22,432021955-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:29:31,503869513-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 363.94k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:29:31,518841667-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:29:40,690326110-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 363.94k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:29:40,705084006-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:29:49,898280806-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 363.94k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:29:49,913475137-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:29:58,827364111-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 363.94k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:29:58,842378090-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:29:58,854055436-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T16:29:58,861146840-07:00][RUNNING][ROUND 1/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:29:58,868137201-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T16:29:58,884325687-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40708\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.58883\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b11955c4-1601-47e1-81dc-a63f1d7b53e7\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid b11955c4-1601-47e1-81dc-a63f1d7b53e7\nlast_changed 2021-05-15T16:30:25.987670-0700\ncreated 2021-05-15T16:30:25.987670-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40708/0,v1:10.10.1.2:40709/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.58883 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 c94177ee-163f-4746-8cd5-2919c7a186c4\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 42df6e64-a4b3-4f83-b330-0931473ab616\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 9bb33846-d952-4859-a46d-33befa748fc5\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42708\n  w/ user/pass: admin / 1a0083b6-07c6-4cee-b26a-504e8437b503\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 16:30:45 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40708
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.58883
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b11955c4-1601-47e1-81dc-a63f1d7b53e7
setting min_mon_release = octopus
epoch 0
fsid b11955c4-1601-47e1-81dc-a63f1d7b53e7
last_changed 2021-05-15T16:30:25.987670-0700
created 2021-05-15T16:30:25.987670-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40708/0,v1:10.10.1.2:40709/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.58883 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 c94177ee-163f-4746-8cd5-2919c7a186c4
0
start osd.0
add osd1 42df6e64-a4b3-4f83-b330-0931473ab616
1
start osd.1
add osd2 9bb33846-d952-4859-a46d-33befa748fc5
2
start osd.2


restful urls: https://10.10.1.2:42708
  w/ user/pass: admin / 1a0083b6-07c6-4cee-b26a-504e8437b503


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T16:30:00.802-0700 7f50d95051c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:30:00.802-0700 7f50d95051c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:30:00.818-0700 7fbce33911c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:30:00.818-0700 7fbce33911c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40708,v1:10.10.1.2:40709] --print /tmp/ceph_monmap.58883 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.58883 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.58883 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42708 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x555cfa74a000 @  0x7efc2946e680 0x7efc2948f824 0x7efc29c2a187 0x7efc29c32355 0x7efc29c2a708 0x7efc29c2a877 0x7efc29c2bc24 0x7efc29c43ec1 0x7efc29bb65f3 0x7efc29c17e97 0x7efc29c1fb1a 0x7efc29322d84 0x7efc2943e609 0x7efc29012293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.Re0hNEAsjW 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c94177ee-163f-4746-8cd5-2919c7a186c4 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCcWaBgB1IZFBAA0uJ4vE4UmmNs173an9Mx6w== --osd-uuid c94177ee-163f-4746-8cd5-2919c7a186c4 
2021-05-15T16:30:36.987-0700 7f7d58376f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:30:37.011-0700 7f7d58376f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:30:37.011-0700 7f7d58376f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56370f974000 @  0x7f7d58d3f680 0x7f7d58d60824 0x563703b8d447 0x563703b954b5 0x563703b8d9c8 0x563703b8db37 0x563703b8eee4 0x56370395fca1 0x563703b37423 0x5637039502a7 0x56370395553a 0x7f7d58892d84 0x7f7d58a17609 0x7f7d58580293
2021-05-15T16:30:37.307-0700 7f7d58376f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 42df6e64-a4b3-4f83-b330-0931473ab616 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55d4d1898000 @  0x7f60aef66680 0x7f60aef87824 0x55d4c5f28447 0x55d4c5f304b5 0x55d4c5f289c8 0x55d4c5f28b37 0x55d4c5f29ee4 0x55d4c5cfaca1 0x55d4c5ed2423 0x55d4c5ceb2a7 0x55d4c5cf053a 0x7f60aeab9d84 0x7f60aec3e609 0x7f60ae7a7293
2021-05-15T16:30:37.919-0700 7f60ae59df00 -1 Falling back to public interface
2021-05-15T16:30:38.175-0700 7f60ae59df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCdWaBgW7TPJBAAVSsLQIKEKlhNK/8MHzsZWg== --osd-uuid 42df6e64-a4b3-4f83-b330-0931473ab616 
2021-05-15T16:30:38.283-0700 7f86bbd7cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:30:38.307-0700 7f86bbd7cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:30:38.307-0700 7f86bbd7cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x555cfdd60000 @  0x7f86bc745680 0x7f86bc766824 0x555cf258d447 0x555cf25954b5 0x555cf258d9c8 0x555cf258db37 0x555cf258eee4 0x555cf235fca1 0x555cf2537423 0x555cf23502a7 0x555cf235553a 0x7f86bc298d84 0x7f86bc41d609 0x7f86bbf86293
2021-05-15T16:30:38.679-0700 7f86bbd7cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9bb33846-d952-4859-a46d-33befa748fc5 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55d046d74000 @  0x7feca1d13680 0x7feca1d34824 0x55d03c5cf447 0x55d03c5d74b5 0x55d03c5cf9c8 0x55d03c5cfb37 0x55d03c5d0ee4 0x55d03c3a1ca1 0x55d03c579423 0x55d03c3922a7 0x55d03c39753a 0x7feca1866d84 0x7feca19eb609 0x7feca1554293
2021-05-15T16:30:39.351-0700 7feca134af00 -1 Falling back to public interface
2021-05-15T16:30:39.603-0700 7feca134af00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCfWaBgnlmsAhAAgwn92m86WTXtL2aITBgwWg== --osd-uuid 9bb33846-d952-4859-a46d-33befa748fc5 
2021-05-15T16:30:39.707-0700 7f2d87aaff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:30:39.727-0700 7f2d87aaff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:30:39.727-0700 7f2d87aaff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55588560c000 @  0x7f2d88478680 0x7f2d88499824 0x55587ad3e447 0x55587ad464b5 0x55587ad3e9c8 0x55587ad3eb37 0x55587ad3fee4 0x55587ab10ca1 0x55587ace8423 0x55587ab012a7 0x55587ab0653a 0x7f2d87fcbd84 0x7f2d88150609 0x7f2d87cb9293
2021-05-15T16:30:40.031-0700 7f2d87aaff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5651122c8000 @  0x7f1f0daee680 0x7f1f0db0f824 0x5651066e0447 0x5651066e84b5 0x5651066e09c8 0x5651066e0b37 0x5651066e1ee4 0x5651064b2ca1 0x56510668a423 0x5651064a32a7 0x5651064a853a 0x7f1f0d641d84 0x7f1f0d7c6609 0x7f1f0d32f293
2021-05-15T16:30:40.687-0700 7f1f0d125f00 -1 Falling back to public interface
2021-05-15T16:30:40.951-0700 7f1f0d125f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T16:30:45,070201918-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T16:30:45,089570586-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T16:30:45,171831611-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:30:45,178226553-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T16:30:49,123623853-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:30:49,130036843-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T16:30:53,348678811-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:30:53,354658931-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T16:30:57,421714661-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:30:57,431399014-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T16:31:05,247587926-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:31:05,254163075-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T16:31:09,736670241-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:31:09,743113606-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T16:31:14,207821801-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:31:14,214840227-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T16:31:18,814243358-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:31:18,820627656-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:31:23,833634665-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:31:23,840222037-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:31:28,877342420-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:31:28,883870050-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T16:31:33,674068642-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:31:33,680693517-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 1 pgp_num_target 64 autoscale_mode on last_change 23 lfor 0/0/23 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T16:31:37,638294376-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:31:37,644680635-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  190 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  190 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  0.98  154      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   69 KiB   0 B   0 B   0 B  100 GiB     0  1.09  151      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  0.93  143      up          osd.2  
                       TOTAL  300 GiB  192 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.93/1.09  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T16:31:41,447960465-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:32:05,497485021-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:32:14,672987956-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:32:23,702913716-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:32:32,729530538-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:32:42,154783916-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   248 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   peering
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:32:42,171396072-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:32:51,346513616-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:32:51,361986073-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:33:00,411853177-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:33:00,426849092-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:33:09,392942909-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:33:09,407945886-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:33:18,726370577-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:33:18,741550850-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:33:18,753635946-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:33:18,760761839-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:33:18,776486131-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=560449
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T16:33:18,789737070-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-15T16:33:18,829542531-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:33:18,835835726-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:33:20.369+0000 ffffbc2a6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:33:21.177+0000 ffffbc2a6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:33:21.181+0000 ffffbc2a6010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:33:21.201569+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-15T23:33:21.201612+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-15T23:33:21.206532+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:33:21.206532+0000     0       0         0         0         0         0           -           0
2021-05-15T23:33:22.206711+0000     1     255     19300     19045   297.564   297.578  0.00443184   0.0130221
2021-05-15T23:33:23.206866+0000     2     255     38158     37903   296.087   294.656    0.109137   0.0131336
2021-05-15T23:33:24.207031+0000     3     255     56826     56571   294.604   291.688  0.00223105   0.0133456
2021-05-15T23:33:25.207215+0000     4     255     74785     74530   291.093   280.609  0.00354774   0.0135509
2021-05-15T23:33:26.207376+0000     5     255     92650     92395   288.693   279.141  0.00139701   0.0136734
2021-05-15T23:33:27.207527+0000     6     255    109237    108982   283.766   259.172    0.136752   0.0139342
2021-05-15T23:33:28.207679+0000     7     255    126891    126636   282.629   275.844  0.00331027   0.0140208
2021-05-15T23:33:29.207840+0000     8     255    144048    143793   280.804   268.078  0.00637929   0.0141577
2021-05-15T23:33:30.208115+0000     9     255    161844    161589   280.491   278.062  0.00339503   0.0141596
2021-05-15T23:33:31.208296+0000    10     255    179202    178947   279.559   271.219  0.00110642   0.0142142
2021-05-15T23:33:32.208450+0000    11     256    197105    196849    279.57   279.719  0.00227414   0.0142248
2021-05-15T23:33:33.208610+0000    12     255    215062    214807   279.651   280.594  0.00102689   0.0142269
2021-05-15T23:33:34.208759+0000    13     255    233329    233074   280.092   285.422    0.124514   0.0142058
2021-05-15T23:33:35.208911+0000    14     255    250938    250683   279.735   275.141  0.00202839   0.0142308
2021-05-15T23:33:36.209077+0000    15     255    267261    267006   278.086   255.047   0.0130331    0.014367
2021-05-15T23:33:37.209343+0000    16     255    284327    284072   277.368   266.656  0.00126984   0.0143656
2021-05-15T23:33:38.209488+0000    17     255    299758    299503   275.233   241.109   0.0117514   0.0145174
2021-05-15T23:33:39.209977+0000    18     255    315148    314893   273.294   240.469  0.00995831   0.0146163
2021-05-15T23:33:40.210161+0000    19     255    332692    332437   273.335   274.125  0.00122383   0.0145797
2021-05-15T23:33:41.210337+0000 min lat: 0.000691494 max lat: 0.204145 avg lat: 0.0146565
2021-05-15T23:33:41.210337+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:33:41.210337+0000    20     223    348110    347887   271.737   241.406    0.186869   0.0146565
2021-05-15T23:33:42.210584+0000 Total time run:         20.0979
Total writes made:      348110
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     270.637
Stddev Bandwidth:       16.9205
Max bandwidth (MB/sec): 297.578
Min bandwidth (MB/sec): 240.469
Average IOPS:           17320
Stddev IOPS:            1082.91
Max IOPS:               19045
Min IOPS:               15390
Average Latency(s):     0.0147321
Stddev Latency(s):      0.0335057
Max latency(s):         0.204145
Min latency(s):         0.000691494

[1;32mlocalhost.localdomain	[2021-05-15T16:33:42,884373872-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 560449


[1;33mlocalhost.localdomain	[2021-05-15T16:33:42,892439575-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:34:07,213749565-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.11k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:34:07,229047573-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:34:16,411448145-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.11k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:34:16,428022034-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:34:25,792187757-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.11k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:34:25,807498750-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:34:34,875048844-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.11k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:34:34,890309936-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:34:43,976055770-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.11k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:34:43,990972537-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:34:44,002795906-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:34:44,010026410-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:34:44,026074737-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=564045
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T16:34:44,039567397-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-15T16:34:44,078123489-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:34:44,084534575-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3e5c9dca-f83a-49cc-9966-387616471943', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3e5c9dca-f83a-49cc-9966-387616471943 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zLf6eD:/tmp/ceph-asok.zLf6eD -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:34:45.535+0000 ffffba6c0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:34:46.319+0000 ffffba6c0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:34:46.319+0000 ffffba6c0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:34:46.343167+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:34:46.343167+0000     0       0         0         0         0         0           -           0
2021-05-15T23:34:47.343337+0000     1     256     26708     26452   413.184   413.312 0.000415297  0.00955449
2021-05-15T23:34:48.343511+0000     2     256     50040     49784   388.843   364.562 0.000317227  0.00964515
2021-05-15T23:34:49.343678+0000     3     255     75328     75073    390.92   395.141  0.00917409   0.0101976
2021-05-15T23:34:50.343890+0000     4     255    103331    103076   402.554   437.547   0.0088623  0.00990825
2021-05-15T23:34:51.344066+0000     5     255    129761    129506   404.622   412.969  0.00286287  0.00985351
2021-05-15T23:34:52.344261+0000     6     255    156784    156529   407.544   422.234  0.00833774  0.00979304
2021-05-15T23:34:53.344443+0000     7     255    181930    181675   405.442   392.906  0.00969015  0.00984456
2021-05-15T23:34:54.344647+0000     8     255    206077    205822   401.915   377.297  0.00873742  0.00993272
2021-05-15T23:34:55.344841+0000     9     255    233479    233224   404.821   428.156   0.0278557  0.00985329
2021-05-15T23:34:56.345021+0000    10     255    260902    260647    407.18   428.484  0.00883978  0.00980568
2021-05-15T23:34:57.345196+0000    11     255    285415    285160   404.977   383.016  0.00554058  0.00985789
2021-05-15T23:34:58.345369+0000    12     255    310044    309789   403.292   384.828  0.00284082   0.0098977
2021-05-15T23:34:59.345555+0000    13     256    336775    336519   404.391   417.656  0.00843647  0.00987512
2021-05-15T23:35:00.345785+0000 Total time run:       13.4057
Total reads made:     348110
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   405.74
Average IOPS:         25967
Stddev IOPS:          1462.87
Max IOPS:             28003
Min IOPS:             23332
Average Latency(s):   0.00984441
Max latency(s):       0.149412
Min latency(s):       0.000282781

[1;32mlocalhost.localdomain	[2021-05-15T16:35:00,996028372-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 564045


[1;33mlocalhost.localdomain	[2021-05-15T16:35:01,003499888-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:35:25,131633267-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.11k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:35:25,146963593-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:35:34,459808405-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.11k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:35:34,475117289-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:35:43,595338890-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.11k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:35:43,612458565-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:35:52,791874081-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.11k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:35:52,806849337-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:36:01,881666854-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.11k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:36:01,896854301-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:36:01,908828289-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T16:36:01,913284969-07:00][RUNNING][ROUND 2/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:36:01,920221474-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T16:36:01,936509453-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40154\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.59975\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a1190fef-4e38-4f8a-8cc4-f774a6be47ad\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid a1190fef-4e38-4f8a-8cc4-f774a6be47ad\nlast_changed 2021-05-15T16:36:29.284049-0700\ncreated 2021-05-15T16:36:29.284049-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40154/0,v1:10.10.1.2:40155/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.59975 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 23fb3bd4-5e75-4db1-9aeb-58f9bb4fc3c4\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 f4347df5-235c-48e5-8ec3-8cadb653d5fc\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 8bbe65aa-eae3-44f8-8b0a-598053ffb945\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42154\n  w/ user/pass: admin / 24c767f5-0db8-4a38-a4c3-d2be22d5700d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:36:48 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40154
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.59975
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a1190fef-4e38-4f8a-8cc4-f774a6be47ad
setting min_mon_release = octopus
epoch 0
fsid a1190fef-4e38-4f8a-8cc4-f774a6be47ad
last_changed 2021-05-15T16:36:29.284049-0700
created 2021-05-15T16:36:29.284049-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40154/0,v1:10.10.1.2:40155/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.59975 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 23fb3bd4-5e75-4db1-9aeb-58f9bb4fc3c4
0
start osd.0
add osd1 f4347df5-235c-48e5-8ec3-8cadb653d5fc
1
start osd.1
add osd2 8bbe65aa-eae3-44f8-8b0a-598053ffb945
2
start osd.2


restful urls: https://10.10.1.2:42154
  w/ user/pass: admin / 24c767f5-0db8-4a38-a4c3-d2be22d5700d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T16:36:03.854-0700 7f8220e811c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:36:03.854-0700 7f8220e811c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:36:03.870-0700 7f54891f51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:36:03.870-0700 7f54891f51c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40154,v1:10.10.1.2:40155] --print /tmp/ceph_monmap.59975 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.59975 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.59975 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42154 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5587126aa000 @  0x7f2a417fb680 0x7f2a4181c824 0x7f2a41fb7187 0x7f2a41fbf355 0x7f2a41fb7708 0x7f2a41fb7877 0x7f2a41fb8c24 0x7f2a41fd0ec1 0x7f2a41f435f3 0x7f2a41fa4e97 0x7f2a41facb1a 0x7f2a416afd84 0x7f2a417cb609 0x7f2a4139f293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.sF05ED8ZY2 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 23fb3bd4-5e75-4db1-9aeb-58f9bb4fc3c4 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAHW6Bghae0LhAAA9NQA5HYh3iZQORdTtSEMQ== --osd-uuid 23fb3bd4-5e75-4db1-9aeb-58f9bb4fc3c4 
2021-05-15T16:36:40.426-0700 7f1444115f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:36:40.446-0700 7f1444115f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:36:40.446-0700 7f1444115f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55e2f4f28000 @  0x7f1444ade680 0x7f1444aff824 0x55e2ea270447 0x55e2ea2784b5 0x55e2ea2709c8 0x55e2ea270b37 0x55e2ea271ee4 0x55e2ea042ca1 0x55e2ea21a423 0x55e2ea0332a7 0x55e2ea03853a 0x7f1444631d84 0x7f14447b6609 0x7f144431f293
2021-05-15T16:36:40.738-0700 7f1444115f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f4347df5-235c-48e5-8ec3-8cadb653d5fc -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5609ddd5e000 @  0x7f176ee46680 0x7f176ee67824 0x5609d39e0447 0x5609d39e84b5 0x5609d39e09c8 0x5609d39e0b37 0x5609d39e1ee4 0x5609d37b2ca1 0x5609d398a423 0x5609d37a32a7 0x5609d37a853a 0x7f176e999d84 0x7f176eb1e609 0x7f176e687293
2021-05-15T16:36:41.350-0700 7f176e47df00 -1 Falling back to public interface
2021-05-15T16:36:41.602-0700 7f176e47df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAJW6BgT7HvARAAIOLYsyNhePhkKMwwqlet5w== --osd-uuid f4347df5-235c-48e5-8ec3-8cadb653d5fc 
2021-05-15T16:36:41.682-0700 7f4fa3a73f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:36:41.702-0700 7f4fa3a73f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:36:41.702-0700 7f4fa3a73f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x558215004000 @  0x7f4fa443c680 0x7f4fa445d824 0x5582097a5447 0x5582097ad4b5 0x5582097a59c8 0x5582097a5b37 0x5582097a6ee4 0x558209577ca1 0x55820974f423 0x5582095682a7 0x55820956d53a 0x7f4fa3f8fd84 0x7f4fa4114609 0x7f4fa3c7d293
2021-05-15T16:36:42.006-0700 7f4fa3a73f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8bbe65aa-eae3-44f8-8b0a-598053ffb945 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5572cefea000 @  0x7f9ceef17680 0x7f9ceef38824 0x5572c5033447 0x5572c503b4b5 0x5572c50339c8 0x5572c5033b37 0x5572c5034ee4 0x5572c4e05ca1 0x5572c4fdd423 0x5572c4df62a7 0x5572c4dfb53a 0x7f9ceea6ad84 0x7f9ceebef609 0x7f9cee758293
2021-05-15T16:36:42.642-0700 7f9cee54ef00 -1 Falling back to public interface
2021-05-15T16:36:42.894-0700 7f9cee54ef00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAKW6BggoA7ExAAE73JGHjvyo7jU94Y6mHeEw== --osd-uuid 8bbe65aa-eae3-44f8-8b0a-598053ffb945 
2021-05-15T16:36:42.990-0700 7ff96aa87f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:36:43.010-0700 7ff96aa87f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:36:43.010-0700 7ff96aa87f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55595cdc8000 @  0x7ff96b450680 0x7ff96b471824 0x5559516b9447 0x5559516c14b5 0x5559516b99c8 0x5559516b9b37 0x5559516baee4 0x55595148bca1 0x555951663423 0x55595147c2a7 0x55595148153a 0x7ff96afa3d84 0x7ff96b128609 0x7ff96ac91293
2021-05-15T16:36:43.338-0700 7ff96aa87f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5562c2070000 @  0x7fc21de37680 0x7fc21de58824 0x5562b6226447 0x5562b622e4b5 0x5562b62269c8 0x5562b6226b37 0x5562b6227ee4 0x5562b5ff8ca1 0x5562b61d0423 0x5562b5fe92a7 0x5562b5fee53a 0x7fc21d98ad84 0x7fc21db0f609 0x7fc21d678293
2021-05-15T16:36:43.914-0700 7fc21d46ef00 -1 Falling back to public interface
2021-05-15T16:36:44.174-0700 7fc21d46ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T16:36:48,340909820-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T16:36:48,360510662-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T16:36:48,444668369-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:36:48,451289802-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T16:36:52,366477491-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:36:52,373162084-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T16:36:56,287229570-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:36:56,293842541-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T16:37:00,156369532-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:37:00,162642668-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T16:37:08,267447150-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:37:08,274183830-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T16:37:13,197186083-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:37:13,203583182-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T16:37:18,010524063-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:37:18,017783950-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T16:37:22,211820622-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:37:22,218121120-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:37:27,167890168-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:37:27,174439015-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:37:32,085123065-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:37:32,091396918-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T16:37:36,277224395-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:37:36,283719127-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T16:37:40,111649488-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:37:40,118120816-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  190 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  190 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  0.98  152      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   69 KiB   0 B   0 B   0 B  100 GiB     0  1.09  153      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  0.93  143      up          osd.2  
                       TOTAL  300 GiB  192 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.93/1.09  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T16:37:43,981340937-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:38:08,019749642-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:38:17,076177172-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:38:26,106213164-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:38:35,445724200-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:38:44,545861457-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:38:44,561362502-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:38:53,563744456-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:38:53,579316731-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:39:02,693818713-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:39:02,708997548-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:39:11,950993161-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:39:11,966409197-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:39:20,919967272-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:39:20,935402119-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:39:20,947611125-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:39:20,954840424-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:39:20,970915242-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=577095
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T16:39:20,984236645-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-15T16:39:21,023806665-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:39:21,030021773-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:39:22.544+0000 ffffb462b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:39:23.352+0000 ffffb462b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:39:23.352+0000 ffffb462b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:39:23.371593+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-15T23:39:23.371639+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-15T23:39:23.376219+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:39:23.376219+0000     0       0         0         0         0         0           -           0
2021-05-15T23:39:24.376394+0000     1     255     20737     20482   320.014   320.031  0.00534974   0.0120986
2021-05-15T23:39:25.376561+0000     2     255     40432     40177   313.848   307.734   0.0106775   0.0126661
2021-05-15T23:39:26.376846+0000     3     255     60217     59962   312.249   309.141    0.012982    0.012757
2021-05-15T23:39:27.377020+0000     4     255     79684     79429   310.217   304.172   0.0117502   0.0128553
2021-05-15T23:39:28.377188+0000     5     255     99339     99084   309.585   307.109   0.0044313   0.0128412
2021-05-15T23:39:29.377349+0000     6     255    118969    118714   309.099   306.719   0.0105617   0.0128839
2021-05-15T23:39:30.377512+0000     7     255    137680    137425   306.701   292.359   0.0120746   0.0130154
2021-05-15T23:39:31.377673+0000     8     255    157266    157011   306.611   306.031   0.0103476   0.0130236
2021-05-15T23:39:32.377842+0000     9     255    176272    176017   305.534   296.969   0.0143142   0.0130708
2021-05-15T23:39:33.378007+0000    10     255    194995    194740    304.23   292.547   0.0138164   0.0131289
2021-05-15T23:39:34.378179+0000    11     255    214350    214095   304.061   302.422   0.0133431   0.0131377
2021-05-15T23:39:35.378352+0000    12     255    233705    233450    303.92   302.422   0.0118833   0.0131449
2021-05-15T23:39:36.378520+0000    13     255    252420    252165   303.032   292.422    0.011487   0.0131856
2021-05-15T23:39:37.378687+0000    14     255    272164    271909   303.419     308.5   0.0133411    0.013169
2021-05-15T23:39:38.378846+0000    15     255    292048    291793     303.9   310.688  0.00229479   0.0131221
2021-05-15T23:39:39.379005+0000    16     255    310780    310525   303.197   292.688  0.00155591   0.0131596
2021-05-15T23:39:40.379165+0000    17     255    329593    329338    302.65   293.953  0.00459622   0.0131934
2021-05-15T23:39:41.379325+0000    18     255    348497    348242   302.243   295.375  0.00578275   0.0132059
2021-05-15T23:39:42.379488+0000    19     255    368094    367839   302.449   306.203  0.00185048   0.0132002
2021-05-15T23:39:43.379644+0000 min lat: 0.00076074 max lat: 0.116321 avg lat: 0.0132208
2021-05-15T23:39:43.379644+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:39:43.379644+0000    20     172    386985    386813   302.148   296.469   0.0255577   0.0132208
2021-05-15T23:39:44.379872+0000 Total time run:         20.0443
Total writes made:      386985
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     301.663
Stddev Bandwidth:       7.74127
Max bandwidth (MB/sec): 320.031
Min bandwidth (MB/sec): 292.359
Average IOPS:           19306
Stddev IOPS:            495.441
Max IOPS:               20482
Min IOPS:               18711
Average Latency(s):     0.0132327
Stddev Latency(s):      0.0149067
Max latency(s):         0.116321
Min latency(s):         0.00076074

[1;32mlocalhost.localdomain	[2021-05-15T16:39:44,979222315-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 577095


[1;33mlocalhost.localdomain	[2021-05-15T16:39:44,987173871-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:40:09,169175061-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 386.99k objects, 5.9 GiB
    usage:   12 GiB used, 288 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:40:09,185061019-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:40:18,237174838-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 386.99k objects, 5.9 GiB
    usage:   12 GiB used, 288 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:40:18,252975176-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:40:27,321548866-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 386.99k objects, 5.9 GiB
    usage:   12 GiB used, 288 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:40:27,337598197-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:40:37,004998187-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 386.99k objects, 5.9 GiB
    usage:   12 GiB used, 288 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:40:37,022601340-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:40:46,116118563-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 386.99k objects, 5.9 GiB
    usage:   12 GiB used, 288 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:40:46,131733902-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:40:46,144229891-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:40:46,151681037-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:40:46,168237639-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=580667
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T16:40:46,182220016-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-15T16:40:46,221211159-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:40:46,227465462-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a8a9ad8-bc05-43cc-a8ce-166d89b88ec4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.86CjhZ:/tmp/ceph-asok.86CjhZ -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:40:47.805+0000 ffffa5a18010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:40:48.617+0000 ffffa5a18010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:40:48.617+0000 ffffa5a18010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:40:48.644999+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:40:48.644999+0000     0       0         0         0         0         0           -           0
2021-05-15T23:40:49.645151+0000     1     255     27403     27148   424.068   424.188  0.00929156  0.00935255
2021-05-15T23:40:50.645338+0000     2     255     52712     52457   409.724   395.453  0.00932693  0.00971592
2021-05-15T23:40:51.645486+0000     3     255     75221     74966   390.368   351.703  0.00863165   0.0102117
2021-05-15T23:40:52.645686+0000     4     255    101250    100995   394.431   406.703  0.00886956    0.010112
2021-05-15T23:40:53.645881+0000     5     255    127401    127146   397.251   408.609  0.00938369   0.0100433
2021-05-15T23:40:54.646061+0000     6     255    153547    153292   399.119   408.531  0.00294208   0.0099922
2021-05-15T23:40:55.646234+0000     7     255    178975    178720   398.851   397.312  0.00879089   0.0100075
2021-05-15T23:40:56.646428+0000     8     255    205173    204918   400.153   409.344  0.00940184  0.00997586
2021-05-15T23:40:57.646606+0000     9     256    231766    231510    401.85     415.5  0.00894088  0.00993482
2021-05-15T23:40:58.646882+0000    10     255    257293    257038   401.541   398.875  0.00862718  0.00994354
2021-05-15T23:40:59.647147+0000    11     256    282374    282118   400.653   391.875 0.000316325  0.00992856
2021-05-15T23:41:00.647320+0000    12     255    308185    307930   400.869   403.312   0.0079328  0.00996083
2021-05-15T23:41:01.647513+0000    13     255    333580    333325    400.55   396.797  0.00850561  0.00996986
2021-05-15T23:41:02.647682+0000    14     255    360006    359751   401.427   412.906  0.00867427  0.00994845
2021-05-15T23:41:03.647887+0000    15     255    385797    385542   401.525   402.984   0.0089578  0.00994616
2021-05-15T23:41:04.648137+0000 Total time run:       15.0509
Total reads made:     386985
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   401.747
Average IOPS:         25711
Stddev IOPS:          1039.06
Max IOPS:             27148
Min IOPS:             22509
Average Latency(s):   0.0099423
Max latency(s):       0.12021
Min latency(s):       0.000264972

[1;32mlocalhost.localdomain	[2021-05-15T16:41:05,276216206-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 580667


[1;33mlocalhost.localdomain	[2021-05-15T16:41:05,283874803-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:41:29,350449615-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 386.99k objects, 5.9 GiB
    usage:   12 GiB used, 288 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:41:29,366289682-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:41:38,454219085-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 386.99k objects, 5.9 GiB
    usage:   12 GiB used, 288 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:41:38,469913238-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:41:48,091464786-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 386.99k objects, 5.9 GiB
    usage:   12 GiB used, 288 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:41:48,106789369-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:41:57,178251654-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 386.99k objects, 5.9 GiB
    usage:   12 GiB used, 288 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:41:57,194054540-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:42:06,354168387-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 386.99k objects, 5.9 GiB
    usage:   12 GiB used, 288 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:42:06,370037898-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:42:06,382551594-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T16:42:06,386932549-07:00][RUNNING][ROUND 3/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:42:06,394100519-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T16:42:06,410659523-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40468\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.61058\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a0cafa21-d886-4b2e-b8f3-45711f3d07ff\nsetting min_mon_release = octopus\nepoch 0\nfsid a0cafa21-d886-4b2e-b8f3-45711f3d07ff\nlast_changed 2021-05-15T16:42:33.863705-0700\ncreated 2021-05-15T16:42:33.863705-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40468/0,v1:10.10.1.2:40469/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.61058 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 ab5cf4cb-3f6f-4886-8b98-a929e7337644\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 74635b3c-3ebc-4ce3-b0a9-259e7cc4044a\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 7b2d898d-041d-4e2d-9c37-ce3cf691aae8\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42468\n'
10.10.1.2: b'  w/ user/pass: admin / 84248bb0-4231-49b9-8ba8-da3319044d96\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:42:53 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40468
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.61058
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a0cafa21-d886-4b2e-b8f3-45711f3d07ff
setting min_mon_release = octopus
epoch 0
fsid a0cafa21-d886-4b2e-b8f3-45711f3d07ff
last_changed 2021-05-15T16:42:33.863705-0700
created 2021-05-15T16:42:33.863705-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40468/0,v1:10.10.1.2:40469/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.61058 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 ab5cf4cb-3f6f-4886-8b98-a929e7337644
0
start osd.0
add osd1 74635b3c-3ebc-4ce3-b0a9-259e7cc4044a
1
start osd.1
add osd2 7b2d898d-041d-4e2d-9c37-ce3cf691aae8
2
start osd.2


restful urls: https://10.10.1.2:42468
  w/ user/pass: admin / 84248bb0-4231-49b9-8ba8-da3319044d96


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T16:42:08.313-0700 7f4585d4a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:42:08.313-0700 7f4585d4a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:42:08.329-0700 7fa0735f81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:42:08.333-0700 7fa0735f81c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40468,v1:10.10.1.2:40469] --print /tmp/ceph_monmap.61058 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.61058 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.61058 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42468 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x564ef731e000 @  0x7fc5fcba2680 0x7fc5fcbc3824 0x7fc5fd35e187 0x7fc5fd366355 0x7fc5fd35e708 0x7fc5fd35e877 0x7fc5fd35fc24 0x7fc5fd377ec1 0x7fc5fd2ea5f3 0x7fc5fd34be97 0x7fc5fd353b1a 0x7fc5fca56d84 0x7fc5fcb72609 0x7fc5fc746293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.pajgxV7b4s 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ab5cf4cb-3f6f-4886-8b98-a929e7337644 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB0XKBglnaLOxAAZTOfvPSy8k8kNjAA64Z/tg== --osd-uuid ab5cf4cb-3f6f-4886-8b98-a929e7337644 
2021-05-15T16:42:45.637-0700 7f84e6008f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:42:45.657-0700 7f84e6008f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:42:45.657-0700 7f84e6008f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x563c380a4000 @  0x7f84e69d1680 0x7f84e69f2824 0x563c2c969447 0x563c2c9714b5 0x563c2c9699c8 0x563c2c969b37 0x563c2c96aee4 0x563c2c73bca1 0x563c2c913423 0x563c2c72c2a7 0x563c2c73153a 0x7f84e6524d84 0x7f84e66a9609 0x7f84e6212293
2021-05-15T16:42:46.041-0700 7f84e6008f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 74635b3c-3ebc-4ce3-b0a9-259e7cc4044a -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x563dfeab0000 @  0x7fac338df680 0x7fac33900824 0x563df2f63447 0x563df2f6b4b5 0x563df2f639c8 0x563df2f63b37 0x563df2f64ee4 0x563df2d35ca1 0x563df2f0d423 0x563df2d262a7 0x563df2d2b53a 0x7fac33432d84 0x7fac335b7609 0x7fac33120293
2021-05-15T16:42:46.649-0700 7fac32f16f00 -1 Falling back to public interface
2021-05-15T16:42:46.901-0700 7fac32f16f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB2XKBgOhM0FRAAz/N8oxX6tntMn166/GghBQ== --osd-uuid 74635b3c-3ebc-4ce3-b0a9-259e7cc4044a 
2021-05-15T16:42:47.025-0700 7fbf7388bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:42:47.045-0700 7fbf7388bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:42:47.045-0700 7fbf7388bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x561656c56000 @  0x7fbf74254680 0x7fbf74275824 0x56164ae17447 0x56164ae1f4b5 0x56164ae179c8 0x56164ae17b37 0x56164ae18ee4 0x56164abe9ca1 0x56164adc1423 0x56164abda2a7 0x56164abdf53a 0x7fbf73da7d84 0x7fbf73f2c609 0x7fbf73a95293
2021-05-15T16:42:47.353-0700 7fbf7388bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7b2d898d-041d-4e2d-9c37-ce3cf691aae8 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55d20d4a2000 @  0x7f632bd6e680 0x7f632bd8f824 0x55d20271b447 0x55d2027234b5 0x55d20271b9c8 0x55d20271bb37 0x55d20271cee4 0x55d2024edca1 0x55d2026c5423 0x55d2024de2a7 0x55d2024e353a 0x7f632b8c1d84 0x7f632ba46609 0x7f632b5af293
2021-05-15T16:42:48.001-0700 7f632b3a5f00 -1 Falling back to public interface
2021-05-15T16:42:48.261-0700 7f632b3a5f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB3XKBgH01mKRAA0tueU9Y/L3+SbVGKApyj7Q== --osd-uuid 7b2d898d-041d-4e2d-9c37-ce3cf691aae8 
2021-05-15T16:42:48.361-0700 7f6724ec5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:42:48.381-0700 7f6724ec5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:42:48.381-0700 7f6724ec5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x558c20bd6000 @  0x7f672588e680 0x7f67258af824 0x558c15b0a447 0x558c15b124b5 0x558c15b0a9c8 0x558c15b0ab37 0x558c15b0bee4 0x558c158dcca1 0x558c15ab4423 0x558c158cd2a7 0x558c158d253a 0x7f67253e1d84 0x7f6725566609 0x7f67250cf293
2021-05-15T16:42:48.705-0700 7f6724ec5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5624d0d88000 @  0x7ffbdd652680 0x7ffbdd673824 0x5624c5112447 0x5624c511a4b5 0x5624c51129c8 0x5624c5112b37 0x5624c5113ee4 0x5624c4ee4ca1 0x5624c50bc423 0x5624c4ed52a7 0x5624c4eda53a 0x7ffbdd1a5d84 0x7ffbdd32a609 0x7ffbdce93293
2021-05-15T16:42:49.317-0700 7ffbdcc89f00 -1 Falling back to public interface
2021-05-15T16:42:49.581-0700 7ffbdcc89f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T16:42:53,703573792-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T16:42:53,723333192-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T16:42:53,804677718-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:42:53,811024472-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T16:42:57,691578851-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:42:57,697812938-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T16:43:01,589772098-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:43:01,596827412-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T16:43:05,479026626-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:43:05,485222591-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T16:43:13,251999778-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:43:13,258403585-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T16:43:17,698743264-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:43:17,704942574-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T16:43:22,149928941-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:43:22,156425042-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T16:43:26,579730141-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:43:26,586124481-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:43:31,401219615-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:43:31,407616056-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:43:35,694129574-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:43:35,700494854-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T16:43:40,294702606-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:43:40,300821037-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T16:43:43,997514628-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:43:44,003910317-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  168 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  168 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.11   90      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.91   89      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.98   80      up          osd.2  
                       TOTAL  300 GiB  170 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.91/1.11  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T16:43:47,969386178-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:44:12,007659461-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:44:21,091880256-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:44:30,187420462-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:44:39,358102984-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:44:48,631997214-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:44:48,647882471-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:44:57,590509537-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:44:57,606366019-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:45:06,705265720-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:45:06,721088580-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:45:16,116771493-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:45:16,132460802-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:45:25,224685727-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:45:25,240403584-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:45:25,252726551-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:45:25,260143653-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:45:25,276937477-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=593832
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T16:45:25,291258678-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-15T16:45:25,331418392-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:45:25,338000856-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:45:26.945+0000 ffffab584010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:45:27.737+0000 ffffab584010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:45:27.741+0000 ffffab584010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:45:27.757223+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-15T23:45:27.757265+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-15T23:45:27.761824+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:45:27.761824+0000     0       0         0         0         0         0           -           0
2021-05-15T23:45:28.762006+0000     1     255     18567     18312   286.107   286.125    0.115928   0.0132336
2021-05-15T23:45:29.762164+0000     2     255     35759     35504   277.344   268.625  0.00643391   0.0140279
2021-05-15T23:45:30.762319+0000     3     255     53356     53101   276.533   274.953  0.00552272   0.0141842
2021-05-15T23:45:31.762465+0000     4     255     70890     70635   275.882   273.969  0.00162016   0.0142879
2021-05-15T23:45:32.762621+0000     5     255     88020     87765   274.228   267.656  0.00259501   0.0144221
2021-05-15T23:45:33.762781+0000     6     255    104826    104571   272.282   262.594  0.00101092   0.0145592
2021-05-15T23:45:34.762930+0000     7     255    122971    122716   273.881   283.516  0.00136685   0.0144922
2021-05-15T23:45:35.763088+0000     8     255    140128    139873    273.15   268.078     0.12038    0.014539
2021-05-15T23:45:36.763252+0000     9     255    157110    156855   272.278   265.344  0.00363988   0.0146167
2021-05-15T23:45:37.763511+0000    10     255    174262    174007   271.843       268  0.00136056   0.0146149
2021-05-15T23:45:38.763657+0000    11     255    191521    191266   271.642   269.672  0.00193551    0.014653
2021-05-15T23:45:39.763819+0000    12     255    209887    209632   272.916   286.969  0.00148515   0.0145947
2021-05-15T23:45:40.763979+0000    13     255    227753    227498   273.392   279.156  0.00563733   0.0145983
2021-05-15T23:45:41.764149+0000    14     255    245865    245610   274.075       283    0.108511   0.0145414
2021-05-15T23:45:42.764303+0000    15     255    262878    262623   273.523   265.828  0.00125514   0.0145619
2021-05-15T23:45:43.764506+0000    16     255    279327    279072   272.488   257.016  0.00787962   0.0146252
2021-05-15T23:45:44.764668+0000    17     255    295588    295333   271.402   254.078  0.00229079   0.0146807
2021-05-15T23:45:45.764838+0000    18     255    311718    311463   270.324   252.031  0.00118695   0.0147419
2021-05-15T23:45:46.764996+0000    19     255    327220    326965   268.842   242.219  0.00896401   0.0148343
2021-05-15T23:45:47.765163+0000 min lat: 0.000729758 max lat: 0.147721 avg lat: 0.0148735
2021-05-15T23:45:47.765163+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:45:47.765163+0000    20     228    343380    343152   268.044   252.922    0.125152   0.0148735
2021-05-15T23:45:48.765392+0000 Total time run:         20.0946
Total writes made:      343380
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     267.002
Stddev Bandwidth:       12.2864
Max bandwidth (MB/sec): 286.969
Min bandwidth (MB/sec): 242.219
Average IOPS:           17088
Stddev IOPS:            786.329
Max IOPS:               18366
Min IOPS:               15502
Average Latency(s):     0.0149364
Stddev Latency(s):      0.0347603
Max latency(s):         0.147721
Min latency(s):         0.000729758

[1;32mlocalhost.localdomain	[2021-05-15T16:45:49,410218441-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 593832


[1;33mlocalhost.localdomain	[2021-05-15T16:45:49,417909664-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:46:13,700680056-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.38k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:46:13,718539504-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:46:22,823185988-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.38k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:46:22,838988588-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:46:31,934626088-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.38k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:46:31,950238563-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:46:41,066625605-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.38k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:46:41,082381636-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:46:50,465697838-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.38k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:46:50,481515513-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:46:50,493967673-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:46:50,501388102-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:46:50,517976011-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=597338
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T16:46:50,532026458-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-15T16:46:50,573275395-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:46:50,581696898-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df861d9-c1a0-48bf-95f6-31323c629867', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df861d9-c1a0-48bf-95f6-31323c629867 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IpcKSn:/tmp/ceph-asok.IpcKSn -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:46:52.074+0000 ffffaa9ab010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:46:52.866+0000 ffffaa9ab010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:46:52.866+0000 ffffaa9ab010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:46:52.890667+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:46:52.890667+0000     0       0         0         0         0         0           -           0
2021-05-15T23:46:53.890835+0000     1     255     27596     27341   427.073   427.203  0.00842423  0.00929453
2021-05-15T23:46:54.891048+0000     2     255     51662     51407   401.513   376.031  0.00894553   0.0099177
2021-05-15T23:46:55.891243+0000     3     255     81294     81039   421.978       463   0.0087017  0.00944703
2021-05-15T23:46:56.891448+0000     4     255    106256    106001   413.971   390.031  0.00840979  0.00963588
2021-05-15T23:46:57.891657+0000     5     255    132959    132704   414.607   417.234  0.00896244  0.00962354
2021-05-15T23:46:58.891893+0000     6     255    158587    158332   412.229   400.438  0.00869723  0.00968161
2021-05-15T23:46:59.892090+0000     7     255    186311    186056   415.211   433.188  0.00936864   0.0096131
2021-05-15T23:47:00.892381+0000     8     255    213493    213238   416.384   424.719  0.00860195  0.00958762
2021-05-15T23:47:01.892590+0000     9     255    240363    240108   416.759   419.844   0.0088649   0.0095797
2021-05-15T23:47:02.892792+0000    10     256    267541    267285   417.538   424.641 0.000309228  0.00950858
2021-05-15T23:47:03.892957+0000    11     256    294196    293940   417.436   416.484 0.000395371  0.00954164
2021-05-15T23:47:04.893233+0000    12     255    320829    320574    417.32   416.156  0.00918761  0.00956844
2021-05-15T23:47:05.893498+0000 Total time run:       12.8786
Total reads made:     343380
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   416.606
Average IOPS:         26662
Stddev IOPS:          1403.13
Max IOPS:             29632
Min IOPS:             24066
Average Latency(s):   0.00958757
Max latency(s):       0.142199
Min latency(s):       0.000288735

[1;32mlocalhost.localdomain	[2021-05-15T16:47:06,506308724-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 597338


[1;33mlocalhost.localdomain	[2021-05-15T16:47:06,514177527-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:47:30,720781488-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.38k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:47:30,736410925-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:47:39,905219486-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.38k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:47:39,921391901-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:47:49,059906251-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.38k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:47:49,076030058-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:47:58,536476870-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.38k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:47:58,552408842-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:48:07,719290362-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.38k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:48:07,734359445-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:48:07,746854342-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T16:48:07,751414219-07:00][RUNNING][ROUND 4/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:48:07,758710411-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T16:48:07,775329233-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40975\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.62149\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 18f4fa8e-997d-4cdc-97c0-94d00db850b4\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 18f4fa8e-997d-4cdc-97c0-94d00db850b4\nlast_changed 2021-05-15T16:48:35.631031-0700\ncreated 2021-05-15T16:48:35.631031-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40975/0,v1:10.10.1.2:40976/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.62149 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 db35748e-4bba-4e7b-867a-527bbc5d9d7d\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 2f0836bb-74d9-4dce-bf1e-75df56684e18\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 af7f4ce2-2690-4efd-9bd7-0aefbbaefb04\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42975\n  w/ user/pass: admin / 0d0aee69-800a-4d6c-b83c-5065ab23324e\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:48:54 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40975
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.62149
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 18f4fa8e-997d-4cdc-97c0-94d00db850b4
setting min_mon_release = octopus
epoch 0
fsid 18f4fa8e-997d-4cdc-97c0-94d00db850b4
last_changed 2021-05-15T16:48:35.631031-0700
created 2021-05-15T16:48:35.631031-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40975/0,v1:10.10.1.2:40976/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.62149 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 db35748e-4bba-4e7b-867a-527bbc5d9d7d
0
start osd.0
add osd1 2f0836bb-74d9-4dce-bf1e-75df56684e18
1
start osd.1
add osd2 af7f4ce2-2690-4efd-9bd7-0aefbbaefb04
2
start osd.2


restful urls: https://10.10.1.2:42975
  w/ user/pass: admin / 0d0aee69-800a-4d6c-b83c-5065ab23324e


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T16:48:09.708-0700 7fd0bc9c61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:48:09.708-0700 7fd0bc9c61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:48:09.724-0700 7f660801f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:48:09.724-0700 7f660801f1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40975,v1:10.10.1.2:40976] --print /tmp/ceph_monmap.62149 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.62149 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.62149 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42975 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55aa42416000 @  0x7f7554f33680 0x7f7554f54824 0x7f75556ef187 0x7f75556f7355 0x7f75556ef708 0x7f75556ef877 0x7f75556f0c24 0x7f7555708ec1 0x7f755567b5f3 0x7f75556dce97 0x7f75556e4b1a 0x7f7554de7d84 0x7f7554f03609 0x7f7554ad7293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.yH9rI9a0pv 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new db35748e-4bba-4e7b-867a-527bbc5d9d7d -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDeXaBgMcv1AhAAMjc2hIpaijnnKRBDgKXgGQ== --osd-uuid db35748e-4bba-4e7b-867a-527bbc5d9d7d 
2021-05-15T16:48:46.688-0700 7f5c6b506f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:48:46.712-0700 7f5c6b506f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:48:46.712-0700 7f5c6b506f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55ad934ca000 @  0x7f5c6becf680 0x7f5c6bef0824 0x55ad893a8447 0x55ad893b04b5 0x55ad893a89c8 0x55ad893a8b37 0x55ad893a9ee4 0x55ad8917aca1 0x55ad89352423 0x55ad8916b2a7 0x55ad8917053a 0x7f5c6ba22d84 0x7f5c6bba7609 0x7f5c6b710293
2021-05-15T16:48:47.016-0700 7f5c6b506f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2f0836bb-74d9-4dce-bf1e-75df56684e18 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x562fed3ec000 @  0x7f5ccbe52680 0x7f5ccbe73824 0x562fe28dd447 0x562fe28e54b5 0x562fe28dd9c8 0x562fe28ddb37 0x562fe28deee4 0x562fe26afca1 0x562fe2887423 0x562fe26a02a7 0x562fe26a553a 0x7f5ccb9a5d84 0x7f5ccbb2a609 0x7f5ccb693293
2021-05-15T16:48:47.580-0700 7f5ccb489f00 -1 Falling back to public interface
2021-05-15T16:48:47.840-0700 7f5ccb489f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDfXaBgSTVGERAAVvs/ghpKoPVd3L5SCtBxFQ== --osd-uuid 2f0836bb-74d9-4dce-bf1e-75df56684e18 
2021-05-15T16:48:47.944-0700 7f483a4fff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:48:47.964-0700 7f483a4fff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:48:47.964-0700 7f483a4fff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5561899a0000 @  0x7f483aec8680 0x7f483aee9824 0x55617eb19447 0x55617eb214b5 0x55617eb199c8 0x55617eb19b37 0x55617eb1aee4 0x55617e8ebca1 0x55617eac3423 0x55617e8dc2a7 0x55617e8e153a 0x7f483aa1bd84 0x7f483aba0609 0x7f483a709293
2021-05-15T16:48:48.268-0700 7f483a4fff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new af7f4ce2-2690-4efd-9bd7-0aefbbaefb04 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55afa351e000 @  0x7f9f7becc680 0x7f9f7beed824 0x55af9777f447 0x55af977874b5 0x55af9777f9c8 0x55af9777fb37 0x55af97780ee4 0x55af97551ca1 0x55af97729423 0x55af975422a7 0x55af9754753a 0x7f9f7ba1fd84 0x7f9f7bba4609 0x7f9f7b70d293
2021-05-15T16:48:48.924-0700 7f9f7b503f00 -1 Falling back to public interface
2021-05-15T16:48:49.184-0700 7f9f7b503f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDgXaBgP6J9JBAADiOq5QvNgNwPL9iI6R2YKQ== --osd-uuid af7f4ce2-2690-4efd-9bd7-0aefbbaefb04 
2021-05-15T16:48:49.272-0700 7f7881ab2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:48:49.292-0700 7f7881ab2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:48:49.292-0700 7f7881ab2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55efe67ac000 @  0x7f788247b680 0x7f788249c824 0x55efdba6a447 0x55efdba724b5 0x55efdba6a9c8 0x55efdba6ab37 0x55efdba6bee4 0x55efdb83cca1 0x55efdba14423 0x55efdb82d2a7 0x55efdb83253a 0x7f7881fced84 0x7f7882153609 0x7f7881cbc293
2021-05-15T16:48:49.672-0700 7f7881ab2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55cf0be34000 @  0x7f7f876d7680 0x7f7f876f8824 0x55cf0079f447 0x55cf007a74b5 0x55cf0079f9c8 0x55cf0079fb37 0x55cf007a0ee4 0x55cf00571ca1 0x55cf00749423 0x55cf005622a7 0x55cf0056753a 0x7f7f8722ad84 0x7f7f873af609 0x7f7f86f18293
2021-05-15T16:48:50.360-0700 7f7f86d0ef00 -1 Falling back to public interface
2021-05-15T16:48:50.628-0700 7f7f86d0ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T16:48:54,585946043-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T16:48:54,606152662-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T16:48:54,689945875-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:48:54,696467227-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T16:48:58,698073275-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:48:58,705171510-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T16:49:02,621809765-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:49:02,628452275-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T16:49:06,547928675-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:49:06,554691714-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T16:49:14,310897240-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:49:14,317492657-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T16:49:18,677494290-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:49:18,684018146-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T16:49:23,367566369-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:49:23,373916829-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T16:49:27,819343076-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:49:27,825613948-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:49:32,394520434-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:49:32,401490113-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:49:36,751288434-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:49:36,757615400-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T16:49:41,382622035-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:49:41,388801303-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T16:49:45,193504870-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:49:45,199734575-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.08   89      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   90      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   80      up          osd.2  
                       TOTAL  300 GiB  173 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.08  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T16:49:49,082884639-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:50:13,619947038-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:50:22,680404256-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:50:31,728087468-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:50:40,856332715-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:50:49,894875793-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:50:49,911030388-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:50:58,960363947-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:50:58,976084974-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:51:08,030676185-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:51:08,046737824-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:51:17,525030686-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:51:17,543075719-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:51:26,567601623-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:51:26,583658445-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:51:26,596360754-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:51:26,604082271-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:51:26,621212178-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=610360
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T16:51:26,635437070-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-15T16:51:26,676275180-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:51:26,682407159-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:51:28.208+0000 ffffb2b99010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:51:29.020+0000 ffffb2b99010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:51:29.020+0000 ffffb2b99010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:51:29.040925+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-15T23:51:29.040965+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-15T23:51:29.045393+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:51:29.045393+0000     0       0         0         0         0         0           -           0
2021-05-15T23:51:30.045572+0000     1     255     18034     17779    277.78   277.797  0.00154086   0.0135575
2021-05-15T23:51:31.045729+0000     2     255     35157     34902   272.642   267.547  0.00156087   0.0143628
2021-05-15T23:51:32.045896+0000     3     256     52692     52436   273.069   273.969  0.00168549   0.0143468
2021-05-15T23:51:33.046043+0000     4     255     70119     69864    272.87   272.312  0.00308228   0.0144479
2021-05-15T23:51:34.046201+0000     5     255     87488     87233   272.565   271.391  0.00221866   0.0145007
2021-05-15T23:51:35.046364+0000     6     255    104321    104066   270.967   263.016  0.00205854   0.0146134
2021-05-15T23:51:36.046518+0000     7     255    121399    121144   270.372   266.844    0.115701   0.0146737
2021-05-15T23:51:37.046674+0000     8     255    138637    138382   270.238   269.344  0.00206588   0.0146839
2021-05-15T23:51:38.046852+0000     9     255    155815    155560   270.029   268.406  0.00179565   0.0147107
2021-05-15T23:51:39.047044+0000    10     255    173028    172773   269.916   268.953  0.00160904   0.0147255
2021-05-15T23:51:40.047225+0000    11     255    190400    190145    270.05   271.438  0.00235156   0.0147263
2021-05-15T23:51:41.047400+0000    12     255    207132    206877   269.329   261.438  0.00582449   0.0148019
2021-05-15T23:51:42.047564+0000    13     255    224950    224695   270.023   278.406  0.00288308   0.0147421
2021-05-15T23:51:43.047736+0000    14     256    242639    242383   270.474   276.375  0.00135973   0.0147228
2021-05-15T23:51:44.047911+0000    15     255    259421    259166   269.921   262.234  0.00406621   0.0147555
2021-05-15T23:51:45.048063+0000    16     256    275790    275534   269.033    255.75  0.00222694   0.0148078
2021-05-15T23:51:46.048328+0000    17     255    291138    290883   267.311   239.828  0.00911026   0.0149464
2021-05-15T23:51:47.048490+0000    18     256    308127    307871   267.205   265.438  0.00152747   0.0149166
2021-05-15T23:51:48.048647+0000    19     255    324619    324364   266.703   257.703  0.00181843   0.0149536
2021-05-15T23:51:49.048804+0000 min lat: 0.000672043 max lat: 0.21003 avg lat: 0.0149821
2021-05-15T23:51:49.048804+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:51:49.048804+0000    20     227    340896    340669   266.104   254.766    0.124322   0.0149821
2021-05-15T23:51:50.049033+0000 Total time run:         20.0944
Total writes made:      340896
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     265.074
Stddev Bandwidth:       9.23481
Max bandwidth (MB/sec): 278.406
Min bandwidth (MB/sec): 239.828
Average IOPS:           16964
Stddev IOPS:            591.028
Max IOPS:               17818
Min IOPS:               15349
Average Latency(s):     0.015045
Stddev Latency(s):      0.0367533
Max latency(s):         0.21003
Min latency(s):         0.000672043

[1;32mlocalhost.localdomain	[2021-05-15T16:51:50,644904256-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 610360


[1;33mlocalhost.localdomain	[2021-05-15T16:51:50,652809025-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:52:14,894318733-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.90k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:52:14,912073588-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:52:24,026647745-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.90k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:52:24,043042582-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:52:33,116555494-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.90k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:52:33,132598376-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:52:42,403541002-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.90k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:52:42,419453319-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:52:51,887541686-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.90k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:52:51,907119963-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:52:51,922089580-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:52:51,930007565-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:52:51,948410202-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=613920
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T16:52:51,967051180-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-15T16:52:52,008507360-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:52:52,015692123-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98cf89c1-d98f-4703-8e0e-546c71f9b090', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98cf89c1-d98f-4703-8e0e-546c71f9b090 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9C41RJ:/tmp/ceph-asok.9C41RJ -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:52:53.521+0000 ffff95f85010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:52:54.329+0000 ffff95f85010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:52:54.329+0000 ffff95f85010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:52:54.353737+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:52:54.353737+0000     0       0         0         0         0         0           -           0
2021-05-15T23:52:55.353899+0000     1     255     24566     24311   379.749   379.859  0.00934244   0.0104502
2021-05-15T23:52:56.354086+0000     2     255     48891     48636   379.878   380.078  0.00880425   0.0104842
2021-05-15T23:52:57.354260+0000     3     256     74978     74722   389.092   407.594  0.00702361   0.0101769
2021-05-15T23:52:58.354476+0000     4     255    104098    103843   405.549   455.016  0.00833328  0.00983649
2021-05-15T23:52:59.354640+0000     5     256    123901    123645   386.311   309.406  0.00885023   0.0103294
2021-05-15T23:53:00.354823+0000     6     255    150503    150248   391.192   415.672  0.00926031   0.0102024
2021-05-15T23:53:01.355014+0000     7     255    177010    176755   394.463   414.172  0.00871891   0.0101192
2021-05-15T23:53:02.355185+0000     8     256    204211    203955   398.271       425  0.00876768   0.0100242
2021-05-15T23:53:03.355354+0000     9     255    232105    231850   402.439   435.859  0.00966439  0.00992087
2021-05-15T23:53:04.355541+0000    10     255    260654    260399   406.795   446.078  0.00728624  0.00981459
2021-05-15T23:53:05.355729+0000    11     255    287593    287338   408.072   420.922  0.00943718  0.00978497
2021-05-15T23:53:06.355915+0000    12     256    312125    311869   406.001   383.297  0.00839365   0.0098358
2021-05-15T23:53:07.356153+0000    13     255    336253    335998   403.765   377.016  0.00081782  0.00988506
2021-05-15T23:53:08.356399+0000 Total time run:       13.2144
Total reads made:     340896
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   403.083
Average IOPS:         25797
Stddev IOPS:          2452.94
Max IOPS:             29121
Min IOPS:             19802
Average Latency(s):   0.00990975
Max latency(s):       0.13133
Min latency(s):       0.000271947

[1;32mlocalhost.localdomain	[2021-05-15T16:53:09,010323824-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 613920


[1;33mlocalhost.localdomain	[2021-05-15T16:53:09,018700774-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:53:33,553214632-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.90k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:53:33,569214680-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:53:43,066950898-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.90k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:53:43,082757746-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:53:52,375052938-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.90k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:53:52,391014879-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:54:01,439659760-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.90k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:54:01,455326696-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:54:10,558331442-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.90k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:54:10,574325548-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:54:10,587237058-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T16:54:10,592221224-07:00][RUNNING][ROUND 5/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:54:10,599888324-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T16:54:10,616533364-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40472\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.63239\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 3658f6f0-8047-4ffc-98c3-059ff5b7d7b1\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 3658f6f0-8047-4ffc-98c3-059ff5b7d7b1\nlast_changed 2021-05-15T16:54:39.635675-0700\ncreated 2021-05-15T16:54:39.635675-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40472/0,v1:10.10.1.2:40473/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.63239 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 3c78f5ce-1c3b-469d-9e10-07feb3188488\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 80628783-3c83-4620-9ae5-4ccef0c92083\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 51712ae2-d7e5-4d58-8eb2-dad99746eaa0\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42472\n  w/ user/pass: admin / 5f4289ad-b492-47ee-a916-12821501616b\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 16:54:58 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40472
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.63239
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 3658f6f0-8047-4ffc-98c3-059ff5b7d7b1
setting min_mon_release = octopus
epoch 0
fsid 3658f6f0-8047-4ffc-98c3-059ff5b7d7b1
last_changed 2021-05-15T16:54:39.635675-0700
created 2021-05-15T16:54:39.635675-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40472/0,v1:10.10.1.2:40473/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.63239 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 3c78f5ce-1c3b-469d-9e10-07feb3188488
0
start osd.0
add osd1 80628783-3c83-4620-9ae5-4ccef0c92083
1
start osd.1
add osd2 51712ae2-d7e5-4d58-8eb2-dad99746eaa0
2
start osd.2


restful urls: https://10.10.1.2:42472
  w/ user/pass: admin / 5f4289ad-b492-47ee-a916-12821501616b


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T16:54:12.539-0700 7f093c88b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:54:12.539-0700 7f093c88b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:54:12.555-0700 7fd66b7ce1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T16:54:12.555-0700 7fd66b7ce1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40472,v1:10.10.1.2:40473] --print /tmp/ceph_monmap.63239 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.63239 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.63239 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42472 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55e5e54ca000 @  0x7fca75708680 0x7fca75729824 0x7fca75ec4187 0x7fca75ecc355 0x7fca75ec4708 0x7fca75ec4877 0x7fca75ec5c24 0x7fca75eddec1 0x7fca75e505f3 0x7fca75eb1e97 0x7fca75eb9b1a 0x7fca755bcd84 0x7fca756d8609 0x7fca752ac293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.opKgFNEw5Y 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3c78f5ce-1c3b-469d-9e10-07feb3188488 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBKX6BgYaFoAhAAsw6+iwuFD67LEjfVAvKQIw== --osd-uuid 3c78f5ce-1c3b-469d-9e10-07feb3188488 
2021-05-15T16:54:50.660-0700 7f6666e71f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:54:50.680-0700 7f6666e71f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T16:54:50.680-0700 7f6666e71f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55f03a39a000 @  0x7f666783a680 0x7f666785b824 0x55f02ed39447 0x55f02ed414b5 0x55f02ed399c8 0x55f02ed39b37 0x55f02ed3aee4 0x55f02eb0bca1 0x55f02ece3423 0x55f02eafc2a7 0x55f02eb0153a 0x7f666738dd84 0x7f6667512609 0x7f666707b293
2021-05-15T16:54:50.984-0700 7f6666e71f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 80628783-3c83-4620-9ae5-4ccef0c92083 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x560c2c872000 @  0x7f10aae91680 0x7f10aaeb2824 0x560c2247f447 0x560c224874b5 0x560c2247f9c8 0x560c2247fb37 0x560c22480ee4 0x560c22251ca1 0x560c22429423 0x560c222422a7 0x560c2224753a 0x7f10aa9e4d84 0x7f10aab69609 0x7f10aa6d2293
2021-05-15T16:54:51.568-0700 7f10aa4c8f00 -1 Falling back to public interface
2021-05-15T16:54:51.828-0700 7f10aa4c8f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBLX6BgrWMrEBAAoS4VCRIIFpUe3/VOIB7Aqg== --osd-uuid 80628783-3c83-4620-9ae5-4ccef0c92083 
2021-05-15T16:54:51.916-0700 7fbadc08af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:54:51.936-0700 7fbadc08af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T16:54:51.936-0700 7fbadc08af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x558f47f14000 @  0x7fbadca53680 0x7fbadca74824 0x558f3d74f447 0x558f3d7574b5 0x558f3d74f9c8 0x558f3d74fb37 0x558f3d750ee4 0x558f3d521ca1 0x558f3d6f9423 0x558f3d5122a7 0x558f3d51753a 0x7fbadc5a6d84 0x7fbadc72b609 0x7fbadc294293
2021-05-15T16:54:52.244-0700 7fbadc08af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 51712ae2-d7e5-4d58-8eb2-dad99746eaa0 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x56331e09c000 @  0x7f1503a6a680 0x7f1503a8b824 0x563313a2d447 0x563313a354b5 0x563313a2d9c8 0x563313a2db37 0x563313a2eee4 0x5633137ffca1 0x5633139d7423 0x5633137f02a7 0x5633137f553a 0x7f15035bdd84 0x7f1503742609 0x7f15032ab293
2021-05-15T16:54:52.892-0700 7f15030a1f00 -1 Falling back to public interface
2021-05-15T16:54:53.148-0700 7f15030a1f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBMX6BgSg1FIxAARHMEELTgP+MapXNWstDvYA== --osd-uuid 51712ae2-d7e5-4d58-8eb2-dad99746eaa0 
2021-05-15T16:54:53.256-0700 7fb3e84b0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:54:53.276-0700 7fb3e84b0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T16:54:53.276-0700 7fb3e84b0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56276b628000 @  0x7fb3e8e79680 0x7fb3e8e9a824 0x56275f8b2447 0x56275f8ba4b5 0x56275f8b29c8 0x56275f8b2b37 0x56275f8b3ee4 0x56275f684ca1 0x56275f85c423 0x56275f6752a7 0x56275f67a53a 0x7fb3e89ccd84 0x7fb3e8b51609 0x7fb3e86ba293
2021-05-15T16:54:53.588-0700 7fb3e84b0f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5604650b2000 @  0x7f07dcd10680 0x7f07dcd31824 0x5604595a7447 0x5604595af4b5 0x5604595a79c8 0x5604595a7b37 0x5604595a8ee4 0x560459379ca1 0x560459551423 0x56045936a2a7 0x56045936f53a 0x7f07dc863d84 0x7f07dc9e8609 0x7f07dc551293
2021-05-15T16:54:54.172-0700 7f07dc347f00 -1 Falling back to public interface
2021-05-15T16:54:54.440-0700 7f07dc347f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T16:54:58,579293165-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T16:54:58,599535947-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T16:54:58,681432086-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:54:58,687673862-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T16:55:02,531417795-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:55:02,537709889-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T16:55:06,485898546-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:55:06,494005722-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T16:55:10,540219817-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:55:10,546713752-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T16:55:18,290989943-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:55:18,297275272-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T16:55:22,739097372-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:55:22,745396892-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T16:55:27,447997103-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:55:27,454411181-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T16:55:32,379447448-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:55:32,385979189-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:55:36,641288686-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:55:36,647844625-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T16:55:41,407866910-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:55:41,414169481-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T16:55:45,762372014-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:55:45,768613997-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T16:55:49,486335968-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:55:49,492550506-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   66 KiB   0 B   0 B   0 B  100 GiB     0  1.15   90      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.89   89      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   80      up          osd.2  
                       TOTAL  300 GiB  173 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.89/1.15  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T16:55:53,399937094-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:56:17,665042675-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:56:26,749915221-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:56:35,819190024-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:56:44,922948590-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:56:54,330743642-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   245 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:56:54,347116910-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:57:03,406987233-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:57:03,423173525-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:57:12,490747139-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:57:12,507090678-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:57:21,706729354-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:57:21,723119371-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:57:30,816230534-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:57:30,832458286-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:57:30,845316756-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:57:30,853028708-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:57:30,870347967-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=627215
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T16:57:30,884504465-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-15T16:57:30,923986329-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:57:30,930368701-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:57:32.484+0000 ffffa0f6b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:57:33.272+0000 ffffa0f6b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:57:33.272+0000 ffffa0f6b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:57:33.291768+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-15T23:57:33.291816+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-15T23:57:33.297492+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:57:33.297492+0000     0       6         6         0         0         0           -           0
2021-05-15T23:57:34.297725+0000     1     255     19169     18914   295.301   295.531   0.0122513   0.0133616
2021-05-15T23:57:35.297887+0000     2     255     38900     38645   301.772   308.297  0.00661657    0.013041
2021-05-15T23:57:36.298356+0000     3     255     57948     57693   300.343   297.625  0.00244421   0.0131672
2021-05-15T23:57:37.298607+0000     4     255     76787     76532   298.829   294.359  0.00162929    0.013271
2021-05-15T23:57:38.298759+0000     5     255     95190     94935   296.564   287.547  0.00250788   0.0133592
2021-05-15T23:57:39.298915+0000     6     255    113594    113339   295.057   287.562  0.00506726   0.0134915
2021-05-15T23:57:40.299082+0000     7     255    132615    132360   295.356   297.203  0.00261577   0.0134476
2021-05-15T23:57:41.299234+0000     8     256    151766    151510   295.833   299.219  0.00127665   0.0134308
2021-05-15T23:57:42.299384+0000     9     255    170134    169879   294.849   287.016  0.00648781   0.0135209
2021-05-15T23:57:43.299538+0000    10     255    189431    189176   295.511   301.516  0.00630664   0.0134991
2021-05-15T23:57:44.299702+0000    11     256    208888    208632   296.278       304  0.00458645   0.0134223
2021-05-15T23:57:45.299855+0000    12     255    226240    225985    294.18   271.141  0.00980719   0.0135239
2021-05-15T23:57:46.300012+0000    13     255    244225    243970   293.164   281.016  0.00141219   0.0135834
2021-05-15T23:57:47.300163+0000    14     255    262341    262086    292.44   283.062   0.0023082   0.0136181
2021-05-15T23:57:48.300320+0000    15     256    279952    279696   291.284   275.156  0.00193155   0.0136723
2021-05-15T23:57:49.300484+0000    16     256    297587    297331   290.298   275.547  0.00151016   0.0137485
2021-05-15T23:57:50.300652+0000    17     255    315679    315424   289.849   282.703  0.00150569   0.0137616
2021-05-15T23:57:51.300816+0000    18     255    332535    332280   288.376   263.375  0.00166202   0.0138139
2021-05-15T23:57:52.300935+0000    19     255    350157    349902   287.688   275.344  0.00781383   0.0138755
2021-05-15T23:57:53.301102+0000 min lat: 0.00068424 max lat: 0.211155 avg lat: 0.0138993
2021-05-15T23:57:53.301102+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:57:53.301102+0000    20     249    367351    367102   286.739    268.75      0.1032   0.0138993
2021-05-15T23:57:54.301345+0000 Total time run:         20.0719
Total writes made:      367351
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     285.965
Stddev Bandwidth:       12.7033
Max bandwidth (MB/sec): 308.297
Min bandwidth (MB/sec): 263.375
Average IOPS:           18301
Stddev IOPS:            813.013
Max IOPS:               19731
Min IOPS:               16856
Average Latency(s):     0.0139502
Stddev Latency(s):      0.0282728
Max latency(s):         0.211155
Min latency(s):         0.00068424

[1;32mlocalhost.localdomain	[2021-05-15T16:57:54,940407228-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 627215


[1;33mlocalhost.localdomain	[2021-05-15T16:57:54,948548537-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:58:19,272285519-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.35k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:58:19,289811847-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:58:28,591723847-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.35k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:58:28,608267521-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:58:37,903972232-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.35k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:58:37,920752236-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:58:46,986224339-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.35k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:58:47,002885441-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:58:56,374505125-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.35k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:58:56,391384658-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:58:56,405718779-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T16:58:56,414192400-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:58:56,433908750-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=630783
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T16:58:56,450927152-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-15T16:58:56,491330409-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T16:58:56,497694507-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67e7eb4c-cf34-4b7d-a2b4-aee8d4de7449 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttvEDI:/tmp/ceph-asok.ttvEDI -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-15T23:58:57.971+0000 ffffa32b9010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:58:58.939+0000 ffffa32b9010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T23:58:58.939+0000 ffffa32b9010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-15T23:58:58.964670+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-15T23:58:58.964670+0000     0       0         0         0         0         0           -           0
2021-05-15T23:58:59.964844+0000     1     255     26879     26624   415.868       416  0.00896088  0.00953826
2021-05-15T23:59:00.965068+0000     2     256     52483     52227   407.913   400.047 0.000322028  0.00962456
2021-05-15T23:59:01.965274+0000     3     255     78425     78170   407.034   405.359  0.00864335  0.00979292
2021-05-15T23:59:02.965525+0000     4     255    103581    103326   403.517   393.062   0.0144232  0.00986893
2021-05-15T23:59:03.965724+0000     5     255    131090    130835   408.762   429.828   0.0088996  0.00976025
2021-05-15T23:59:04.965899+0000     6     255    155175    154920   403.345   376.328  0.00944142   0.0098932
2021-05-15T23:59:05.966068+0000     7     255    182741    182486   407.245   430.719   0.0092247  0.00980032
2021-05-15T23:59:06.966247+0000     8     256    210777    210521   411.085   438.047  0.00928471  0.00970985
2021-05-15T23:59:07.966418+0000     9     255    238930    238675   414.279   439.906  0.00910756  0.00963603
2021-05-15T23:59:08.966601+0000    10     255    267039    266784   416.764   439.203  0.00859171  0.00957953
2021-05-15T23:59:09.966782+0000    11     255    295210    294955   418.884   440.172   0.0090472  0.00953149
2021-05-15T23:59:10.966968+0000    12     256    322263    322007   419.195   422.688  0.00361494  0.00952395
2021-05-15T23:59:11.967158+0000    13     255    350146    349891   420.457   435.688  0.00941345  0.00949667
2021-05-15T23:59:12.967386+0000 Total time run:       13.6526
Total reads made:     367351
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   420.424
Average IOPS:         26907
Stddev IOPS:          1334.82
Max IOPS:             28171
Min IOPS:             24085
Average Latency(s):   0.0094999
Max latency(s):       0.114991
Min latency(s):       0.000269052

[1;32mlocalhost.localdomain	[2021-05-15T16:59:13,616932651-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 630783


[1;33mlocalhost.localdomain	[2021-05-15T16:59:13,625433984-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T16:59:37,659718334-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.35k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:59:37,676128306-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:59:47,319368808-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.35k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:59:47,336107273-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T16:59:56,656158823-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.35k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T16:59:56,672957797-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:00:05,729296758-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.35k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:00:05,746011026-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:00:14,726150203-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.35k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:00:14,742757186-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:00:14,755628238-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T17:00:14,763378408-07:00][RUNNING][ROUND 1/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:00:14,771149433-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T17:00:14,787646429-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40447\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.64329\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 25ebe847-c13f-4ed5-9e8c-229ef5e5fcd2\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 25ebe847-c13f-4ed5-9e8c-229ef5e5fcd2\nlast_changed 2021-05-15T17:00:41.872578-0700\ncreated 2021-05-15T17:00:41.872578-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40447/0,v1:10.10.1.2:40448/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.64329 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 ca903065-c819-486f-abb9-e792bd187f8f\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 4f53bc61-7f2e-4d39-bd56-06771ef9c334\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 bd9dc41b-6dd2-42e8-9611-658a1225ddfd\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42447\n  w/ user/pass: admin / 7f7ce985-5c46-4e4f-be13-92e180ef1feb\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:01:01 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40447
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.64329
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 25ebe847-c13f-4ed5-9e8c-229ef5e5fcd2
setting min_mon_release = octopus
epoch 0
fsid 25ebe847-c13f-4ed5-9e8c-229ef5e5fcd2
last_changed 2021-05-15T17:00:41.872578-0700
created 2021-05-15T17:00:41.872578-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40447/0,v1:10.10.1.2:40448/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.64329 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 ca903065-c819-486f-abb9-e792bd187f8f
0
start osd.0
add osd1 4f53bc61-7f2e-4d39-bd56-06771ef9c334
1
start osd.1
add osd2 bd9dc41b-6dd2-42e8-9611-658a1225ddfd
2
start osd.2


restful urls: https://10.10.1.2:42447
  w/ user/pass: admin / 7f7ce985-5c46-4e4f-be13-92e180ef1feb


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T17:00:16.695-0700 7f3f93d6f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:00:16.695-0700 7f3f93d6f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:00:16.711-0700 7fb077b321c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:00:16.711-0700 7fb077b321c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40447,v1:10.10.1.2:40448] --print /tmp/ceph_monmap.64329 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.64329 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.64329 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42447 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5588b20fa000 @  0x7f4cef980680 0x7f4cef9a1824 0x7f4cf013c187 0x7f4cf0144355 0x7f4cf013c708 0x7f4cf013c877 0x7f4cf013dc24 0x7f4cf0155ec1 0x7f4cf00c85f3 0x7f4cf0129e97 0x7f4cf0131b1a 0x7f4cef834d84 0x7f4cef950609 0x7f4cef524293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.xxCPUfudxc 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ca903065-c819-486f-abb9-e792bd187f8f -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC0YKBgukdECRAA/KEdQj/nCZVoxzX1oMDojw== --osd-uuid ca903065-c819-486f-abb9-e792bd187f8f 
2021-05-15T17:00:52.799-0700 7f3cb34b7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:00:52.819-0700 7f3cb34b7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:00:52.819-0700 7f3cb34b7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x556cca862000 @  0x7f3cb3e80680 0x7f3cb3ea1824 0x556cc0639447 0x556cc06414b5 0x556cc06399c8 0x556cc0639b37 0x556cc063aee4 0x556cc040bca1 0x556cc05e3423 0x556cc03fc2a7 0x556cc040153a 0x7f3cb39d3d84 0x7f3cb3b58609 0x7f3cb36c1293
2021-05-15T17:00:53.151-0700 7f3cb34b7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4f53bc61-7f2e-4d39-bd56-06771ef9c334 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x56052a68a000 @  0x7fb212294680 0x7fb2122b5824 0x56051edf5447 0x56051edfd4b5 0x56051edf59c8 0x56051edf5b37 0x56051edf6ee4 0x56051ebc7ca1 0x56051ed9f423 0x56051ebb82a7 0x56051ebbd53a 0x7fb211de7d84 0x7fb211f6c609 0x7fb211ad5293
2021-05-15T17:00:53.763-0700 7fb2118cbf00 -1 Falling back to public interface
2021-05-15T17:00:54.015-0700 7fb2118cbf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC1YKBgKnuPGxAAmPXfxVjvrKrWtzp0mzv8RA== --osd-uuid 4f53bc61-7f2e-4d39-bd56-06771ef9c334 
2021-05-15T17:00:54.127-0700 7f394076ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:00:54.147-0700 7f394076ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:00:54.147-0700 7f394076ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b8f03fc000 @  0x7f3941137680 0x7f3941158824 0x55b8e5687447 0x55b8e568f4b5 0x55b8e56879c8 0x55b8e5687b37 0x55b8e5688ee4 0x55b8e5459ca1 0x55b8e5631423 0x55b8e544a2a7 0x55b8e544f53a 0x7f3940c8ad84 0x7f3940e0f609 0x7f3940978293
2021-05-15T17:00:54.543-0700 7f394076ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new bd9dc41b-6dd2-42e8-9611-658a1225ddfd -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x557f2cd90000 @  0x7f59c24aa680 0x7f59c24cb824 0x557f21e83447 0x557f21e8b4b5 0x557f21e839c8 0x557f21e83b37 0x557f21e84ee4 0x557f21c55ca1 0x557f21e2d423 0x557f21c462a7 0x557f21c4b53a 0x7f59c1ffdd84 0x7f59c2182609 0x7f59c1ceb293
2021-05-15T17:00:55.311-0700 7f59c1ae1f00 -1 Falling back to public interface
2021-05-15T17:00:55.567-0700 7f59c1ae1f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC3YKBgzj6IABAA3lErFCeaaqIZ6TQ6QxGhqA== --osd-uuid bd9dc41b-6dd2-42e8-9611-658a1225ddfd 
2021-05-15T17:00:55.687-0700 7f60d7920f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:00:55.707-0700 7f60d7920f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:00:55.707-0700 7f60d7920f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55cd5b9fa000 @  0x7f60d82e9680 0x7f60d830a824 0x55cd4fe9e447 0x55cd4fea64b5 0x55cd4fe9e9c8 0x55cd4fe9eb37 0x55cd4fe9fee4 0x55cd4fc70ca1 0x55cd4fe48423 0x55cd4fc612a7 0x55cd4fc6653a 0x7f60d7e3cd84 0x7f60d7fc1609 0x7f60d7b2a293
2021-05-15T17:00:56.023-0700 7f60d7920f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55e8b598e000 @  0x7f9b94f69680 0x7f9b94f8a824 0x55e8a9d4f447 0x55e8a9d574b5 0x55e8a9d4f9c8 0x55e8a9d4fb37 0x55e8a9d50ee4 0x55e8a9b21ca1 0x55e8a9cf9423 0x55e8a9b122a7 0x55e8a9b1753a 0x7f9b94abcd84 0x7f9b94c41609 0x7f9b947aa293
2021-05-15T17:00:56.659-0700 7f9b945a0f00 -1 Falling back to public interface
2021-05-15T17:00:56.943-0700 7f9b945a0f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T17:01:01,066448154-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T17:01:01,087226557-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T17:01:01,170590257-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:01:01,177227578-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T17:01:05,234005208-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:01:05,240218722-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T17:01:09,109628258-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:01:09,116163524-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T17:01:12,957129909-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:01:12,963405848-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T17:01:21,187705465-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:01:21,194730124-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T17:01:25,644833257-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:01:25,651160214-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T17:01:29,990987474-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:01:29,997569555-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T17:01:34,722275979-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:01:34,729275125-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:01:39,509593248-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:01:39,516031535-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:01:43,977286771-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:01:43,983605408-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T17:01:48,621397353-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:01:48,627732326-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T17:01:52,572916554-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:01:52,579547173-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  190 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  190 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  0.98  154      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   69 KiB   0 B   0 B   0 B  100 GiB     0  1.09  151      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  0.93  143      up          osd.2  
                       TOTAL  300 GiB  192 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.93/1.09  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T17:01:56,315544196-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:02:20,301812600-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:02:29,566550124-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:02:38,733971586-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:02:47,825135860-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:02:56,887372088-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   245 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:02:56,904081701-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:03:06,111427655-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:03:06,128003657-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:03:15,356501856-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:03:15,373271610-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:03:24,416856748-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:03:24,433614266-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:03:33,600671318-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:03:33,617073830-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:03:33,630583310-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:03:33,638264881-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:03:33,655963176-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=643925
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T17:03:33,670424437-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-15T17:03:33,711225912-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:03:33,717586047-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:03:35.254+0000 ffffaa243010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:03:36.063+0000 ffffaa243010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:03:36.063+0000 ffffaa243010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:03:36.082906+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-16T00:03:36.082952+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T00:03:36.096556+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:03:36.096556+0000     0       0         0         0         0         0           -           0
2021-05-16T00:03:37.096750+0000     1     255     14660     14405   900.258   900.312   0.0254462    0.017376
2021-05-16T00:03:38.096930+0000     2     255     29087     28832   900.892   901.688  0.00495012   0.0175479
2021-05-16T00:03:39.097107+0000     3     256     43446     43190   899.667   897.375  0.00539486   0.0176799
2021-05-16T00:03:40.097282+0000     4     255     57789     57534   898.836     896.5   0.0120288   0.0177024
2021-05-16T00:03:41.097517+0000     5     255     72137     71882   898.376    896.75  0.00473922   0.0177437
2021-05-16T00:03:42.097695+0000     6     255     86556     86301   898.818   901.188  0.00960909   0.0177534
2021-05-16T00:03:43.097884+0000     7     255    101002    100747   899.373   902.875   0.0125549   0.0177392
2021-05-16T00:03:44.098077+0000     8     255    115805    115550   902.578   925.188   0.0157949   0.0176779
2021-05-16T00:03:45.098276+0000     9     255    130379    130124    903.48   910.875  0.00625358    0.017667
2021-05-16T00:03:46.098442+0000    10     255    144606    144351   902.036   889.188   0.0400087   0.0176919
2021-05-16T00:03:47.098625+0000    11     255    160134    159879   908.244     970.5   0.0135537   0.0175871
2021-05-16T00:03:48.098818+0000    12     255    174465    174210   907.183   895.688   0.0354087    0.017608
2021-05-16T00:03:49.099014+0000    13     255    188740    188485   906.016   892.188   0.0184018   0.0176344
2021-05-16T00:03:50.099190+0000    14     256    203111    202855   905.441   898.125  0.00582447   0.0176453
2021-05-16T00:03:51.099393+0000    15     255    217516    217261   905.091   900.375   0.0573054   0.0176385
2021-05-16T00:03:52.099567+0000    16     255    232119    231864   905.556   912.688    0.018472   0.0176466
2021-05-16T00:03:53.099743+0000    17     255    246427    246172   904.882    894.25  0.00744582   0.0176567
2021-05-16T00:03:54.099927+0000    18     255    260531    260276   903.574     881.5   0.0236814   0.0176832
2021-05-16T00:03:55.100084+0000    19     256    274274    274018   901.214   858.875  0.00488205   0.0177287
2021-05-16T00:03:56.100245+0000 min lat: 0.000819966 max lat: 0.073456 avg lat: 0.0177883
2021-05-16T00:03:56.100245+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:03:56.100245+0000    20      23    287702    287679   898.837   853.812    0.010826   0.0177883
2021-05-16T00:03:57.100492+0000 Total time run:         20.0079
Total writes made:      287702
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     898.716
Stddev Bandwidth:       23.2901
Max bandwidth (MB/sec): 970.5
Min bandwidth (MB/sec): 853.812
Average IOPS:           14379
Stddev IOPS:            372.642
Max IOPS:               15528
Min IOPS:               13661
Average Latency(s):     0.0177877
Stddev Latency(s):      0.0123269
Max latency(s):         0.073456
Min latency(s):         0.000819966

[1;32mlocalhost.localdomain	[2021-05-15T17:03:57,738315689-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 643925


[1;33mlocalhost.localdomain	[2021-05-15T17:03:57,746465459-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:04:21,982225293-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 287.70k objects, 18 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:04:21,998583533-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:04:31,073603910-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 287.70k objects, 18 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:04:31,090146630-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:04:40,348696288-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 287.70k objects, 18 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:04:40,365654488-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:04:49,768139860-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 287.70k objects, 18 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:04:49,784400737-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:04:58,962644722-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 287.70k objects, 18 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:04:58,979118106-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:04:58,992351052-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:04:59,000154951-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:04:59,017777549-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=647419
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T17:04:59,032395477-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-15T17:04:59,071435885-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:04:59,078118760-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59a49638-f14b-4a6f-a1f8-7b7fb00b6f20', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59a49638-f14b-4a6f-a1f8-7b7fb00b6f20 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.L8p3tF:/tmp/ceph-asok.L8p3tF -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:05:00.867+0000 ffffaedfb010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:05:01.655+0000 ffffaedfb010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:05:01.655+0000 ffffaedfb010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:05:01.680051+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:05:01.680051+0000     0       0         0         0         0         0           -           0
2021-05-16T00:05:02.680221+0000     1     255     17620     17365      1085   1085.31   0.0258816   0.0145633
2021-05-16T00:05:03.680411+0000     2     255     34692     34437   1075.89      1067   0.0150968   0.0147696
2021-05-16T00:05:04.680609+0000     3     255     51869     51614   1075.05   1073.56   0.0145517   0.0148074
2021-05-16T00:05:05.680858+0000     4     256     65374     65118   1017.23       844   0.0172012   0.0156576
2021-05-16T00:05:06.681044+0000     5     255     81136     80881   1010.79   985.188   0.0337126    0.015723
2021-05-16T00:05:07.681245+0000     6     255     96153     95898   998.717   938.562   0.0145159   0.0159664
2021-05-16T00:05:08.681511+0000     7     255    112925    112670   1005.75   1048.25  0.00258479   0.0157985
2021-05-16T00:05:09.681700+0000     8     256    128547    128291   1002.05   976.312 0.000357816   0.0158824
2021-05-16T00:05:10.681914+0000     9     255    145009    144754   1005.01   1028.94   0.0150172   0.0158745
2021-05-16T00:05:11.682098+0000    10     255    162489    162234   1013.74    1092.5   0.0145237   0.0157399
2021-05-16T00:05:12.682307+0000    11     255    180152    179897   1021.92   1103.94   0.0144707   0.0156149
2021-05-16T00:05:13.682487+0000    12     255    197758    197503   1028.44   1100.38   0.0143893    0.015517
2021-05-16T00:05:14.682696+0000    13     255    214608    214353   1030.32   1053.12   0.0148331   0.0154891
2021-05-16T00:05:15.682890+0000    14     255    232368    232113      1036      1110   0.0147078   0.0154051
2021-05-16T00:05:16.683115+0000    15     255    248248    247993   1033.08     992.5   0.0144836   0.0154497
2021-05-16T00:05:17.683288+0000    16     255    264571    264316   1032.27   1020.19   0.0105364   0.0154618
2021-05-16T00:05:18.683465+0000    17     256    278874    278618   1024.12   893.875 0.000465761   0.0155738
2021-05-16T00:05:19.683695+0000 Total time run:       17.6223
Total reads made:     287702
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   1020.38
Average IOPS:         16326
Stddev IOPS:          1234.53
Max IOPS:             17760
Min IOPS:             13504
Average Latency(s):   0.0156475
Max latency(s):       0.141116
Min latency(s):       0.000325336

[1;32mlocalhost.localdomain	[2021-05-15T17:05:20,329098272-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 647419


[1;33mlocalhost.localdomain	[2021-05-15T17:05:20,337952732-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:05:44,380419165-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 287.70k objects, 18 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:05:44,398762578-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:05:53,477890982-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 287.70k objects, 18 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:05:53,494452330-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:06:02,528431651-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 287.70k objects, 18 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:06:02,544892053-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:06:11,720878140-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 287.70k objects, 18 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:06:11,737794800-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:06:20,989931811-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 287.70k objects, 18 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:06:21,006671612-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:06:21,020124478-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T17:06:21,025074901-07:00][RUNNING][ROUND 2/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:06:21,032724393-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T17:06:21,049803465-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40945\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.65420\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f7e25ff1-0811-41f4-ad50-dc7cc5267080\nsetting min_mon_release = octopus\nepoch 0\nfsid f7e25ff1-0811-41f4-ad50-dc7cc5267080\nlast_changed 2021-05-15T17:06:49.393001-0700\ncreated 2021-05-15T17:06:49.393001-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40945/0,v1:10.10.1.2:40946/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.65420 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 0762b57f-7b3e-48f8-92f8-5fe476f88566\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 31918b3a-788e-46ee-9909-11cb25ab1a9b\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ec87e59e-7510-4d00-a87f-b68104b1d58d\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42945\n  w/ user/pass: admin / dea0d2af-3842-4c19-81d7-f0e549390c1d\n\n'
10.10.1.2: b'\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:07:09 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40945
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.65420
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f7e25ff1-0811-41f4-ad50-dc7cc5267080
setting min_mon_release = octopus
epoch 0
fsid f7e25ff1-0811-41f4-ad50-dc7cc5267080
last_changed 2021-05-15T17:06:49.393001-0700
created 2021-05-15T17:06:49.393001-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40945/0,v1:10.10.1.2:40946/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.65420 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 0762b57f-7b3e-48f8-92f8-5fe476f88566
0
start osd.0
add osd1 31918b3a-788e-46ee-9909-11cb25ab1a9b
1
start osd.1
add osd2 ec87e59e-7510-4d00-a87f-b68104b1d58d
2
start osd.2


restful urls: https://10.10.1.2:42945
  w/ user/pass: admin / dea0d2af-3842-4c19-81d7-f0e549390c1d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T17:06:22.974-0700 7fe9fa0a71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:06:22.974-0700 7fe9fa0a71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:06:22.990-0700 7f69705001c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:06:22.990-0700 7f69705001c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40945,v1:10.10.1.2:40946] --print /tmp/ceph_monmap.65420 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.65420 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.65420 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42945 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55abff906000 @  0x7f4bbd41a680 0x7f4bbd43b824 0x7f4bbdbd6187 0x7f4bbdbde355 0x7f4bbdbd6708 0x7f4bbdbd6877 0x7f4bbdbd7c24 0x7f4bbdbefec1 0x7f4bbdb625f3 0x7f4bbdbc3e97 0x7f4bbdbcbb1a 0x7f4bbd2ced84 0x7f4bbd3ea609 0x7f4bbcfbe293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.XKCC16Ki6t 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0762b57f-7b3e-48f8-92f8-5fe476f88566 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAkYqBgTECLIxAAsP6vGQe6FeEhoQGC2J4WGg== --osd-uuid 0762b57f-7b3e-48f8-92f8-5fe476f88566 
2021-05-15T17:07:01.242-0700 7fc1202b8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:07:01.262-0700 7fc1202b8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:07:01.262-0700 7fc1202b8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55e713394000 @  0x7fc120c81680 0x7fc120ca2824 0x55e7084ef447 0x55e7084f74b5 0x55e7084ef9c8 0x55e7084efb37 0x55e7084f0ee4 0x55e7082c1ca1 0x55e708499423 0x55e7082b22a7 0x55e7082b753a 0x7fc1207d4d84 0x7fc120959609 0x7fc1204c2293
2021-05-15T17:07:01.566-0700 7fc1202b8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 31918b3a-788e-46ee-9909-11cb25ab1a9b -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5652a5c3c000 @  0x7f3c86693680 0x7f3c866b4824 0x56529a108447 0x56529a1104b5 0x56529a1089c8 0x56529a108b37 0x56529a109ee4 0x565299edaca1 0x56529a0b2423 0x565299ecb2a7 0x565299ed053a 0x7f3c861e6d84 0x7f3c8636b609 0x7f3c85ed4293
2021-05-15T17:07:02.174-0700 7f3c85ccaf00 -1 Falling back to public interface
2021-05-15T17:07:02.426-0700 7f3c85ccaf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAlYqBg/iVuNBAAk86QPMQi6tSbI4HsXS+srA== --osd-uuid 31918b3a-788e-46ee-9909-11cb25ab1a9b 
2021-05-15T17:07:02.566-0700 7fc506bbdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:07:02.586-0700 7fc506bbdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:07:02.586-0700 7fc506bbdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x558e65794000 @  0x7fc507586680 0x7fc5075a7824 0x558e5a8f0447 0x558e5a8f84b5 0x558e5a8f09c8 0x558e5a8f0b37 0x558e5a8f1ee4 0x558e5a6c2ca1 0x558e5a89a423 0x558e5a6b32a7 0x558e5a6b853a 0x7fc5070d9d84 0x7fc50725e609 0x7fc506dc7293
2021-05-15T17:07:02.946-0700 7fc506bbdf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ec87e59e-7510-4d00-a87f-b68104b1d58d -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55cfcc024000 @  0x7f0569773680 0x7f0569794824 0x55cfc03a7447 0x55cfc03af4b5 0x55cfc03a79c8 0x55cfc03a7b37 0x55cfc03a8ee4 0x55cfc0179ca1 0x55cfc0351423 0x55cfc016a2a7 0x55cfc016f53a 0x7f05692c6d84 0x7f056944b609 0x7f0568fb4293
2021-05-15T17:07:03.598-0700 7f0568daaf00 -1 Falling back to public interface
2021-05-15T17:07:03.854-0700 7f0568daaf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAnYqBgx96WERAAXYRzl7boSQBq/CgXMYvPOA== --osd-uuid ec87e59e-7510-4d00-a87f-b68104b1d58d 
2021-05-15T17:07:03.966-0700 7f40ef3b5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:07:03.986-0700 7f40ef3b5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:07:03.986-0700 7f40ef3b5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x564820298000 @  0x7f40efd7e680 0x7f40efd9f824 0x564815a01447 0x564815a094b5 0x564815a019c8 0x564815a01b37 0x564815a02ee4 0x5648157d3ca1 0x5648159ab423 0x5648157c42a7 0x5648157c953a 0x7f40ef8d1d84 0x7f40efa56609 0x7f40ef5bf293
2021-05-15T17:07:04.298-0700 7f40ef3b5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x56158cd7c000 @  0x7fb0568a1680 0x7fb0568c2824 0x561582c1c447 0x561582c244b5 0x561582c1c9c8 0x561582c1cb37 0x561582c1dee4 0x5615829eeca1 0x561582bc6423 0x5615829df2a7 0x5615829e453a 0x7fb0563f4d84 0x7fb056579609 0x7fb0560e2293
2021-05-15T17:07:04.970-0700 7fb055ed8f00 -1 Falling back to public interface
2021-05-15T17:07:05.242-0700 7fb055ed8f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T17:07:09,312906376-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T17:07:09,333410008-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T17:07:09,415104561-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:07:09,421641151-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T17:07:13,378215893-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:07:13,384731539-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T17:07:17,263993249-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:07:17,270300806-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T17:07:21,291028396-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:07:21,297602370-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T17:07:29,045117274-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:07:29,051528647-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T17:07:33,162986045-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:07:33,169377628-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T17:07:37,647361113-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:07:37,653900847-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T17:07:42,453426777-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:07:42,460100949-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:07:47,373196561-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:07:47,379717284-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:07:52,007776057-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:07:52,014957423-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T17:07:56,233697099-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:07:56,240059077-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T17:08:00,126743531-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:08:00,133153892-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  180 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  180 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   66 KiB   0 B   0 B   0 B  100 GiB     0  1.10   88      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.92   91      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  0.98   80      up          osd.2  
                       TOTAL  300 GiB  181 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.92/1.10  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T17:08:03,832864538-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:08:27,825888306-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:08:36,937287801-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:08:46,204308718-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:08:55,171351727-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:09:04,271417095-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:09:04,287783411-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:09:13,252528934-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:09:13,268874061-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:09:22,386080593-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:09:22,402246029-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:09:31,522923664-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:09:31,539067103-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:09:40,575022793-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:09:40,591197942-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:09:40,604280374-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:09:40,611859854-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:09:40,629797393-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=660619
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T17:09:40,644657862-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-15T17:09:40,684663990-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:09:40,691032308-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:09:42.201+0000 ffff9295f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:09:43.001+0000 ffff9295f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:09:43.001+0000 ffff9295f010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:09:43.026687+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-16T00:09:43.026748+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T00:09:43.045011+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:09:43.045011+0000     0      42        42         0         0         0           -           0
2021-05-16T00:09:44.045224+0000     1     256     15337     15081   940.081   942.562  0.00519955   0.0167332
2021-05-16T00:09:45.045403+0000     2     255     30219     29964   935.057   930.188  0.00504444   0.0169715
2021-05-16T00:09:46.045596+0000     3     255     46100     45845   954.146   992.562   0.0171685   0.0166805
2021-05-16T00:09:47.045764+0000     4     255     61491     61236   956.052   961.938    0.024733   0.0166607
2021-05-16T00:09:48.045942+0000     5     256     76837     76581    956.62   959.062   0.0213344   0.0166751
2021-05-16T00:09:49.046122+0000     6     255     92956     92701   965.066    1007.5  0.00571804   0.0165219
2021-05-16T00:09:50.046307+0000     7     256    108014    107758   961.614   941.062   0.0151201   0.0165968
2021-05-16T00:09:51.046498+0000     8     255    123635    123380   963.435   976.375   0.0134474   0.0165717
2021-05-16T00:09:52.046680+0000     9     255    139017    138762   963.187   961.375   0.0240598   0.0165724
2021-05-16T00:09:53.046850+0000    10     255    154227    153972   961.915   950.625   0.0276717   0.0165972
2021-05-16T00:09:54.047019+0000    11     255    169260    169005   959.869   939.562  0.00974824   0.0166314
2021-05-16T00:09:55.047196+0000    12     255    184603    184348   959.777   958.938  0.00545488   0.0166425
2021-05-16T00:09:56.047410+0000    13     255    199808    199553   959.033   950.312   0.0103139   0.0166604
2021-05-16T00:09:57.047600+0000    14     255    215002    214747   958.349   949.625   0.0183722   0.0166702
2021-05-16T00:09:58.047774+0000    15     255    230644    230389   959.622   977.625   0.0182035    0.016652
2021-05-16T00:09:59.047945+0000    16     255    245792    245537   958.808    946.75  0.00612107   0.0166673
2021-05-16T00:10:00.048116+0000    17     255    260481    260226   956.402   918.062   0.0343766   0.0167001
2021-05-16T00:10:01.048354+0000    18     255    275459    275204   955.263   936.125  0.00570026   0.0167292
2021-05-16T00:10:02.048514+0000    19     255    289011    288756   949.559       847   0.0649029    0.016789
2021-05-16T00:10:03.048671+0000 min lat: 0.000866942 max lat: 0.108716 avg lat: 0.0169268
2021-05-16T00:10:03.048671+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:10:03.048671+0000    20     256    301635    301379   941.523   788.938  0.00121047   0.0169268
2021-05-16T00:10:04.048910+0000 Total time run:         20.0371
Total writes made:      301635
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     940.866
Stddev Bandwidth:       48.166
Max bandwidth (MB/sec): 1007.5
Min bandwidth (MB/sec): 788.938
Average IOPS:           15053
Stddev IOPS:            770.656
Max IOPS:               16120
Min IOPS:               12623
Average Latency(s):     0.0169904
Stddev Latency(s):      0.0109288
Max latency(s):         0.108992
Min latency(s):         0.000866942

[1;32mlocalhost.localdomain	[2021-05-15T17:10:04,679765925-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 660619


[1;33mlocalhost.localdomain	[2021-05-15T17:10:04,688478458-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:10:28,747866006-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.64k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:10:28,764517047-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:10:37,810304995-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.64k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:10:37,826813837-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:10:46,986845885-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.64k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:10:47,005626523-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:10:56,380253718-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.64k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:10:56,396677983-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:11:05,855150556-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.64k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:11:05,872026125-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:11:05,885840069-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:11:05,893987501-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:11:05,911753940-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=664144
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T17:11:05,926784520-07:00] INFO: > Run rados bench[0m
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.2
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
[1;30mlocalhost.localdomain	[2021-05-15T17:11:05,966790783-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:11:05,973201199-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '59734ac5-c8d5-48ed-9cd2-d5484147c177', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 59734ac5-c8d5-48ed-9cd2-d5484147c177 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4XbtkX:/tmp/ceph-asok.4XbtkX -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:11:07.712+0000 ffff93085010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:11:08.500+0000 ffff93085010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:11:08.504+0000 ffff93085010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:11:08.526008+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:11:08.526008+0000     0       0         0         0         0         0           -           0
2021-05-16T00:11:09.526186+0000     1     255     18092     17837   1114.47   1114.81   0.0141932    0.014198
2021-05-16T00:11:10.526395+0000     2     255     35994     35739   1116.56   1118.88   0.0142838    0.014236
2021-05-16T00:11:11.526575+0000     3     255     53386     53131   1106.64      1087   0.0146734   0.0143856
2021-05-16T00:11:12.526769+0000     4     255     70655     70400   1099.76   1079.31   0.0136935   0.0144886
2021-05-16T00:11:13.526990+0000     5     255     86241     85986   1074.59   974.125  0.00709041   0.0148222
2021-05-16T00:11:14.527175+0000     6     255    103377    103122   1073.96      1071   0.0146524   0.0148468
2021-05-16T00:11:15.527520+0000     7     255    120867    120612   1076.64   1093.12   0.0146139   0.0148131
2021-05-16T00:11:16.527731+0000     8     255    138652    138397   1080.98   1111.56   0.0143498   0.0147562
2021-05-16T00:11:17.527997+0000     9     255    156389    156134   1084.01   1108.56   0.0238936   0.0147148
2021-05-16T00:11:18.528169+0000    10     256    172418    172162   1075.77   1001.75  0.00043257   0.0148055
2021-05-16T00:11:19.528342+0000    11     256    189222    188966   1073.43   1050.25    0.010656   0.0147993
2021-05-16T00:11:20.528556+0000    12     255    205220    204965   1067.29   999.938   0.0143351   0.0149515
2021-05-16T00:11:21.528765+0000    13     255    221572    221317   1063.79      1022   0.0146027   0.0150014
2021-05-16T00:11:22.528947+0000    14     256    239015    238759   1065.65   1090.12   0.0146067   0.0149758
2021-05-16T00:11:23.529224+0000    15     255    256108    255853   1065.82   1068.38   0.0141912   0.0149746
2021-05-16T00:11:24.529432+0000    16     255    273112    272857   1065.61   1062.75   0.0148851   0.0149775
2021-05-16T00:11:25.529741+0000    17     256    287742    287486   1056.69   914.312   0.0139677   0.0151052
2021-05-16T00:11:26.529927+0000 Total time run:       17.9097
Total reads made:     301635
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   1052.63
Average IOPS:         16842
Stddev IOPS:          910.141
Max IOPS:             17902
Min IOPS:             14629
Average Latency(s):   0.0151675
Max latency(s):       0.148259
Min latency(s):       0.000319975

[1;32mlocalhost.localdomain	[2021-05-15T17:11:27,177871210-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 664144


[1;33mlocalhost.localdomain	[2021-05-15T17:11:27,186732581-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:11:51,355210687-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.64k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:11:51,372172323-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:12:00,334143044-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.64k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:12:00,351032470-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:12:09,452574905-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.64k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:12:09,469934088-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:12:18,608722283-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.64k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:12:18,629307272-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:12:27,712815512-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.64k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:12:27,730615359-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:12:27,744351534-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T17:12:27,749671626-07:00][RUNNING][ROUND 3/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:12:27,757708144-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T17:12:27,775358581-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40748\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.66504\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 579ed9ac-e482-4523-8659-96f954ca92b1\nsetting min_mon_release = octopus\nepoch 0\nfsid 579ed9ac-e482-4523-8659-96f954ca92b1\nlast_changed 2021-05-15T17:12:55.896184-0700\ncreated 2021-05-15T17:12:55.896184-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40748/0,v1:10.10.1.2:40749/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.66504 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 8d2de8a6-9f93-4a10-aff0-94f1af83d560\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 3b4ebafa-d461-4537-9d1a-db61547f9f31\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 c2257baa-4294-4b1d-b7ea-6003931c70ef\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42748\n  w/ user/pass: admin / 4bca8d75-6419-44e7-bb97-1181f357f5cf\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:13:14 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40748
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.66504
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 579ed9ac-e482-4523-8659-96f954ca92b1
setting min_mon_release = octopus
epoch 0
fsid 579ed9ac-e482-4523-8659-96f954ca92b1
last_changed 2021-05-15T17:12:55.896184-0700
created 2021-05-15T17:12:55.896184-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40748/0,v1:10.10.1.2:40749/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.66504 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 8d2de8a6-9f93-4a10-aff0-94f1af83d560
0
start osd.0
add osd1 3b4ebafa-d461-4537-9d1a-db61547f9f31
1
start osd.1
add osd2 c2257baa-4294-4b1d-b7ea-6003931c70ef
2
start osd.2


restful urls: https://10.10.1.2:42748
  w/ user/pass: admin / 4bca8d75-6419-44e7-bb97-1181f357f5cf


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T17:12:29.685-0700 7f172bc621c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:12:29.685-0700 7f172bc621c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:12:29.701-0700 7f954893e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:12:29.701-0700 7f954893e1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40748,v1:10.10.1.2:40749] --print /tmp/ceph_monmap.66504 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.66504 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.66504 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42748 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55c640252000 @  0x7f630bf9f680 0x7f630bfc0824 0x7f630c75b187 0x7f630c763355 0x7f630c75b708 0x7f630c75b877 0x7f630c75cc24 0x7f630c774ec1 0x7f630c6e75f3 0x7f630c748e97 0x7f630c750b1a 0x7f630be53d84 0x7f630bf6f609 0x7f630bb43293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.jPiZ0auxd1 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8d2de8a6-9f93-4a10-aff0-94f1af83d560 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCSY6Bg7vEwFBAALSm/wSDf1DLpyHudx6K/kg== --osd-uuid 8d2de8a6-9f93-4a10-aff0-94f1af83d560 
2021-05-15T17:13:06.974-0700 7f3536dd9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:13:06.994-0700 7f3536dd9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:13:06.994-0700 7f3536dd9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x559947df2000 @  0x7f35377a2680 0x7f35377c3824 0x55993d29a447 0x55993d2a24b5 0x55993d29a9c8 0x55993d29ab37 0x55993d29bee4 0x55993d06cca1 0x55993d244423 0x55993d05d2a7 0x55993d06253a 0x7f35372f5d84 0x7f353747a609 0x7f3536fe3293
2021-05-15T17:13:07.302-0700 7f3536dd9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3b4ebafa-d461-4537-9d1a-db61547f9f31 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5568c0854000 @  0x7f797f77f680 0x7f797f7a0824 0x5568b514c447 0x5568b51544b5 0x5568b514c9c8 0x5568b514cb37 0x5568b514dee4 0x5568b4f1eca1 0x5568b50f6423 0x5568b4f0f2a7 0x5568b4f1453a 0x7f797f2d2d84 0x7f797f457609 0x7f797efc0293
2021-05-15T17:13:07.910-0700 7f797edb6f00 -1 Falling back to public interface
2021-05-15T17:13:08.166-0700 7f797edb6f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCTY6BgeUaZJBAAjV1I9BVYmMNRXNzCajpUrQ== --osd-uuid 3b4ebafa-d461-4537-9d1a-db61547f9f31 
2021-05-15T17:13:08.282-0700 7efe50f3cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:13:08.302-0700 7efe50f3cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:13:08.302-0700 7efe50f3cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5612ba51a000 @  0x7efe51905680 0x7efe51926824 0x5612af29c447 0x5612af2a44b5 0x5612af29c9c8 0x5612af29cb37 0x5612af29dee4 0x5612af06eca1 0x5612af246423 0x5612af05f2a7 0x5612af06453a 0x7efe51458d84 0x7efe515dd609 0x7efe51146293
2021-05-15T17:13:08.622-0700 7efe50f3cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c2257baa-4294-4b1d-b7ea-6003931c70ef -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5642c4448000 @  0x7f9578099680 0x7f95780ba824 0x5642b8e55447 0x5642b8e5d4b5 0x5642b8e559c8 0x5642b8e55b37 0x5642b8e56ee4 0x5642b8c27ca1 0x5642b8dff423 0x5642b8c182a7 0x5642b8c1d53a 0x7f9577becd84 0x7f9577d71609 0x7f95778da293
2021-05-15T17:13:09.274-0700 7f95776d0f00 -1 Falling back to public interface
2021-05-15T17:13:09.522-0700 7f95776d0f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCUY6Bg0qbYORAAPp+z0D55Hs1vY8UubQN1Sw== --osd-uuid c2257baa-4294-4b1d-b7ea-6003931c70ef 
2021-05-15T17:13:09.646-0700 7f6a26c59f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:13:09.666-0700 7f6a26c59f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:13:09.666-0700 7f6a26c59f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55e840724000 @  0x7f6a27622680 0x7f6a27643824 0x55e8353fb447 0x55e8354034b5 0x55e8353fb9c8 0x55e8353fbb37 0x55e8353fcee4 0x55e8351cdca1 0x55e8353a5423 0x55e8351be2a7 0x55e8351c353a 0x7f6a27175d84 0x7f6a272fa609 0x7f6a26e63293
2021-05-15T17:13:09.966-0700 7f6a26c59f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55fe6dbce000 @  0x7fa21894b680 0x7fa21896c824 0x55fe635ca447 0x55fe635d24b5 0x55fe635ca9c8 0x55fe635cab37 0x55fe635cbee4 0x55fe6339cca1 0x55fe63574423 0x55fe6338d2a7 0x55fe6339253a 0x7fa21849ed84 0x7fa218623609 0x7fa21818c293
2021-05-15T17:13:10.562-0700 7fa217f82f00 -1 Falling back to public interface
2021-05-15T17:13:10.830-0700 7fa217f82f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T17:13:14,927333149-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T17:13:14,948045591-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T17:13:15,028473196-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:13:15,035020388-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T17:13:19,167127458-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:13:19,174699909-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T17:13:23,148112939-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:13:23,154523006-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T17:13:27,182601122-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:13:27,189107280-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T17:13:34,925348633-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:13:34,931955548-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T17:13:40,109508100-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:13:40,116001954-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T17:13:44,583029097-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:13:44,589305203-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T17:13:48,899031557-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:13:48,905281574-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:13:53,818234044-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:13:53,824698237-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:13:58,094334566-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:13:58,100666976-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T17:14:02,663053672-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:14:02,669296547-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T17:14:06,755505266-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:14:06,761972023-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  190 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  190 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  0.98  153      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   69 KiB   0 B   0 B   0 B  100 GiB     0  1.09  152      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  0.93  143      up          osd.2  
                       TOTAL  300 GiB  192 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.93/1.09  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T17:14:10,579798248-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:14:34,731014221-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:14:43,839322077-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:14:53,223354261-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:02,266591904-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:11,331044393-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   245 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:11,348191953-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:20,473471941-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:20,495126123-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:29,692551272-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:29,709854989-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:38,811480525-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:38,828372915-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:47,894284429-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:47,911413002-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:47,924864743-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:15:47,932940105-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:15:47,951384399-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=677369
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T17:15:47,966561234-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-15T17:15:48,007732007-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:15:48,014397477-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:15:49.524+0000 ffff912ea010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:15:50.328+0000 ffff912ea010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:15:50.328+0000 ffff912ea010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:15:50.346340+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-16T00:15:50.346398+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T00:15:50.359490+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:15:50.359490+0000     0       0         0         0         0         0           -           0
2021-05-16T00:15:51.359683+0000     1     255     14803     14548   909.188    909.25   0.0104029   0.0173093
2021-05-16T00:15:52.359865+0000     2     255     29093     28838   901.075   893.125  0.00838263   0.0176274
2021-05-16T00:15:53.360029+0000     3     255     43402     43147   898.772   894.312  0.00801234   0.0176759
2021-05-16T00:15:54.360222+0000     4     255     57507     57252   894.427   881.562   0.0521755   0.0177746
2021-05-16T00:15:55.360415+0000     5     255     71852     71597   894.819   896.562   0.0152633   0.0178067
2021-05-16T00:15:56.360577+0000     6     255     85870     85615    891.68   876.125   0.0194366   0.0178893
2021-05-16T00:15:57.360730+0000     7     255     99261     99006   883.841   836.938   0.0143797   0.0180568
2021-05-16T00:15:58.360903+0000     8     255    112849    112594   879.499    849.25   0.0153899   0.0181575
2021-05-16T00:15:59.361084+0000     9     255    125748    125493   871.337   806.188  0.00637996   0.0183255
2021-05-16T00:16:00.361280+0000    10     255    139083    138828   867.531   833.438   0.0128335   0.0184097
2021-05-16T00:16:01.361462+0000    11     256    150398    150142   852.936   707.125   0.0549104   0.0186518
2021-05-16T00:16:02.361643+0000    12     255    163529    163274   850.242    820.75   0.0107125    0.018776
2021-05-16T00:16:03.361812+0000    13     255    175753    175498   843.598       764   0.0174485   0.0189281
2021-05-16T00:16:04.361990+0000    14     255    188160    187905   838.719   775.438  0.00703456   0.0190486
2021-05-16T00:16:05.362177+0000    15     255    199274    199019   829.104   694.625  0.00998491   0.0192661
2021-05-16T00:16:06.362362+0000    16     255    212016    211761   827.049   796.375   0.0136876   0.0193207
2021-05-16T00:16:07.362548+0000    17     255    223843    223588   821.873   739.188  0.00539942    0.019443
2021-05-16T00:16:08.362751+0000    18     255    236311    236056   819.496    779.25   0.0291649   0.0194962
2021-05-16T00:16:09.362922+0000    19     255    248956    248701   817.953   790.312   0.0244107   0.0195407
2021-05-16T00:16:10.363086+0000 min lat: 0.000859599 max lat: 0.153317 avg lat: 0.0195903
2021-05-16T00:16:10.363086+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:16:10.363086+0000    20     148    261317    261169   816.012    779.25  0.00409365   0.0195903
2021-05-16T00:16:11.363344+0000 Total time run:         20.0125
Total writes made:      261317
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     816.105
Stddev Bandwidth:       63.9605
Max bandwidth (MB/sec): 909.25
Min bandwidth (MB/sec): 694.625
Average IOPS:           13057
Stddev IOPS:            1023.37
Max IOPS:               14548
Min IOPS:               11114
Average Latency(s):     0.019589
Stddev Latency(s):      0.0148073
Max latency(s):         0.153317
Min latency(s):         0.000859599

[1;32mlocalhost.localdomain	[2021-05-15T17:16:12,107704496-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 677369


[1;33mlocalhost.localdomain	[2021-05-15T17:16:12,116514993-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:16:36,130563547-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 261.32k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:16:36,148263105-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:16:45,213413926-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 261.32k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:16:45,230925035-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:16:54,604672813-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 261.32k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:16:54,623929147-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:17:04,038973198-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 261.32k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:17:04,056112076-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:17:13,128624957-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 261.32k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:17:13,146608474-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:17:13,160511475-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:17:13,168685514-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:17:13,187428203-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=680906
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T17:17:13,202784631-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-15T17:17:13,241727217-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:17:13,247991633-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '743874b0-5945-4387-b5d6-5da4765da2ca', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 743874b0-5945-4387-b5d6-5da4765da2ca --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BB6TYE:/tmp/ceph-asok.BB6TYE -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:17:14.719+0000 ffff8b854010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:17:15.543+0000 ffff8b854010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:17:15.547+0000 ffff8b854010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:17:15.570657+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:17:15.570657+0000     0      20        20         0         0         0           -           0
2021-05-16T00:17:16.570843+0000     1     255     17825     17570   1097.12   1098.12   0.0143674   0.0144095
2021-05-16T00:17:17.571045+0000     2     255     35586     35331   1103.48   1110.06   0.0143287   0.0143988
2021-05-16T00:17:18.571246+0000     3     255     52785     52530   1093.89   1074.94   0.0146431   0.0145491
2021-05-16T00:17:19.571459+0000     4     255     70558     70303   1098.06   1110.81   0.0141125   0.0145065
2021-05-16T00:17:20.571650+0000     5     255     88454     88199   1102.11    1118.5   0.0142827   0.0144602
2021-05-16T00:17:21.571853+0000     6     255    106094    105839   1102.14    1102.5   0.0164528   0.0144625
2021-05-16T00:17:22.572075+0000     7     255    123558    123303   1100.58    1091.5    0.014339   0.0144884
2021-05-16T00:17:23.572294+0000     8     255    141419    141164   1102.52   1116.31    0.013958   0.0144659
2021-05-16T00:17:24.572508+0000     9     255    159564    159309      1106   1134.06   0.0138773   0.0144227
2021-05-16T00:17:25.572736+0000    10     255    176838    176583   1103.33   1079.62    0.013979   0.0144589
2021-05-16T00:17:26.572942+0000    11     255    194613    194358      1104   1110.94   0.0142115   0.0144513
2021-05-16T00:17:27.573153+0000    12     255    212418    212163   1104.72   1112.81   0.0143441   0.0144428
2021-05-16T00:17:28.573427+0000    13     255    230036    229781   1104.42   1101.12   0.0181372   0.0144456
2021-05-16T00:17:29.573689+0000    14     255    247550    247295    1103.7   1094.62   0.0146035   0.0144579
2021-05-16T00:17:30.573911+0000 Total time run:       14.7804
Total reads made:     261317
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   1105
Average IOPS:         17679
Stddev IOPS:          251.127
Max IOPS:             18145
Min IOPS:             17199
Average Latency(s):   0.0144458
Max latency(s):       0.029657
Min latency(s):       0.00485507

[1;32mlocalhost.localdomain	[2021-05-15T17:17:31,205738105-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 680906


[1;33mlocalhost.localdomain	[2021-05-15T17:17:31,213964442-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:17:55,167663645-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 261.32k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:17:55,185202420-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:18:04,130318032-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 261.32k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:18:04,147339938-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:18:13,191957894-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 261.32k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:18:13,209556675-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:18:22,750730495-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 261.32k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:18:22,768213350-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:18:32,068262600-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 261.32k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:18:32,085905260-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:18:32,099307766-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T17:18:32,104396746-07:00][RUNNING][ROUND 4/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:18:32,112145593-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T17:18:32,129126027-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40403\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.67592\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f336aad3-df2e-402e-92bd-9613b0e499b2\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid f336aad3-df2e-402e-92bd-9613b0e499b2\nlast_changed 2021-05-15T17:19:00.912121-0700\ncreated 2021-05-15T17:19:00.912121-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40403/0,v1:10.10.1.2:40404/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.67592 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 43bc4828-c1cc-48ca-9675-46eb187f7ca0\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 e69bf447-cd88-482f-a793-73eecc6882dd\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 622ed0f8-01f7-44c1-bddc-185b1ccec439\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42403\n  w/ user/pass: admin / 57e77196-de82-49d5-a207-89251a52e29d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:19:19 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40403
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.67592
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f336aad3-df2e-402e-92bd-9613b0e499b2
setting min_mon_release = octopus
epoch 0
fsid f336aad3-df2e-402e-92bd-9613b0e499b2
last_changed 2021-05-15T17:19:00.912121-0700
created 2021-05-15T17:19:00.912121-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40403/0,v1:10.10.1.2:40404/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.67592 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 43bc4828-c1cc-48ca-9675-46eb187f7ca0
0
start osd.0
add osd1 e69bf447-cd88-482f-a793-73eecc6882dd
1
start osd.1
add osd2 622ed0f8-01f7-44c1-bddc-185b1ccec439
2
start osd.2


restful urls: https://10.10.1.2:42403
  w/ user/pass: admin / 57e77196-de82-49d5-a207-89251a52e29d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T17:18:34.056-0700 7f3d01edb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:18:34.056-0700 7f3d01edb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:18:34.072-0700 7f0be51d61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:18:34.072-0700 7f0be51d61c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40403,v1:10.10.1.2:40404] --print /tmp/ceph_monmap.67592 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.67592 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.67592 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42403 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55ae4ffe2000 @  0x7fd963dcc680 0x7fd963ded824 0x7fd964588187 0x7fd964590355 0x7fd964588708 0x7fd964588877 0x7fd964589c24 0x7fd9645a1ec1 0x7fd9645145f3 0x7fd964575e97 0x7fd96457db1a 0x7fd963c80d84 0x7fd963d9c609 0x7fd963970293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.KysctHUlkK 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 43bc4828-c1cc-48ca-9675-46eb187f7ca0 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD/ZKBg69IBDxAA2TawovDdUlVZIGD9TQHc4Q== --osd-uuid 43bc4828-c1cc-48ca-9675-46eb187f7ca0 
2021-05-15T17:19:11.905-0700 7fa34a252f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:19:11.925-0700 7fa34a252f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:19:11.925-0700 7fa34a252f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b1f400c000 @  0x7fa34ac1b680 0x7fa34ac3c824 0x55b1e8d9d447 0x55b1e8da54b5 0x55b1e8d9d9c8 0x55b1e8d9db37 0x55b1e8d9eee4 0x55b1e8b6fca1 0x55b1e8d47423 0x55b1e8b602a7 0x55b1e8b6553a 0x7fa34a76ed84 0x7fa34a8f3609 0x7fa34a45c293
2021-05-15T17:19:12.229-0700 7fa34a252f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e69bf447-cd88-482f-a793-73eecc6882dd -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55865b090000 @  0x7f03c10a2680 0x7f03c10c3824 0x558650f2f447 0x558650f374b5 0x558650f2f9c8 0x558650f2fb37 0x558650f30ee4 0x558650d01ca1 0x558650ed9423 0x558650cf22a7 0x558650cf753a 0x7f03c0bf5d84 0x7f03c0d7a609 0x7f03c08e3293
2021-05-15T17:19:12.809-0700 7f03c06d9f00 -1 Falling back to public interface
2021-05-15T17:19:13.061-0700 7f03c06d9f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAAZaBgh6NqHhAAuB3eLzJKPTAy5XfdPEwKnA== --osd-uuid e69bf447-cd88-482f-a793-73eecc6882dd 
2021-05-15T17:19:13.181-0700 7f02d6456f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:19:13.201-0700 7f02d6456f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:19:13.201-0700 7f02d6456f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55c74e964000 @  0x7f02d6e1f680 0x7f02d6e40824 0x55c743de9447 0x55c743df14b5 0x55c743de99c8 0x55c743de9b37 0x55c743deaee4 0x55c743bbbca1 0x55c743d93423 0x55c743bac2a7 0x55c743bb153a 0x7f02d6972d84 0x7f02d6af7609 0x7f02d6660293
2021-05-15T17:19:13.509-0700 7f02d6456f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 622ed0f8-01f7-44c1-bddc-185b1ccec439 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55c3bd8b2000 @  0x7f351e22c680 0x7f351e24d824 0x55c3b37ef447 0x55c3b37f74b5 0x55c3b37ef9c8 0x55c3b37efb37 0x55c3b37f0ee4 0x55c3b35c1ca1 0x55c3b3799423 0x55c3b35b22a7 0x55c3b35b753a 0x7f351dd7fd84 0x7f351df04609 0x7f351da6d293
2021-05-15T17:19:14.185-0700 7f351d863f00 -1 Falling back to public interface
2021-05-15T17:19:14.441-0700 7f351d863f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQABZaBgnD0dNBAAKlCrq/xuCBquJbwVeheRCA== --osd-uuid 622ed0f8-01f7-44c1-bddc-185b1ccec439 
2021-05-15T17:19:14.545-0700 7fa3563c6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:19:14.565-0700 7fa3563c6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:19:14.565-0700 7fa3563c6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x563a364fa000 @  0x7fa356d8f680 0x7fa356db0824 0x563a2b0be447 0x563a2b0c64b5 0x563a2b0be9c8 0x563a2b0beb37 0x563a2b0bfee4 0x563a2ae90ca1 0x563a2b068423 0x563a2ae812a7 0x563a2ae8653a 0x7fa3568e2d84 0x7fa356a67609 0x7fa3565d0293
2021-05-15T17:19:14.945-0700 7fa3563c6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55e0dcf88000 @  0x7fde98f58680 0x7fde98f79824 0x55e0d1393447 0x55e0d139b4b5 0x55e0d13939c8 0x55e0d1393b37 0x55e0d1394ee4 0x55e0d1165ca1 0x55e0d133d423 0x55e0d11562a7 0x55e0d115b53a 0x7fde98aabd84 0x7fde98c30609 0x7fde98799293
2021-05-15T17:19:15.565-0700 7fde9858ff00 -1 Falling back to public interface
2021-05-15T17:19:15.833-0700 7fde9858ff00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T17:19:19,873869339-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T17:19:19,894770485-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T17:19:19,977792270-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:19:19,984288225-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T17:19:23,924662677-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:19:23,931027016-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T17:19:27,719849582-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:19:27,726221947-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T17:19:31,698822969-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:19:31,704850261-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T17:19:39,696356483-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:19:39,702876906-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T17:19:43,906454878-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:19:43,912921788-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T17:19:48,771498652-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:19:48,778009908-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T17:19:53,133155786-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:19:53,139598496-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:19:57,701561504-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:19:57,707905498-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:20:01,969026380-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:20:01,975299952-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T17:20:06,592699833-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:20:06,598911249-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T17:20:10,383148858-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:20:10,390717548-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.06   91      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.94   88      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  1.01   80      up          osd.2  
                       TOTAL  300 GiB  177 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.94/1.06  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T17:20:14,111021125-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:20:38,265099268-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:20:47,576598774-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:20:56,699476518-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:05,729473597-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:14,686092365-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:14,703387663-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:23,917818592-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:23,934735406-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:32,820606037-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:32,837921136-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:42,002782262-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:42,020163340-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:51,043141007-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:51,062342888-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:51,078104135-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:21:51,088169349-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:21:51,112608217-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=693942
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T17:21:51,130104198-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-15T17:21:51,174974234-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:21:51,183292547-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:21:52.689+0000 ffffb2eb2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:21:53.497+0000 ffffb2eb2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:21:53.497+0000 ffffb2eb2010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:21:53.516637+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-16T00:21:53.516686+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T00:21:53.529945+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:21:53.529945+0000     0       0         0         0         0         0           -           0
2021-05-16T00:21:54.530146+0000     1     255     14289     14034    877.07   877.125   0.0161247   0.0178135
2021-05-16T00:21:55.530322+0000     2     255     28283     28028   875.771   874.625   0.0106682   0.0181016
2021-05-16T00:21:56.530522+0000     3     256     42857     42601   887.391   910.812  0.00120192    0.017898
2021-05-16T00:21:57.530689+0000     4     256     57032     56776   886.991   885.938  0.00121622   0.0179189
2021-05-16T00:21:58.530859+0000     5     256     70997     70741   884.125   872.812  0.00812451   0.0180391
2021-05-16T00:21:59.531076+0000     6     255     84925     84670   881.833   870.562  0.00265178   0.0180594
2021-05-16T00:22:00.531265+0000     7     256     98712     98456   878.923   861.625  0.00100595   0.0181386
2021-05-16T00:22:01.531456+0000     8     255    112293    112038   875.147   848.875  0.00777854   0.0182497
2021-05-16T00:22:02.531631+0000     9     255    126544    126289   876.856   890.688   0.0280542   0.0182017
2021-05-16T00:22:03.532120+0000    10     255    140863    140608   878.621   894.938   0.0107431    0.018166
2021-05-16T00:22:04.532310+0000    11     255    155609    155354   882.515   921.625   0.0401634   0.0180947
2021-05-16T00:22:05.532491+0000    12     255    169553    169298   881.584     871.5    0.029865   0.0181127
2021-05-16T00:22:06.532691+0000    13     255    184034    183779   883.376   905.062  0.00756544   0.0180911
2021-05-16T00:22:07.532864+0000    14     256    197560    197304   880.647   845.312   0.0477842   0.0181107
2021-05-16T00:22:08.533020+0000    15     256    208811    208555   868.809   703.188  0.00108895   0.0183863
2021-05-16T00:22:09.533175+0000    16     255    221294    221039   863.267    780.25    0.010314   0.0185133
2021-05-16T00:22:10.533372+0000    17     255    234089    233834   859.518   799.688  0.00591306   0.0185973
2021-05-16T00:22:11.533576+0000    18     256    248377    248121   861.364   892.938   0.0141321    0.018552
2021-05-16T00:22:12.533750+0000    19     255    261222    260967   858.278   802.875   0.0114825   0.0186182
2021-05-16T00:22:13.533943+0000 min lat: 0.0008495 max lat: 0.217092 avg lat: 0.018785
2021-05-16T00:22:13.533943+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:22:13.533943+0000    20      73    272477    272404   851.098   714.812  0.00886989    0.018785
2021-05-16T00:22:14.534275+0000 Total time run:         20.0074
Total writes made:      272477
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     851.174
Stddev Bandwidth:       61.1259
Max bandwidth (MB/sec): 921.625
Min bandwidth (MB/sec): 703.188
Average IOPS:           13618
Stddev IOPS:            978.014
Max IOPS:               14746
Min IOPS:               11251
Average Latency(s):     0.0187836
Stddev Latency(s):      0.0139653
Max latency(s):         0.217092
Min latency(s):         0.0008495

[1;32mlocalhost.localdomain	[2021-05-15T17:22:15,241111289-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 693942


[1;33mlocalhost.localdomain	[2021-05-15T17:22:15,250811462-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:22:39,293802026-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 272.48k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:22:39,311510346-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:22:48,394145462-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 272.48k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:22:48,411784140-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:22:57,757491203-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 272.48k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:22:57,775159870-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:23:06,919911376-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 272.48k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:23:06,937631260-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:23:16,130501229-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 272.48k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:23:16,148441694-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:23:16,162931143-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:23:16,171064005-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:23:16,189725083-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=697509
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T17:23:16,205148765-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-15T17:23:16,244598569-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:23:16,250935503-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dba45226-4b68-45f3-9123-2674e0343454', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dba45226-4b68-45f3-9123-2674e0343454 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWBfoj:/tmp/ceph-asok.NWBfoj -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:23:17.710+0000 ffff8522b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:23:18.515+0000 ffff8522b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:23:18.515+0000 ffff8522b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:23:18.539190+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:23:18.539190+0000     0       0         0         0         0         0           -           0
2021-05-16T00:23:19.539385+0000     1     255     16579     16324   1019.91   1020.25    0.014557   0.0154921
2021-05-16T00:23:20.539602+0000     2     255     34473     34218   1069.02   1118.38    0.014724    0.014859
2021-05-16T00:23:21.539796+0000     3     255     52134     51879   1080.54   1103.81   0.0146061   0.0147275
2021-05-16T00:23:22.540066+0000     4     255     69045     68790   1074.57   1056.94   0.0155008   0.0148181
2021-05-16T00:23:23.540273+0000     5     255     86459     86204   1077.29   1088.38   0.0144194    0.014793
2021-05-16T00:23:24.540511+0000     6     255    104024    103769   1080.66   1097.81   0.0144782   0.0147513
2021-05-16T00:23:25.540747+0000     7     255    121656    121401   1083.67      1102   0.0145047   0.0147142
2021-05-16T00:23:26.540936+0000     8     255    138790    138535   1082.05   1070.88   0.0141157   0.0147393
2021-05-16T00:23:27.541125+0000     9     255    156325    156070   1083.57   1095.94   0.0145349   0.0147207
2021-05-16T00:23:28.541363+0000    10     255    174118    173863   1086.39   1112.06   0.0148425    0.014684
2021-05-16T00:23:29.541561+0000    11     255    190942    190687    1083.2    1051.5   0.0144876   0.0147286
2021-05-16T00:23:30.541732+0000    12     255    208471    208216   1084.22   1095.56    0.014482   0.0147162
2021-05-16T00:23:31.541918+0000    13     255    225957    225702   1084.87   1092.88   0.0147873   0.0147082
2021-05-16T00:23:32.542088+0000    14     255    243409    243154   1085.27   1090.75   0.0153665   0.0147031
2021-05-16T00:23:33.542261+0000    15     255    260607    260352   1084.57   1074.88   0.0144704    0.014714
2021-05-16T00:23:34.542513+0000 Total time run:       15.688
Total reads made:     272477
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   1085.53
Average IOPS:         17368
Stddev IOPS:          414.475
Max IOPS:             17894
Min IOPS:             16324
Average Latency(s):   0.0147053
Max latency(s):       0.0344196
Min latency(s):       0.000359234

[1;32mlocalhost.localdomain	[2021-05-15T17:23:35,152469102-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 697509


[1;33mlocalhost.localdomain	[2021-05-15T17:23:35,161548379-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:23:59,342784635-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 272.48k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:23:59,360668416-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:24:08,548554579-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 272.48k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:24:08,565862850-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:24:17,600058623-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 272.48k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:24:17,617282820-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:24:26,765771864-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 272.48k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:24:26,782965065-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:24:35,920966116-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 272.48k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:24:35,938414522-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:24:35,952370103-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T17:24:35,957679516-07:00][RUNNING][ROUND 5/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:24:35,966064161-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T17:24:35,982990812-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40497\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.68704\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid cd5c086a-87f2-4528-901e-d184c5d4d063\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid cd5c086a-87f2-4528-901e-d184c5d4d063\nlast_changed 2021-05-15T17:25:04.680949-0700\ncreated 2021-05-15T17:25:04.680949-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40497/0,v1:10.10.1.2:40498/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.68704 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 8c81f775-e145-4bcf-bab0-7de71b7da757\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 35e894c7-0617-47ba-abca-9b6341e76fa8\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 f9b91bcb-5d95-439b-969d-50dbc897e199\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42497\n  w/ user/pass: admin / 02479da6-b884-47d1-b28a-30bf8fa7096c\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:25:23 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40497
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.68704
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid cd5c086a-87f2-4528-901e-d184c5d4d063
setting min_mon_release = octopus
epoch 0
fsid cd5c086a-87f2-4528-901e-d184c5d4d063
last_changed 2021-05-15T17:25:04.680949-0700
created 2021-05-15T17:25:04.680949-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40497/0,v1:10.10.1.2:40498/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.68704 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 8c81f775-e145-4bcf-bab0-7de71b7da757
0
start osd.0
add osd1 35e894c7-0617-47ba-abca-9b6341e76fa8
1
start osd.1
add osd2 f9b91bcb-5d95-439b-969d-50dbc897e199
2
start osd.2


restful urls: https://10.10.1.2:42497
  w/ user/pass: admin / 02479da6-b884-47d1-b28a-30bf8fa7096c


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T17:24:37.880-0700 7fde412591c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:24:37.880-0700 7fde412591c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:24:37.896-0700 7f0c9f8d91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:24:37.896-0700 7f0c9f8d91c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40497,v1:10.10.1.2:40498] --print /tmp/ceph_monmap.68704 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.68704 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.68704 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42497 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x560856214000 @  0x7f90e9363680 0x7f90e9384824 0x7f90e9b1f187 0x7f90e9b27355 0x7f90e9b1f708 0x7f90e9b1f877 0x7f90e9b20c24 0x7f90e9b38ec1 0x7f90e9aab5f3 0x7f90e9b0ce97 0x7f90e9b14b1a 0x7f90e9217d84 0x7f90e9333609 0x7f90e8f07293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.TDdGLyYlY9 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8c81f775-e145-4bcf-bab0-7de71b7da757 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBrZqBgx3fmARAA9LeqY79BULq97rg1n5JpnQ== --osd-uuid 8c81f775-e145-4bcf-bab0-7de71b7da757 
2021-05-15T17:25:15.672-0700 7fb15fabcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:25:15.692-0700 7fb15fabcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:25:15.692-0700 7fb15fabcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x563f9b23c000 @  0x7fb160485680 0x7fb1604a6824 0x563f8fbcf447 0x563f8fbd74b5 0x563f8fbcf9c8 0x563f8fbcfb37 0x563f8fbd0ee4 0x563f8f9a1ca1 0x563f8fb79423 0x563f8f9922a7 0x563f8f99753a 0x7fb15ffd8d84 0x7fb16015d609 0x7fb15fcc6293
2021-05-15T17:25:15.984-0700 7fb15fabcf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 35e894c7-0617-47ba-abca-9b6341e76fa8 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5587b19f4000 @  0x7f6df12fa680 0x7f6df131b824 0x5587a70ff447 0x5587a71074b5 0x5587a70ff9c8 0x5587a70ffb37 0x5587a7100ee4 0x5587a6ed1ca1 0x5587a70a9423 0x5587a6ec22a7 0x5587a6ec753a 0x7f6df0e4dd84 0x7f6df0fd2609 0x7f6df0b3b293
2021-05-15T17:25:16.584-0700 7f6df0931f00 -1 Falling back to public interface
2021-05-15T17:25:16.840-0700 7f6df0931f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBsZqBg8z11ERAAcPRNM9Rb0AFt5W1J8rCKQA== --osd-uuid 35e894c7-0617-47ba-abca-9b6341e76fa8 
2021-05-15T17:25:16.944-0700 7f5e27162f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:25:16.964-0700 7f5e27162f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:25:16.964-0700 7f5e27162f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x564a7fe9e000 @  0x7f5e27b2b680 0x7f5e27b4c824 0x564a74d4d447 0x564a74d554b5 0x564a74d4d9c8 0x564a74d4db37 0x564a74d4eee4 0x564a74b1fca1 0x564a74cf7423 0x564a74b102a7 0x564a74b1553a 0x7f5e2767ed84 0x7f5e27803609 0x7f5e2736c293
2021-05-15T17:25:17.272-0700 7f5e27162f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f9b91bcb-5d95-439b-969d-50dbc897e199 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x565038c94000 @  0x7f4cc7682680 0x7f4cc76a3824 0x56502d32d447 0x56502d3354b5 0x56502d32d9c8 0x56502d32db37 0x56502d32eee4 0x56502d0ffca1 0x56502d2d7423 0x56502d0f02a7 0x56502d0f553a 0x7f4cc71d5d84 0x7f4cc735a609 0x7f4cc6ec3293
2021-05-15T17:25:17.912-0700 7f4cc6cb9f00 -1 Falling back to public interface
2021-05-15T17:25:18.164-0700 7f4cc6cb9f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBtZqBg/vP3IxAABat6vjkKMiMFHtcIxl/gTw== --osd-uuid f9b91bcb-5d95-439b-969d-50dbc897e199 
2021-05-15T17:25:18.264-0700 7fa55b621f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:25:18.284-0700 7fa55b621f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:25:18.284-0700 7fa55b621f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55eb7a050000 @  0x7fa55bfea680 0x7fa55c00b824 0x55eb6e2f0447 0x55eb6e2f84b5 0x55eb6e2f09c8 0x55eb6e2f0b37 0x55eb6e2f1ee4 0x55eb6e0c2ca1 0x55eb6e29a423 0x55eb6e0b32a7 0x55eb6e0b853a 0x7fa55bb3dd84 0x7fa55bcc2609 0x7fa55b82b293
2021-05-15T17:25:18.616-0700 7fa55b621f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55e3c90a8000 @  0x7fc04327c680 0x7fc04329d824 0x55e3bd97e447 0x55e3bd9864b5 0x55e3bd97e9c8 0x55e3bd97eb37 0x55e3bd97fee4 0x55e3bd750ca1 0x55e3bd928423 0x55e3bd7412a7 0x55e3bd74653a 0x7fc042dcfd84 0x7fc042f54609 0x7fc042abd293
2021-05-15T17:25:19.228-0700 7fc0428b3f00 -1 Falling back to public interface
2021-05-15T17:25:19.496-0700 7fc0428b3f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T17:25:23,599739459-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T17:25:23,621148328-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T17:25:23,703490992-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:25:23,709898042-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T17:25:27,723244926-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:25:27,729933782-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T17:25:31,539454440-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:25:31,545926571-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T17:25:35,423343967-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:25:35,429885424-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T17:25:43,329906648-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:25:43,336267896-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T17:25:47,404283631-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:25:47,410612728-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T17:25:51,801414566-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:25:51,807716779-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T17:25:56,289198570-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:25:56,295681742-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:26:00,893680063-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:26:00,899900407-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:26:05,404860793-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:26:05,411151934-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T17:26:09,684265163-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:26:09,690549298-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T17:26:13,749048558-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:26:13,755443545-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.06   90      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.94   89      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  1.01   80      up          osd.2  
                       TOTAL  300 GiB  177 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.94/1.06  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T17:26:17,479405828-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:26:41,516436345-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:26:50,563300772-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (19s)
      [========....................] (remaining: 47s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:26:59,741431471-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (29s)
      [========....................] (remaining: 64s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:27:08,739158749-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 81s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:27:17,707568046-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   peering
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 39s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:27:26,602005402-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:27:26,619709212-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:27:35,816590895-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:27:35,834380413-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:27:45,224425250-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:27:45,241752468-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:27:54,331195640-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:27:54,348589048-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:28:03,539244440-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:28:03,556528585-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:28:03,570888245-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:28:03,579358472-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:28:03,598443030-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=711060
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T17:28:03,614250065-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-15T17:28:03,655396504-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:28:03,661960506-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:28:05.181+0000 ffff9e9ae010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:28:05.974+0000 ffff9e9ae010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:28:05.974+0000 ffff9e9ae010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:28:05.996495+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-16T00:28:05.996541+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T00:28:06.009856+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:28:06.009856+0000     0       0         0         0         0         0           -           0
2021-05-16T00:28:07.010046+0000     1     255     15629     15374   960.811   960.875   0.0165664   0.0164007
2021-05-16T00:28:08.010246+0000     2     255     31992     31737   991.649   1022.69   0.0182682   0.0160258
2021-05-16T00:28:09.010438+0000     3     255     47679     47424   987.849   980.438   0.0213371   0.0161126
2021-05-16T00:28:10.010605+0000     4     255     63405     63150   986.565   982.875   0.0111719   0.0161531
2021-05-16T00:28:11.010769+0000     5     255     78920     78665   983.157   969.688  0.00773088   0.0162225
2021-05-16T00:28:12.011055+0000     6     256     94510     94254   981.636   974.312   0.0136742   0.0162551
2021-05-16T00:28:13.011223+0000     7     255    110023    109768   979.897   969.625   0.0180856   0.0162887
2021-05-16T00:28:14.011379+0000     8     255    125423    125168   977.704     962.5   0.0123309   0.0163279
2021-05-16T00:28:15.011552+0000     9     255    141023    140768   977.385       975    0.015263   0.0163409
2021-05-16T00:28:16.011716+0000    10     255    155861    155606   972.369   927.375   0.0234868   0.0164255
2021-05-16T00:28:17.011900+0000    11     255    170653    170398   968.001     924.5  0.00705002    0.016501
2021-05-16T00:28:18.012075+0000    12     255    185512    185257   964.712   928.688  0.00659047   0.0165551
2021-05-16T00:28:19.012237+0000    13     255    199694    199439   958.675   886.375   0.0235357   0.0166635
2021-05-16T00:28:20.012437+0000    14     255    213756    213501   952.962   878.875  0.00963023   0.0167643
2021-05-16T00:28:21.012632+0000    15     255    229898    229643   956.677   1008.88  0.00817073   0.0167028
2021-05-16T00:28:22.012838+0000    16     255    244813    244558   955.134   932.188   0.0144804   0.0167291
2021-05-16T00:28:23.013043+0000    17     255    259300    259045     952.2   905.438  0.00534195   0.0167797
2021-05-16T00:28:24.013209+0000    18     255    272960    272705   946.722    853.75   0.0149519   0.0168826
2021-05-16T00:28:25.013396+0000    19     255    287053    286798   943.245   880.812   0.0189337   0.0169411
2021-05-16T00:28:26.013683+0000 min lat: 0.000848673 max lat: 0.156911 avg lat: 0.0169732
2021-05-16T00:28:26.013683+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:28:26.013683+0000    20      68    301544    301476   941.938   917.375  0.00232277   0.0169732
2021-05-16T00:28:27.013937+0000 Total time run:         20.0141
Total writes made:      301544
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     941.661
Stddev Bandwidth:       45.836
Max bandwidth (MB/sec): 1022.69
Min bandwidth (MB/sec): 853.75
Average IOPS:           15066
Stddev IOPS:            733.375
Max IOPS:               16363
Min IOPS:               13660
Average Latency(s):     0.0169727
Stddev Latency(s):      0.010751
Max latency(s):         0.156911
Min latency(s):         0.000848673

[1;32mlocalhost.localdomain	[2021-05-15T17:28:27,648669023-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 711060


[1;33mlocalhost.localdomain	[2021-05-15T17:28:27,657630054-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:28:51,711824744-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.55k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:28:51,729395553-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:29:01,061094891-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.55k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:29:01,078753373-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:29:10,327785459-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.55k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:29:10,345867760-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:29:19,846429125-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.55k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:29:19,863927636-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:29:29,172586473-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.55k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:29:29,190591466-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:29:29,204923232-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:29:29,213514032-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:29:29,232422696-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=714551
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T17:29:29,248548738-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-15T17:29:29,288099463-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:29:29,294485129-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e2e8f119-9d43-48ec-93a1-5363e0977031', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e2e8f119-9d43-48ec-93a1-5363e0977031 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FKiUN2:/tmp/ceph-asok.FKiUN2 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:29:30.927+0000 ffffbe51f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:29:32.060+0000 ffffbe51f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:29:32.060+0000 ffffbe51f010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:29:32.085496+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:29:32.085496+0000     0       0         0         0         0         0           -           0
2021-05-16T00:29:33.085669+0000     1     255     16836     16581   1035.88   1036.31   0.0144646   0.0152722
2021-05-16T00:29:34.085871+0000     2     256     33799     33543   1047.89   1060.12   0.0143166   0.0151686
2021-05-16T00:29:35.086093+0000     3     255     51035     50780   1057.62   1077.31   0.0144613   0.0150525
2021-05-16T00:29:36.086356+0000     4     255     68843     68588   1071.39      1113   0.0145966   0.0148703
2021-05-16T00:29:37.086542+0000     5     255     84800     84545   1056.54   997.312   0.0144987   0.0150867
2021-05-16T00:29:38.086757+0000     6     255    102505    102250   1064.84   1106.56   0.0143194    0.014974
2021-05-16T00:29:39.086951+0000     7     255    119862    119607   1067.66   1084.81   0.0143321   0.0149378
2021-05-16T00:29:40.087139+0000     8     255    136891    136636   1067.22   1064.31   0.0158731   0.0149449
2021-05-16T00:29:41.087340+0000     9     255    155003    154748   1074.39      1132   0.0143116   0.0148485
2021-05-16T00:29:42.087531+0000    10     255    172688    172433   1077.46   1105.31    0.014236   0.0148079
2021-05-16T00:29:43.087716+0000    11     255    188980    188725   1072.06   1018.25  0.00307455    0.014872
2021-05-16T00:29:44.087914+0000    12     256    205951    205695   1071.09   1060.62   0.0139103   0.0148984
2021-05-16T00:29:45.088165+0000    13     255    223397    223142   1072.56   1090.44   0.0146938   0.0148786
2021-05-16T00:29:46.088426+0000    14     255    241259    241004   1075.67   1116.38   0.0144568   0.0148364
2021-05-16T00:29:47.088625+0000    15     256    256996    256740   1069.51     983.5   0.0102555   0.0148976
2021-05-16T00:29:48.088840+0000    16     255    273309    273054   1066.38   1019.62   0.0143128    0.014967
2021-05-16T00:29:49.089042+0000    17     255    290301    290046   1066.11      1062   0.0147329   0.0149712
2021-05-16T00:29:50.089301+0000 Total time run:       17.7135
Total reads made:     301544
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   1063.97
Average IOPS:         17023
Stddev IOPS:          696.149
Max IOPS:             18112
Min IOPS:             15736
Average Latency(s):   0.0150051
Max latency(s):       0.0866482
Min latency(s):       0.000332874

[1;32mlocalhost.localdomain	[2021-05-15T17:29:50,738489419-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 714551


[1;33mlocalhost.localdomain	[2021-05-15T17:29:50,747294841-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:30:14,774598547-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.55k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:30:14,792399760-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:30:24,072018842-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.55k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:30:24,092085078-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:30:33,291079714-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.55k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:30:33,309046220-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:30:42,250440660-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.55k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:30:42,268472012-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:30:51,279548903-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 301.55k objects, 18 GiB
    usage:   37 GiB used, 263 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:30:51,297726073-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:30:51,312909210-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T17:30:51,321927693-07:00][RUNNING][ROUND 1/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:30:51,330323382-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T17:30:51,347699912-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40426\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.69790\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 7e9c4c78-0c4f-4b7e-8f3e-22092e7a07f6\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 7e9c4c78-0c4f-4b7e-8f3e-22092e7a07f6\nlast_changed 2021-05-15T17:31:19.009329-0700\ncreated 2021-05-15T17:31:19.009329-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40426/0,v1:10.10.1.2:40427/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.69790 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 69f66847-f5c0-428a-bad8-c0d899a8032f\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 3d38b67b-6294-4f02-8f6e-fc60b513f9b1\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 167a34ce-671e-4734-9235-c646832e061a\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42426\n  w/ user/pass: admin / a2fa6ad9-80f9-4950-bafb-9cd28aefd1a7\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:31:37 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40426
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.69790
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 7e9c4c78-0c4f-4b7e-8f3e-22092e7a07f6
setting min_mon_release = octopus
epoch 0
fsid 7e9c4c78-0c4f-4b7e-8f3e-22092e7a07f6
last_changed 2021-05-15T17:31:19.009329-0700
created 2021-05-15T17:31:19.009329-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40426/0,v1:10.10.1.2:40427/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.69790 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 69f66847-f5c0-428a-bad8-c0d899a8032f
0
start osd.0
add osd1 3d38b67b-6294-4f02-8f6e-fc60b513f9b1
1
start osd.1
add osd2 167a34ce-671e-4734-9235-c646832e061a
2
start osd.2


restful urls: https://10.10.1.2:42426
  w/ user/pass: admin / a2fa6ad9-80f9-4950-bafb-9cd28aefd1a7


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T17:30:53.267-0700 7f1137a4a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:30:53.267-0700 7f1137a4a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:30:53.283-0700 7f40ecf0a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:30:53.283-0700 7f40ecf0a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40426,v1:10.10.1.2:40427] --print /tmp/ceph_monmap.69790 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.69790 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.69790 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42426 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x563860b94000 @  0x7f283fa57680 0x7f283fa78824 0x7f2840213187 0x7f284021b355 0x7f2840213708 0x7f2840213877 0x7f2840214c24 0x7f284022cec1 0x7f284019f5f3 0x7f2840200e97 0x7f2840208b1a 0x7f283f90bd84 0x7f283fa27609 0x7f283f5fb293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.AJ8LqNOg4u 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 69f66847-f5c0-428a-bad8-c0d899a8032f -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDhZ6BgmBxIFxAAEw0YZXmBl5SHVgyxV4zV+w== --osd-uuid 69f66847-f5c0-428a-bad8-c0d899a8032f 
2021-05-15T17:31:30.036-0700 7f6d57364f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:31:30.056-0700 7f6d57364f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:31:30.056-0700 7f6d57364f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x560c08ce2000 @  0x7f6d57d2d680 0x7f6d57d4e824 0x560bfe48e447 0x560bfe4964b5 0x560bfe48e9c8 0x560bfe48eb37 0x560bfe48fee4 0x560bfe260ca1 0x560bfe438423 0x560bfe2512a7 0x560bfe25653a 0x7f6d57880d84 0x7f6d57a05609 0x7f6d5756e293
2021-05-15T17:31:30.360-0700 7f6d57364f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3d38b67b-6294-4f02-8f6e-fc60b513f9b1 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x562764840000 @  0x7f9ae1c33680 0x7f9ae1c54824 0x562759654447 0x56275965c4b5 0x5627596549c8 0x562759654b37 0x562759655ee4 0x562759426ca1 0x5627595fe423 0x5627594172a7 0x56275941c53a 0x7f9ae1786d84 0x7f9ae190b609 0x7f9ae1474293
2021-05-15T17:31:30.920-0700 7f9ae126af00 -1 Falling back to public interface
2021-05-15T17:31:31.172-0700 7f9ae126af00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDiZ6BgEwRQJRAAFawmtgmgrelSDTN0dqj3ow== --osd-uuid 3d38b67b-6294-4f02-8f6e-fc60b513f9b1 
2021-05-15T17:31:31.268-0700 7f8b3b5b1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:31:31.288-0700 7f8b3b5b1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:31:31.288-0700 7f8b3b5b1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5603df5c2000 @  0x7f8b3bf7a680 0x7f8b3bf9b824 0x5603d4d93447 0x5603d4d9b4b5 0x5603d4d939c8 0x5603d4d93b37 0x5603d4d94ee4 0x5603d4b65ca1 0x5603d4d3d423 0x5603d4b562a7 0x5603d4b5b53a 0x7f8b3bacdd84 0x7f8b3bc52609 0x7f8b3b7bb293
2021-05-15T17:31:31.588-0700 7f8b3b5b1f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 167a34ce-671e-4734-9235-c646832e061a -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55bad4c6c000 @  0x7f9b56c43680 0x7f9b56c64824 0x55bac9c47447 0x55bac9c4f4b5 0x55bac9c479c8 0x55bac9c47b37 0x55bac9c48ee4 0x55bac9a19ca1 0x55bac9bf1423 0x55bac9a0a2a7 0x55bac9a0f53a 0x7f9b56796d84 0x7f9b5691b609 0x7f9b56484293
2021-05-15T17:31:32.228-0700 7f9b5627af00 -1 Falling back to public interface
2021-05-15T17:31:32.484-0700 7f9b5627af00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDjZ6Bgc47lNRAAw+lkcDCW+/q1QYTFLgHe+w== --osd-uuid 167a34ce-671e-4734-9235-c646832e061a 
2021-05-15T17:31:32.596-0700 7f3f78336f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:31:32.616-0700 7f3f78336f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:31:32.616-0700 7f3f78336f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x556443672000 @  0x7f3f78cff680 0x7f3f78d20824 0x55643874c447 0x5564387544b5 0x55643874c9c8 0x55643874cb37 0x55643874dee4 0x55643851eca1 0x5564386f6423 0x55643850f2a7 0x55643851453a 0x7f3f78852d84 0x7f3f789d7609 0x7f3f78540293
2021-05-15T17:31:32.932-0700 7f3f78336f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55eddaf78000 @  0x7f93cd01e680 0x7f93cd03f824 0x55edd0426447 0x55edd042e4b5 0x55edd04269c8 0x55edd0426b37 0x55edd0427ee4 0x55edd01f8ca1 0x55edd03d0423 0x55edd01e92a7 0x55edd01ee53a 0x7f93ccb71d84 0x7f93cccf6609 0x7f93cc85f293
2021-05-15T17:31:33.540-0700 7f93cc655f00 -1 Falling back to public interface
2021-05-15T17:31:33.800-0700 7f93cc655f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T17:31:37,965537215-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T17:31:37,986948249-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T17:31:38,067926282-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:31:38,074283779-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T17:31:42,093431922-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:31:42,099658667-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T17:31:45,947421296-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:31:45,953785152-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T17:31:49,820603863-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:31:49,826953614-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T17:31:58,191543040-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:31:58,199392085-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T17:32:02,171057245-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:32:02,177574199-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T17:32:06,706656842-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:32:06,712809882-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T17:32:11,189681978-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:32:11,195954620-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:32:15,693827968-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:32:15,700191557-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:32:19,971924467-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:32:19,978296845-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T17:32:24,726082047-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:32:24,732902129-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T17:32:28,636585512-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:32:28,644753011-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.08   89      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.89   90      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  1.03   80      up          osd.2  
                       TOTAL  300 GiB  174 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.89/1.08  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T17:32:32,400159272-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:32:56,453143542-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (15s)
      [=======.....................] (remaining: 40s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:33:05,974965131-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 59s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:33:15,099753114-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (30s)
      [========....................] (remaining: 67s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:33:24,091632961-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 81s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:33:33,234043530-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 100s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:33:42,558858873-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:33:42,576733269-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:33:51,614290822-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:33:51,632206168-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:34:00,640309438-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:34:00,658464328-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:34:09,910118858-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:34:09,927738345-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:34:19,166063066-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:34:19,183965994-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:34:19,198385576-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:34:19,206686026-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:34:19,226250812-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=728250
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T17:34:19,242312057-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-15T17:34:19,284015602-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:34:19,290510422-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:34:20.825+0000 ffffa0775010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:34:21.617+0000 ffffa0775010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:34:21.617+0000 ffffa0775010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:34:21.639672+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-16T00:34:21.639736+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T00:34:21.689306+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:34:21.689306+0000     0       0         0         0         0         0           -           0
2021-05-16T00:34:22.689529+0000     1     255      5024      4769   1192.18   1192.25   0.0413203   0.0515485
2021-05-16T00:34:23.689763+0000     2     255      9865      9610   1201.07   1210.25   0.0864023    0.051979
2021-05-16T00:34:24.690110+0000     3     255     14776     14521   1209.82   1227.75   0.0392417   0.0521034
2021-05-16T00:34:25.690333+0000     4     255     19653     19398   1212.11   1219.25   0.0475676   0.0521174
2021-05-16T00:34:26.691052+0000     5     255     24539     24284   1213.82    1221.5    0.100269   0.0522348
2021-05-16T00:34:27.691323+0000     6     256     29271     29015   1208.58   1182.75   0.0409423   0.0525382
2021-05-16T00:34:28.691519+0000     7     255     33839     33584   1199.08   1142.25   0.0410061   0.0530211
2021-05-16T00:34:29.691788+0000     8     255     37954     37699   1177.75   1028.75   0.0422229   0.0540269
2021-05-16T00:34:30.692027+0000     9     255     42496     42241   1173.03    1135.5   0.0495775   0.0542453
2021-05-16T00:34:31.692225+0000    10     255     46995     46740   1168.18   1124.75   0.0432633   0.0544981
2021-05-16T00:34:32.692410+0000    11     255     50835     50580   1149.24       960   0.0503971   0.0554467
2021-05-16T00:34:33.692594+0000    12     255     55374     55119   1148.01   1134.75   0.0481561   0.0554417
2021-05-16T00:34:34.692785+0000    13     255     58707     58452   1123.79    833.25   0.0606319   0.0566909
2021-05-16T00:34:35.692965+0000    14     255     63402     63147   1127.34   1173.75   0.0522901   0.0565515
2021-05-16T00:34:36.693167+0000    15     255     67162     66907   1114.84       940   0.0552947   0.0572078
2021-05-16T00:34:37.693395+0000    16     255     71167     70912   1107.73   1001.25   0.0903307   0.0576093
2021-05-16T00:34:38.693768+0000    17     255     75232     74977   1102.32   1016.25   0.0401158   0.0578989
2021-05-16T00:34:39.693974+0000    18     255     79820     79565   1104.79      1147   0.0708583    0.057739
2021-05-16T00:34:40.694149+0000    19     255     83722     83467   1097.98     975.5   0.0526284   0.0581294
2021-05-16T00:34:41.694355+0000 min lat: 0.00448154 max lat: 0.350661 avg lat: 0.0579643
2021-05-16T00:34:41.694355+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:34:41.694355+0000    20      64     88290     88226   1102.56   1189.75  0.00463885   0.0579643
2021-05-16T00:34:42.694601+0000 Total time run:         20.0163
Total writes made:      88290
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     1102.73
Stddev Bandwidth:       114
Max bandwidth (MB/sec): 1227.75
Min bandwidth (MB/sec): 833.25
Average IOPS:           4410
Stddev IOPS:            455.999
Max IOPS:               4911
Min IOPS:               3333
Average Latency(s):     0.057943
Stddev Latency(s):      0.0253297
Max latency(s):         0.350661
Min latency(s):         0.00448154

[1;32mlocalhost.localdomain	[2021-05-15T17:34:43,332903449-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 728250


[1;33mlocalhost.localdomain	[2021-05-15T17:34:43,341892778-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:35:07,788273959-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 88.29k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:35:07,806274485-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:35:16,935372903-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 88.29k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:35:16,953468376-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:35:26,053965189-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 88.29k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:35:26,072464320-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:35:35,161307684-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 88.29k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:35:35,179616212-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:35:44,381030513-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 88.29k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:35:44,399384523-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:35:44,414155567-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:35:44,422593666-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:35:44,442204870-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=731815
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T17:35:44,458189044-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-15T17:35:44,502279623-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:35:44,510152220-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0b348005-1c52-4f78-b8a0-2c15f472efa4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0b348005-1c52-4f78-b8a0-2c15f472efa4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oD1JnS:/tmp/ceph-asok.oD1JnS -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:35:46.006+0000 ffffa39e6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:35:46.822+0000 ffffa39e6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:35:46.822+0000 ffffa39e6010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:35:46.847105+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:35:46.847105+0000     0       0         0         0         0         0           -           0
2021-05-16T00:35:47.847338+0000     1     255      6200      5945    1485.7   1486.25   0.0444065   0.0419079
2021-05-16T00:35:48.847619+0000     2     255     12257     12002   1499.76   1514.25   0.0428574   0.0420622
2021-05-16T00:35:49.847963+0000     3     255     18906     18651   1553.74   1662.25   0.0379568   0.0407917
2021-05-16T00:35:50.848162+0000     4     255     25528     25273   1579.09    1655.5    0.041146   0.0401917
2021-05-16T00:35:51.848392+0000     5     255     31615     31360   1567.55   1521.75   0.0439329   0.0405304
2021-05-16T00:35:52.848687+0000     6     255     38051     37796   1574.38      1609     0.03736   0.0404022
2021-05-16T00:35:53.848929+0000     7     256     44407     44151   1576.38   1588.75   0.0413303   0.0403599
2021-05-16T00:35:54.849176+0000     8     255     50666     50411   1574.91      1565   0.0428847   0.0404059
2021-05-16T00:35:55.849399+0000     9     255     57002     56747   1575.88      1584   0.0404101   0.0404007
2021-05-16T00:35:56.849652+0000    10     255     63458     63203   1579.65      1614   0.0414321   0.0403155
2021-05-16T00:35:57.849943+0000    11     255     69766     69511   1579.37      1577    0.038798   0.0403349
2021-05-16T00:35:58.850238+0000    12     255     75638     75383   1570.05      1468   0.0427658   0.0405756
2021-05-16T00:35:59.850553+0000    13     255     81486     81231    1561.7      1462    0.043715   0.0407982
2021-05-16T00:36:00.850886+0000    14     255     87788     87533   1562.65    1575.5   0.0450489   0.0407646
2021-05-16T00:36:01.851209+0000 Total time run:       14.177
Total reads made:     88290
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1556.92
Average IOPS:         6227
Stddev IOPS:          257.273
Max IOPS:             6649
Min IOPS:             5848
Average Latency(s):   0.040801
Max latency(s):       0.159523
Min latency(s):       0.000496039

[1;32mlocalhost.localdomain	[2021-05-15T17:36:02,507582204-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 731815


[1;33mlocalhost.localdomain	[2021-05-15T17:36:02,516644325-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:36:26,465439302-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 88.29k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:36:26,483463384-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:36:36,001550239-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 88.29k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:36:36,019771721-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:36:45,117943726-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 88.29k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:36:45,136060978-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:36:54,254386286-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 88.29k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:36:54,272886118-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:37:03,446643834-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 88.29k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:37:03,465242184-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:37:03,480360738-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T17:37:03,486264672-07:00][RUNNING][ROUND 2/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:37:03,494625362-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T17:37:03,512008679-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40244\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.70893\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f2cbf645-cca8-4c58-b098-fdc2300e88ba\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid f2cbf645-cca8-4c58-b098-fdc2300e88ba\nlast_changed 2021-05-15T17:37:31.411193-0700\ncreated 2021-05-15T17:37:31.411193-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40244/0,v1:10.10.1.2:40245/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.70893 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 c46787af-1fd7-47cb-b072-0dec21c450b0\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 54de0958-a24b-4092-82f6-59cc4f61375f\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 98f74801-5c8a-459c-b482-db072ffd083c\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42244\n  w/ user/pass: admin / c7c2448a-b53b-4c40-9d4a-2d43d863a174\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:37:50 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40244
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.70893
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f2cbf645-cca8-4c58-b098-fdc2300e88ba
setting min_mon_release = octopus
epoch 0
fsid f2cbf645-cca8-4c58-b098-fdc2300e88ba
last_changed 2021-05-15T17:37:31.411193-0700
created 2021-05-15T17:37:31.411193-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40244/0,v1:10.10.1.2:40245/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.70893 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 c46787af-1fd7-47cb-b072-0dec21c450b0
0
start osd.0
add osd1 54de0958-a24b-4092-82f6-59cc4f61375f
1
start osd.1
add osd2 98f74801-5c8a-459c-b482-db072ffd083c
2
start osd.2


restful urls: https://10.10.1.2:42244
  w/ user/pass: admin / c7c2448a-b53b-4c40-9d4a-2d43d863a174


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T17:37:05.543-0700 7f82d1c2f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:37:05.543-0700 7f82d1c2f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:37:05.559-0700 7f4c3dba31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:37:05.559-0700 7f4c3dba31c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40244,v1:10.10.1.2:40245] --print /tmp/ceph_monmap.70893 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.70893 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.70893 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42244 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55d85b4c4000 @  0x7f7d024ed680 0x7f7d0250e824 0x7f7d02ca9187 0x7f7d02cb1355 0x7f7d02ca9708 0x7f7d02ca9877 0x7f7d02caac24 0x7f7d02cc2ec1 0x7f7d02c355f3 0x7f7d02c96e97 0x7f7d02c9eb1a 0x7f7d023a1d84 0x7f7d024bd609 0x7f7d02091293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.X5WkS6ydK4 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c46787af-1fd7-47cb-b072-0dec21c450b0 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBVaaBg7QKuJxAAv9ETE+tvlzTK4Od2zfpEMw== --osd-uuid c46787af-1fd7-47cb-b072-0dec21c450b0 
2021-05-15T17:37:42.287-0700 7ff217c58f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:37:42.307-0700 7ff217c58f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:37:42.307-0700 7ff217c58f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55eecb49c000 @  0x7ff218621680 0x7ff218642824 0x55eec0c2f447 0x55eec0c374b5 0x55eec0c2f9c8 0x55eec0c2fb37 0x55eec0c30ee4 0x55eec0a01ca1 0x55eec0bd9423 0x55eec09f22a7 0x55eec09f753a 0x7ff218174d84 0x7ff2182f9609 0x7ff217e62293
2021-05-15T17:37:42.611-0700 7ff217c58f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 54de0958-a24b-4092-82f6-59cc4f61375f -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x558edc68a000 @  0x7f3a52064680 0x7f3a52085824 0x558ed0851447 0x558ed08594b5 0x558ed08519c8 0x558ed0851b37 0x558ed0852ee4 0x558ed0623ca1 0x558ed07fb423 0x558ed06142a7 0x558ed061953a 0x7f3a51bb7d84 0x7f3a51d3c609 0x7f3a518a5293
2021-05-15T17:37:43.223-0700 7f3a5169bf00 -1 Falling back to public interface
2021-05-15T17:37:43.483-0700 7f3a5169bf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBWaaBgU5IeNxAA0Q/2t1XRQIa2joURXBkS/w== --osd-uuid 54de0958-a24b-4092-82f6-59cc4f61375f 
2021-05-15T17:37:43.595-0700 7fc9a57e7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:37:43.615-0700 7fc9a57e7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:37:43.615-0700 7fc9a57e7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x564a5710e000 @  0x7fc9a61b0680 0x7fc9a61d1824 0x564a4c54a447 0x564a4c5524b5 0x564a4c54a9c8 0x564a4c54ab37 0x564a4c54bee4 0x564a4c31cca1 0x564a4c4f4423 0x564a4c30d2a7 0x564a4c31253a 0x7fc9a5d03d84 0x7fc9a5e88609 0x7fc9a59f1293
2021-05-15T17:37:43.931-0700 7fc9a57e7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 98f74801-5c8a-459c-b482-db072ffd083c -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55e4c3a36000 @  0x7fb3df05a680 0x7fb3df07b824 0x55e4b8537447 0x55e4b853f4b5 0x55e4b85379c8 0x55e4b8537b37 0x55e4b8538ee4 0x55e4b8309ca1 0x55e4b84e1423 0x55e4b82fa2a7 0x55e4b82ff53a 0x7fb3debadd84 0x7fb3ded32609 0x7fb3de89b293
2021-05-15T17:37:44.599-0700 7fb3de691f00 -1 Falling back to public interface
2021-05-15T17:37:44.871-0700 7fb3de691f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBYaaBgKqWTERAAkqPiorBqcbTxmRpyoWNh9w== --osd-uuid 98f74801-5c8a-459c-b482-db072ffd083c 
2021-05-15T17:37:44.951-0700 7f575a580f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:37:44.971-0700 7f575a580f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:37:44.971-0700 7f575a580f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55edf6b28000 @  0x7f575af49680 0x7f575af6a824 0x55edeb41c447 0x55edeb4244b5 0x55edeb41c9c8 0x55edeb41cb37 0x55edeb41dee4 0x55edeb1eeca1 0x55edeb3c6423 0x55edeb1df2a7 0x55edeb1e453a 0x7f575aa9cd84 0x7f575ac21609 0x7f575a78a293
2021-05-15T17:37:45.295-0700 7f575a580f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55fcb7f4c000 @  0x7ffaee0d2680 0x7ffaee0f3824 0x55fcacde9447 0x55fcacdf14b5 0x55fcacde99c8 0x55fcacde9b37 0x55fcacdeaee4 0x55fcacbbbca1 0x55fcacd93423 0x55fcacbac2a7 0x55fcacbb153a 0x7ffaedc25d84 0x7ffaeddaa609 0x7ffaed913293
2021-05-15T17:37:45.947-0700 7ffaed709f00 -1 Falling back to public interface
2021-05-15T17:37:46.215-0700 7ffaed709f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T17:37:50,273943566-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T17:37:50,295632339-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T17:37:50,377662291-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:37:50,383944421-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T17:37:54,217855254-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:37:54,224290802-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T17:37:58,073581484-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:37:58,080197995-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T17:38:01,985030178-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:38:01,991279855-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T17:38:10,280913674-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:38:10,287467516-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T17:38:15,155198453-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:38:15,161561323-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T17:38:19,499954611-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:38:19,506549228-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T17:38:24,069108067-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:38:24,075364356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:38:28,230573448-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:38:28,237227354-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:38:32,980114987-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:38:32,986727305-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T17:38:37,283711915-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:38:37,290290731-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T17:38:41,110212628-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:38:41,116690084-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  180 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  180 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   66 KiB   0 B   0 B   0 B  100 GiB     0  1.10   89      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.92   90      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  0.98   80      up          osd.2  
                       TOTAL  300 GiB  181 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.92/1.10  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T17:38:45,022384913-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:39:08,957516196-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:39:18,066684340-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:39:27,438420435-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:39:36,531524839-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:39:45,488932380-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:39:45,506799825-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:39:54,578573258-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:39:54,596772739-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:40:03,848027918-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:40:03,865758800-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:40:12,993746164-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:40:13,012126959-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:40:22,099339811-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:40:22,117218536-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:40:22,131734762-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:40:22,140338056-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:40:22,160112779-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=744964
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T17:40:22,176179671-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-15T17:40:22,217969864-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:40:22,224442592-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:40:23.733+0000 ffffbe45a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:40:24.537+0000 ffffbe45a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:40:24.541+0000 ffffbe45a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:40:24.561418+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-16T00:40:24.561507+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T00:40:24.611391+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:40:24.611391+0000     0       0         0         0         0         0           -           0
2021-05-16T00:40:25.611595+0000     1     255      4810      4555   1138.69   1138.75   0.0454578   0.0536121
2021-05-16T00:40:26.611796+0000     2     255      9530      9275   1159.23      1180   0.0440111   0.0538506
2021-05-16T00:40:27.612017+0000     3     255     14055     13800   1149.82   1131.25   0.0461556   0.0546888
2021-05-16T00:40:28.612214+0000     4     255     18662     18407   1150.24   1151.75   0.0418948   0.0549897
2021-05-16T00:40:29.612407+0000     5     255     23231     22976    1148.6   1142.25   0.0427598   0.0551647
2021-05-16T00:40:30.612580+0000     6     255     27666     27411   1141.93   1108.75   0.0963535   0.0555465
2021-05-16T00:40:31.612754+0000     7     255     32332     32077   1145.41    1166.5   0.0861895   0.0554577
2021-05-16T00:40:32.612996+0000     8     255     37004     36749    1148.2      1168   0.0897195   0.0554038
2021-05-16T00:40:33.613406+0000     9     255     41554     41299   1146.96    1137.5   0.0823361   0.0555217
2021-05-16T00:40:34.613711+0000    10     255     46272     46017   1150.18    1179.5   0.0341601    0.055395
2021-05-16T00:40:35.614022+0000    11     255     50726     50471   1146.81    1113.5   0.0461084   0.0555194
2021-05-16T00:40:36.614198+0000    12     255     55450     55195   1149.64      1181   0.0921255   0.0554183
2021-05-16T00:40:37.614383+0000    13     255     60110     59855   1150.81      1165   0.0463587   0.0553842
2021-05-16T00:40:38.614729+0000    14     255     64466     64211   1146.36      1089   0.0801653   0.0556386
2021-05-16T00:40:39.615107+0000    15     255     69176     68921   1148.41    1177.5   0.0479861   0.0555333
2021-05-16T00:40:40.615308+0000    16     255     73427     73172   1143.04   1062.75    0.134811   0.0557842
2021-05-16T00:40:41.615629+0000    17     255     77221     76966   1131.58     948.5    0.157676   0.0563894
2021-05-16T00:40:42.616099+0000    18     255     81795     81540   1132.21    1143.5   0.0437553   0.0563591
2021-05-16T00:40:43.616292+0000    19     255     86155     85900   1129.98      1090   0.0849613   0.0564902
2021-05-16T00:40:44.616466+0000 min lat: 0.00274881 max lat: 0.241816 avg lat: 0.0563813
2021-05-16T00:40:44.616466+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:40:44.616466+0000    20      87     90768     90681   1133.23   1195.25  0.00435865   0.0563813
2021-05-16T00:40:45.616715+0000 Total time run:         20.0195
Total writes made:      90768
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     1133.49
Stddev Bandwidth:       56.3023
Max bandwidth (MB/sec): 1195.25
Min bandwidth (MB/sec): 948.5
Average IOPS:           4533
Stddev IOPS:            225.209
Max IOPS:               4781
Min IOPS:               3794
Average Latency(s):     0.0563702
Stddev Latency(s):      0.0222499
Max latency(s):         0.241816
Min latency(s):         0.00274881

[1;32mlocalhost.localdomain	[2021-05-15T17:40:46,258191058-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 744964


[1;33mlocalhost.localdomain	[2021-05-15T17:40:46,267482357-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:41:10,393411800-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 90.77k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:41:10,412241766-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:41:19,458664000-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 90.77k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:41:19,477198050-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:41:28,879293405-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 90.77k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:41:28,898402054-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:41:38,095005573-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 90.77k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:41:38,113653620-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:41:47,204543024-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 90.77k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:41:47,223092905-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:41:47,238469178-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:41:47,247183986-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:41:47,267178671-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=748489
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T17:41:47,283692823-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-15T17:41:47,324271980-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:41:47,330590408-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb127f2b-895d-4d09-9e28-5c5db46ad320', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb127f2b-895d-4d09-9e28-5c5db46ad320 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfb93F:/tmp/ceph-asok.dfb93F -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:41:48.792+0000 ffff87974010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:41:49.592+0000 ffff87974010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:41:49.592+0000 ffff87974010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:41:49.616148+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:41:49.616148+0000     0       0         0         0         0         0           -           0
2021-05-16T00:41:50.616342+0000     1     255      6718      6463   1615.21   1615.75   0.0374221   0.0386967
2021-05-16T00:41:51.616662+0000     2     255     12788     12533   1566.11    1517.5   0.0416496   0.0402896
2021-05-16T00:41:52.616984+0000     3     255     18713     18458   1537.67   1481.25   0.0409503   0.0412096
2021-05-16T00:41:53.617223+0000     4     256     24765     24509   1531.35   1512.75   0.0431171   0.0414368
2021-05-16T00:41:54.617578+0000     5     255     31536     31281   1563.56      1693   0.0378299   0.0406511
2021-05-16T00:41:55.617781+0000     6     255     38317     38062   1585.45   1695.25   0.0427933   0.0400986
2021-05-16T00:41:56.618059+0000     7     255     44447     44192   1577.82    1532.5   0.0415451   0.0403231
2021-05-16T00:41:57.618258+0000     8     255     50844     50589   1580.46   1599.25   0.0419999   0.0402693
2021-05-16T00:41:58.618535+0000     9     255     56909     56654   1573.28   1516.25   0.0419585   0.0404661
2021-05-16T00:41:59.618868+0000    10     255     63086     62831   1570.33   1544.25   0.0418058   0.0405537
2021-05-16T00:42:00.619188+0000    11     255     69122     68867   1564.71      1509   0.0414026   0.0407079
2021-05-16T00:42:01.619405+0000    12     255     75456     75201   1566.24    1583.5   0.0371767   0.0406822
2021-05-16T00:42:02.621291+0000    13     255     81871     81616    1568.9   1603.75   0.0408483   0.0406136
2021-05-16T00:42:03.621571+0000    14     255     88276     88021   1571.18   1601.25   0.0369745   0.0405648
2021-05-16T00:42:04.621804+0000 Total time run:       14.4048
Total reads made:     90768
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1575.3
Average IOPS:         6301
Stddev IOPS:          268.574
Max IOPS:             6781
Min IOPS:             5925
Average Latency(s):   0.0404505
Max latency(s):       0.0810036
Min latency(s):       0.0032323

[1;32mlocalhost.localdomain	[2021-05-15T17:42:05,290686609-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 748489


[1;33mlocalhost.localdomain	[2021-05-15T17:42:05,300058862-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:42:29,483376787-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 90.77k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:42:29,501735774-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:42:38,576523842-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 90.77k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:42:38,594809463-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:42:47,699743229-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 90.77k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:42:47,717839528-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:42:56,983015428-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 90.77k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:42:57,001548002-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:43:06,313087418-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 90.77k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:43:06,331988034-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:43:06,347163683-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T17:43:06,352880555-07:00][RUNNING][ROUND 3/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:43:06,361328812-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T17:43:06,379421655-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40152\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.71972\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 209d9b81-1bb6-4531-836d-9ee140c4409e\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 209d9b81-1bb6-4531-836d-9ee140c4409e\nlast_changed 2021-05-15T17:43:34.972899-0700\ncreated 2021-05-15T17:43:34.972899-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40152/0,v1:10.10.1.2:40153/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.71972 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 7589851a-5424-4bd8-9af0-1c4be7d8f9c4\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 08f30c00-9708-426c-aa82-a6590c1c5a4b\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 cbab1e5c-d06b-4438-92be-eda0de24411e\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42152\n  w/ user/pass: admin / fdd1a760-0ae8-464e-b55a-a63119a6075c\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:43:54 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40152
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.71972
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 209d9b81-1bb6-4531-836d-9ee140c4409e
setting min_mon_release = octopus
epoch 0
fsid 209d9b81-1bb6-4531-836d-9ee140c4409e
last_changed 2021-05-15T17:43:34.972899-0700
created 2021-05-15T17:43:34.972899-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40152/0,v1:10.10.1.2:40153/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.71972 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 7589851a-5424-4bd8-9af0-1c4be7d8f9c4
0
start osd.0
add osd1 08f30c00-9708-426c-aa82-a6590c1c5a4b
1
start osd.1
add osd2 cbab1e5c-d06b-4438-92be-eda0de24411e
2
start osd.2


restful urls: https://10.10.1.2:42152
  w/ user/pass: admin / fdd1a760-0ae8-464e-b55a-a63119a6075c


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T17:43:08.278-0700 7fec6934e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:43:08.278-0700 7fec6934e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:43:08.298-0700 7f33dbe2f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:43:08.298-0700 7f33dbe2f1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40152,v1:10.10.1.2:40153] --print /tmp/ceph_monmap.71972 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.71972 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.71972 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42152 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x562837ad8000 @  0x7efd6ed62680 0x7efd6ed83824 0x7efd6f51e187 0x7efd6f526355 0x7efd6f51e708 0x7efd6f51e877 0x7efd6f51fc24 0x7efd6f537ec1 0x7efd6f4aa5f3 0x7efd6f50be97 0x7efd6f513b1a 0x7efd6ec16d84 0x7efd6ed32609 0x7efd6e906293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.aTCDC6yRhu 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7589851a-5424-4bd8-9af0-1c4be7d8f9c4 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDBaqBgY3clHhAAfvW7yNAQoaAFjBiX2wVgGQ== --osd-uuid 7589851a-5424-4bd8-9af0-1c4be7d8f9c4 
2021-05-15T17:43:46.122-0700 7f07de6b1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:43:46.142-0700 7f07de6b1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:43:46.142-0700 7f07de6b1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5596fd91a000 @  0x7f07df07a680 0x7f07df09b824 0x5596f34b2447 0x5596f34ba4b5 0x5596f34b29c8 0x5596f34b2b37 0x5596f34b3ee4 0x5596f3284ca1 0x5596f345c423 0x5596f32752a7 0x5596f327a53a 0x7f07debcdd84 0x7f07ded52609 0x7f07de8bb293
2021-05-15T17:43:46.450-0700 7f07de6b1f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 08f30c00-9708-426c-aa82-a6590c1c5a4b -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x561ccb230000 @  0x7f59f18ae680 0x7f59f18cf824 0x561cc0391447 0x561cc03994b5 0x561cc03919c8 0x561cc0391b37 0x561cc0392ee4 0x561cc0163ca1 0x561cc033b423 0x561cc01542a7 0x561cc015953a 0x7f59f1401d84 0x7f59f1586609 0x7f59f10ef293
2021-05-15T17:43:47.106-0700 7f59f0ee5f00 -1 Falling back to public interface
2021-05-15T17:43:47.362-0700 7f59f0ee5f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDCaqBgbapuMBAA2/0M63AXU6Ig7KGU6Jlnow== --osd-uuid 08f30c00-9708-426c-aa82-a6590c1c5a4b 
2021-05-15T17:43:47.462-0700 7f1f6dfccf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:43:47.482-0700 7f1f6dfccf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:43:47.482-0700 7f1f6dfccf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x563720a4e000 @  0x7f1f6e995680 0x7f1f6e9b6824 0x563715998447 0x5637159a04b5 0x5637159989c8 0x563715998b37 0x563715999ee4 0x56371576aca1 0x563715942423 0x56371575b2a7 0x56371576053a 0x7f1f6e4e8d84 0x7f1f6e66d609 0x7f1f6e1d6293
2021-05-15T17:43:47.794-0700 7f1f6dfccf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new cbab1e5c-d06b-4438-92be-eda0de24411e -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5619abbe0000 @  0x7fcdef483680 0x7fcdef4a4824 0x5619a0b37447 0x5619a0b3f4b5 0x5619a0b379c8 0x5619a0b37b37 0x5619a0b38ee4 0x5619a0909ca1 0x5619a0ae1423 0x5619a08fa2a7 0x5619a08ff53a 0x7fcdeefd6d84 0x7fcdef15b609 0x7fcdeecc4293
2021-05-15T17:43:48.458-0700 7fcdeeabaf00 -1 Falling back to public interface
2021-05-15T17:43:48.714-0700 7fcdeeabaf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDEaqBgUG6rCRAAbqy/oZftSUVYo+9pVHs11Q== --osd-uuid cbab1e5c-d06b-4438-92be-eda0de24411e 
2021-05-15T17:43:48.802-0700 7f26b93a9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:43:48.822-0700 7f26b93a9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:43:48.822-0700 7f26b93a9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5638df604000 @  0x7f26b9d72680 0x7f26b9d93824 0x5638d3e95447 0x5638d3e9d4b5 0x5638d3e959c8 0x5638d3e95b37 0x5638d3e96ee4 0x5638d3c67ca1 0x5638d3e3f423 0x5638d3c582a7 0x5638d3c5d53a 0x7f26b98c5d84 0x7f26b9a4a609 0x7f26b95b3293
2021-05-15T17:43:49.210-0700 7f26b93a9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55f960df6000 @  0x7fcb07106680 0x7fcb07127824 0x55f95625c447 0x55f9562644b5 0x55f95625c9c8 0x55f95625cb37 0x55f95625dee4 0x55f95602eca1 0x55f956206423 0x55f95601f2a7 0x55f95602453a 0x7fcb06c59d84 0x7fcb06dde609 0x7fcb06947293
2021-05-15T17:43:49.866-0700 7fcb0673df00 -1 Falling back to public interface
2021-05-15T17:43:50.138-0700 7fcb0673df00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T17:43:54,161651037-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T17:43:54,183392226-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T17:43:54,266259861-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:43:54,272725466-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T17:43:58,142514901-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:43:58,148949509-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T17:44:01,989908526-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:44:01,996174909-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T17:44:05,970447044-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:44:05,976862148-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T17:44:14,071182061-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:44:14,078368436-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T17:44:18,985882735-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:44:18,992395731-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T17:44:23,430545301-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:44:23,436660471-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T17:44:27,989877449-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:44:27,996039126-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:44:32,742974156-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:44:32,749441842-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:44:37,720871798-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:44:37,727676623-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T17:44:41,705666229-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:44:41,712134094-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T17:44:45,495706153-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:44:45,502189695-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   66 KiB   0 B   0 B   0 B  100 GiB     0  1.12   90      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.94   89      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.94   80      up          osd.2  
                       TOTAL  300 GiB  177 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.94/1.12  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T17:44:49,213040749-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:45:13,372206289-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:45:22,417623356-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:45:31,607249419-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:45:40,857883130-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:45:49,837274385-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:45:49,855527726-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:45:58,939690225-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:45:58,958370120-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:46:08,132577490-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:46:08,150902944-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:46:17,340321895-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:46:17,358800435-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:46:26,395096674-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:46:26,413319132-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:46:26,428452279-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:46:26,437213811-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:46:26,457410614-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=761671
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T17:46:26,474158539-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-15T17:46:26,516285294-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:46:26,522796778-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:46:28.002+0000 ffffbbb44010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:46:28.806+0000 ffffbbb44010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:46:28.806+0000 ffffbbb44010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:46:28.827833+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-16T00:46:28.827899+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T00:46:28.877706+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:46:28.877706+0000     0       0         0         0         0         0           -           0
2021-05-16T00:46:29.877910+0000     1     255      5080      4825   1206.17   1206.25   0.0799681   0.0506754
2021-05-16T00:46:30.878134+0000     2     255     10089      9834   1229.07   1252.25   0.0425995   0.0508853
2021-05-16T00:46:31.878339+0000     3     255     14946     14691   1224.05   1214.25   0.0404856   0.0514635
2021-05-16T00:46:32.878512+0000     4     255     20026     19771   1235.48      1270   0.0837666   0.0511909
2021-05-16T00:46:33.878698+0000     5     255     25151     24896   1244.59   1281.25   0.0422338   0.0509921
2021-05-16T00:46:34.878949+0000     6     255     30110     29855   1243.73   1239.75   0.0818199   0.0510905
2021-05-16T00:46:35.879184+0000     7     255     34945     34690   1238.69   1208.75   0.0496699   0.0512842
2021-05-16T00:46:36.879361+0000     8     255     39306     39051   1220.11   1090.25   0.0465498   0.0520568
2021-05-16T00:46:37.879589+0000     9     255     44172     43917   1219.68    1216.5   0.0440065   0.0522104
2021-05-16T00:46:38.880386+0000    10     255     48965     48710   1217.44   1198.25   0.0546991   0.0523132
2021-05-16T00:46:39.880586+0000    11     255     53527     53272   1210.43    1140.5   0.0382377   0.0526519
2021-05-16T00:46:40.880774+0000    12     255     58232     57977   1207.56   1176.25   0.0434596   0.0527786
2021-05-16T00:46:41.881043+0000    13     255     63311     63056   1212.32   1269.75   0.0950278   0.0526148
2021-05-16T00:46:42.881402+0000    14     255     67796     67541   1205.78   1121.25   0.0434793   0.0528973
2021-05-16T00:46:43.881611+0000    15     255     72421     72166   1202.46   1156.25    0.155872   0.0530408
2021-05-16T00:46:44.881818+0000    16     255     77370     77115   1204.62   1237.25   0.0418189   0.0529618
2021-05-16T00:46:45.882029+0000    17     255     82411     82156   1207.88   1260.25   0.0442943   0.0528384
2021-05-16T00:46:46.882240+0000    18     255     87316     87061   1208.89   1226.25   0.0408096   0.0528042
2021-05-16T00:46:47.882433+0000    19     255     92331     92076   1211.23   1253.75   0.0760277    0.052707
2021-05-16T00:46:48.882623+0000 min lat: 0.00233165 max lat: 0.20153 avg lat: 0.0525982
2021-05-16T00:46:48.882623+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:46:48.882623+0000    20     126     97315     97189   1214.57   1278.25   0.0027513   0.0525982
2021-05-16T00:46:49.882921+0000 Total time run:         20.0268
Total writes made:      97315
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     1214.81
Stddev Bandwidth:       54.2104
Max bandwidth (MB/sec): 1281.25
Min bandwidth (MB/sec): 1090.25
Average IOPS:           4859
Stddev IOPS:            216.842
Max IOPS:               5125
Min IOPS:               4361
Average Latency(s):     0.0525838
Stddev Latency(s):      0.0198776
Max latency(s):         0.20153
Min latency(s):         0.00233165

[1;32mlocalhost.localdomain	[2021-05-15T17:46:50,540961012-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 761671


[1;33mlocalhost.localdomain	[2021-05-15T17:46:50,550239952-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:47:14,568157089-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.32k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:47:14,587449238-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:47:23,763886444-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.32k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:47:23,782830200-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:47:33,076498522-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.32k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:47:33,095438856-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:47:42,242832625-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.32k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:47:42,261847880-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:47:51,337815433-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.32k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:47:51,356749073-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:47:51,372578807-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:47:51,381519013-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:47:51,401728144-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=765165
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T17:47:51,418214138-07:00] INFO: > Run rados bench[0m
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.3
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
[1;30mlocalhost.localdomain	[2021-05-15T17:47:51,458816513-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:47:51,465186396-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb572f32-b08d-4664-9b96-93e8c6dd49f7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb572f32-b08d-4664-9b96-93e8c6dd49f7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0bwO5B:/tmp/ceph-asok.0bwO5B -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:47:52.924+0000 ffffaa9bb010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:47:53.745+0000 ffffaa9bb010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:47:53.745+0000 ffffaa9bb010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:47:53.766989+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:47:53.766989+0000     0      14        14         0         0         0           -           0
2021-05-16T00:47:54.767244+0000     1     255      6041      5786   1445.25    1446.5   0.0407843   0.0431877
2021-05-16T00:47:55.767452+0000     2     255     12340     12085   1509.81   1574.75   0.0392044   0.0418134
2021-05-16T00:47:56.767803+0000     3     255     18636     18381   1531.02      1574   0.0414226   0.0413661
2021-05-16T00:47:57.768143+0000     4     255     24779     24524   1532.07   1535.75   0.0379829   0.0414388
2021-05-16T00:47:58.768431+0000     5     255     31438     31183   1558.51   1664.75   0.0400687   0.0407662
2021-05-16T00:47:59.769025+0000     6     255     37806     37551   1563.93      1592   0.0432132   0.0406405
2021-05-16T00:48:00.769321+0000     7     255     44071     43816    1564.2   1566.25   0.0377139   0.0406825
2021-05-16T00:48:01.769676+0000     8     255     50627     50372   1573.48      1639   0.0375958   0.0404585
2021-05-16T00:48:02.769878+0000     9     255     56698     56443   1567.25   1517.75   0.0427055   0.0406192
2021-05-16T00:48:03.770079+0000    10     255     62907     62652   1565.72   1552.25   0.0400377   0.0406741
2021-05-16T00:48:04.770265+0000    11     255     69616     69361   1575.83   1677.25   0.0378505    0.040426
2021-05-16T00:48:05.770491+0000    12     256     76295     76039    1583.6    1669.5   0.0376326   0.0402349
2021-05-16T00:48:06.770775+0000    13     255     82413     82158   1579.43   1529.75   0.0372736   0.0403476
2021-05-16T00:48:07.771135+0000    14     255     88574     88319   1576.59   1540.25   0.0404471   0.0404195
2021-05-16T00:48:08.771378+0000    15     255     95057     94802   1579.51   1620.75   0.0300283   0.0403604
2021-05-16T00:48:09.771617+0000 Total time run:       15.4121
Total reads made:     97315
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1578.55
Average IOPS:         6314
Stddev IOPS:          259.051
Max IOPS:             6709
Min IOPS:             5786
Average Latency(s):   0.040408
Max latency(s):       0.121024
Min latency(s):       0.000651428

[1;32mlocalhost.localdomain	[2021-05-15T17:48:10,402698806-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 765165


[1;33mlocalhost.localdomain	[2021-05-15T17:48:10,412886492-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:48:34,418006100-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.32k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:48:34,436885362-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:48:43,712500247-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.32k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:48:43,731315267-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:48:52,817561835-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.32k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:48:52,836358325-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:49:01,924645275-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.32k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:49:01,943817999-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:49:11,142450352-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.32k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:49:11,164174611-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:49:11,182092081-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T17:49:11,189434908-07:00][RUNNING][ROUND 4/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:49:11,199596874-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T17:49:11,218468898-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40621\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.73063\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6a095447-02b9-49e6-88b9-776354ffa4d3\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 6a095447-02b9-49e6-88b9-776354ffa4d3\nlast_changed 2021-05-15T17:49:39.922238-0700\ncreated 2021-05-15T17:49:39.922238-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40621/0,v1:10.10.1.2:40622/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.73063 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 7a611afd-f978-4859-865a-386382a7335c\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 9db57639-6273-4900-a90b-ce5698650dad\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 a69f976f-c768-4d9b-93d5-65d99f32c7e7\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42621\n  w/ user/pass: admin / 9b013e6a-920f-4a66-b090-30eb5025e9aa\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:49:58 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40621
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.73063
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6a095447-02b9-49e6-88b9-776354ffa4d3
setting min_mon_release = octopus
epoch 0
fsid 6a095447-02b9-49e6-88b9-776354ffa4d3
last_changed 2021-05-15T17:49:39.922238-0700
created 2021-05-15T17:49:39.922238-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40621/0,v1:10.10.1.2:40622/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.73063 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 7a611afd-f978-4859-865a-386382a7335c
0
start osd.0
add osd1 9db57639-6273-4900-a90b-ce5698650dad
1
start osd.1
add osd2 a69f976f-c768-4d9b-93d5-65d99f32c7e7
2
start osd.2


restful urls: https://10.10.1.2:42621
  w/ user/pass: admin / 9b013e6a-920f-4a66-b090-30eb5025e9aa


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T17:49:13.145-0700 7fed0090e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:49:13.145-0700 7fed0090e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:49:13.161-0700 7fcec148a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:49:13.161-0700 7fcec148a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40621,v1:10.10.1.2:40622] --print /tmp/ceph_monmap.73063 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.73063 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.73063 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42621 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x555d9c6cc000 @  0x7fbeb4c33680 0x7fbeb4c54824 0x7fbeb53ef187 0x7fbeb53f7355 0x7fbeb53ef708 0x7fbeb53ef877 0x7fbeb53f0c24 0x7fbeb5408ec1 0x7fbeb537b5f3 0x7fbeb53dce97 0x7fbeb53e4b1a 0x7fbeb4ae7d84 0x7fbeb4c03609 0x7fbeb47d7293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.pDFfpKvltw 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7a611afd-f978-4859-865a-386382a7335c -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAubKBge52hDBAA2W6DtMFbHRnhOIUCpouARg== --osd-uuid 7a611afd-f978-4859-865a-386382a7335c 
2021-05-15T17:49:50.854-0700 7f067096af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:49:50.874-0700 7f067096af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:49:50.874-0700 7f067096af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55bc4b88e000 @  0x7f0671333680 0x7f0671354824 0x55bc41434447 0x55bc4143c4b5 0x55bc414349c8 0x55bc41434b37 0x55bc41435ee4 0x55bc41206ca1 0x55bc413de423 0x55bc411f72a7 0x55bc411fc53a 0x7f0670e86d84 0x7f067100b609 0x7f0670b74293
2021-05-15T17:49:51.238-0700 7f067096af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9db57639-6273-4900-a90b-ce5698650dad -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5566d8432000 @  0x7f6aefd46680 0x7f6aefd67824 0x5566cca6d447 0x5566cca754b5 0x5566cca6d9c8 0x5566cca6db37 0x5566cca6eee4 0x5566cc83fca1 0x5566cca17423 0x5566cc8302a7 0x5566cc83553a 0x7f6aef899d84 0x7f6aefa1e609 0x7f6aef587293
2021-05-15T17:49:51.854-0700 7f6aef37df00 -1 Falling back to public interface
2021-05-15T17:49:52.114-0700 7f6aef37df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAvbKBgNLvXIBAAPx9FYNdT7KTN/jrUBC+FkA== --osd-uuid 9db57639-6273-4900-a90b-ce5698650dad 
2021-05-15T17:49:52.206-0700 7f685d7b2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:49:52.230-0700 7f685d7b2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:49:52.230-0700 7f685d7b2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5590fcef4000 @  0x7f685e17b680 0x7f685e19c824 0x5590f11aa447 0x5590f11b24b5 0x5590f11aa9c8 0x5590f11aab37 0x5590f11abee4 0x5590f0f7cca1 0x5590f1154423 0x5590f0f6d2a7 0x5590f0f7253a 0x7f685dcced84 0x7f685de53609 0x7f685d9bc293
2021-05-15T17:49:52.534-0700 7f685d7b2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a69f976f-c768-4d9b-93d5-65d99f32c7e7 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55a207fe8000 @  0x7fc2e7650680 0x7fc2e7671824 0x55a1fca4e447 0x55a1fca564b5 0x55a1fca4e9c8 0x55a1fca4eb37 0x55a1fca4fee4 0x55a1fc820ca1 0x55a1fc9f8423 0x55a1fc8112a7 0x55a1fc81653a 0x7fc2e71a3d84 0x7fc2e7328609 0x7fc2e6e91293
2021-05-15T17:49:53.174-0700 7fc2e6c87f00 -1 Falling back to public interface
2021-05-15T17:49:53.430-0700 7fc2e6c87f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAwbKBguBwHMhAAKPRlns46EA9eodatu0O2sw== --osd-uuid a69f976f-c768-4d9b-93d5-65d99f32c7e7 
2021-05-15T17:49:53.522-0700 7f4ea0190f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:49:53.542-0700 7f4ea0190f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:49:53.542-0700 7f4ea0190f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b27b50c000 @  0x7f4ea0b59680 0x7f4ea0b7a824 0x55b2709d6447 0x55b2709de4b5 0x55b2709d69c8 0x55b2709d6b37 0x55b2709d7ee4 0x55b2707a8ca1 0x55b270980423 0x55b2707992a7 0x55b27079e53a 0x7f4ea06acd84 0x7f4ea0831609 0x7f4ea039a293
2021-05-15T17:49:53.854-0700 7f4ea0190f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x558e7670c000 @  0x7f51f889d680 0x7f51f88be824 0x558e6b8bc447 0x558e6b8c44b5 0x558e6b8bc9c8 0x558e6b8bcb37 0x558e6b8bdee4 0x558e6b68eca1 0x558e6b866423 0x558e6b67f2a7 0x558e6b68453a 0x7f51f83f0d84 0x7f51f8575609 0x7f51f80de293
2021-05-15T17:49:54.446-0700 7f51f7ed4f00 -1 Falling back to public interface
2021-05-15T17:49:54.706-0700 7f51f7ed4f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T17:49:58,854851439-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T17:49:58,877042465-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T17:49:58,958330567-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:49:58,964441919-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T17:50:02,866380331-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:50:02,872785193-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T17:50:06,600746971-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:50:06,607289767-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T17:50:10,458436582-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:50:10,464961779-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T17:50:18,717650455-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:50:18,725596937-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T17:50:22,799426523-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:50:22,805918024-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T17:50:27,292358280-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:50:27,298727390-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T17:50:31,512821869-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:50:31,519133233-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:50:36,093822768-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:50:36,100129511-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:50:40,655961403-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:50:40,662304107-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T17:50:44,704159011-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:50:44,710372753-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T17:50:48,623393325-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:50:48,629682575-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.08   91      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   88      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   80      up          osd.2  
                       TOTAL  300 GiB  173 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.08  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T17:50:52,428658949-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:51:16,511371341-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:51:25,928957165-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:51:34,996051455-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:51:44,104334335-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:51:53,706244314-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:51:53,725123667-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:52:02,673400263-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:52:02,692215900-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:52:11,786572842-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:52:11,805083100-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:52:21,019529470-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:52:21,038125362-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:52:30,111687492-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:52:30,130636003-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:52:30,146269395-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:52:30,155802993-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:52:30,178603716-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=778160
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T17:52:30,201229509-07:00] INFO: > Run rados bench[0m
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.4
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mlocalhost.localdomain	[2021-05-15T17:52:30,247208450-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:52:30,254053061-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:52:31.805+0000 ffffae1a0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:52:32.601+0000 ffffae1a0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:52:32.601+0000 ffffae1a0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:52:32.618977+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-16T00:52:32.619038+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T00:52:32.668600+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:52:32.668600+0000     0       0         0         0         0         0           -           0
2021-05-16T00:52:33.668815+0000     1     255      5349      5094   1273.39    1273.5   0.0421322   0.0481921
2021-05-16T00:52:34.669066+0000     2     255     10637     10382   1297.53      1322   0.0467865   0.0482253
2021-05-16T00:52:35.669269+0000     3     255     15883     15628    1302.1    1311.5   0.0415497   0.0484369
2021-05-16T00:52:36.669472+0000     4     255     21019     20764   1297.51      1284    0.047699   0.0487473
2021-05-16T00:52:37.669639+0000     5     255     26148     25893   1294.41   1282.25   0.0504579   0.0490015
2021-05-16T00:52:38.669894+0000     6     255     31369     31114   1296.16   1305.25   0.0431143   0.0490273
2021-05-16T00:52:39.670342+0000     7     255     36514     36259   1294.67   1286.25   0.0473447   0.0491245
2021-05-16T00:52:40.670549+0000     8     255     41726     41471   1295.67      1303    0.044435   0.0491346
2021-05-16T00:52:41.670971+0000     9     255     46089     45834   1272.85   1090.75   0.0512242   0.0500195
2021-05-16T00:52:42.671225+0000    10     255     51143     50888   1271.88    1263.5   0.0452319   0.0500994
2021-05-16T00:52:43.671541+0000    11     255     56439     56184   1276.58      1324   0.0460884   0.0499425
2021-05-16T00:52:44.671743+0000    12     255     61167     60912   1268.68      1182   0.0454954    0.050251
2021-05-16T00:52:45.671946+0000    13     255     66010     65755   1264.21   1210.75   0.0511598   0.0504471
2021-05-16T00:52:46.672159+0000    14     255     70986     70731   1262.74      1244   0.0489478    0.050523
2021-05-16T00:52:47.672426+0000    15     255     75233     74978   1249.33   1061.75   0.0404993   0.0510789
2021-05-16T00:52:48.672796+0000    16     255     79821     79566    1242.9      1147   0.0399959   0.0513369
2021-05-16T00:52:49.672992+0000    17     255     84291     84036   1235.51    1117.5    0.049511   0.0516676
2021-05-16T00:52:50.673309+0000    18     255     89004     88749   1232.31   1178.25   0.0376477    0.051808
2021-05-16T00:52:51.673658+0000    19     255     93340     93085   1224.48      1084    0.101192   0.0521179
2021-05-16T00:52:52.673854+0000 min lat: 0.00369147 max lat: 0.228955 avg lat: 0.0520119
2021-05-16T00:52:52.673854+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:52:52.673854+0000    20      80     98415     98335   1228.87    1312.5   0.0181034   0.0520119
2021-05-16T00:52:53.674107+0000 Total time run:         20.0163
Total writes made:      98415
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     1229.18
Stddev Bandwidth:       88.1566
Max bandwidth (MB/sec): 1324
Min bandwidth (MB/sec): 1061.75
Average IOPS:           4916
Stddev IOPS:            352.626
Max IOPS:               5296
Min IOPS:               4247
Average Latency(s):     0.0519885
Stddev Latency(s):      0.0178708
Max latency(s):         0.228955
Min latency(s):         0.00369147

[1;32mlocalhost.localdomain	[2021-05-15T17:52:54,304590440-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 778160


[1;33mlocalhost.localdomain	[2021-05-15T17:52:54,316613670-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:53:18,410337888-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.42k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:53:18,429621942-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:53:27,735298142-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.42k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:53:27,754426603-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:53:36,826275974-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.42k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:53:36,845055640-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:53:45,930410955-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.42k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:53:45,949524171-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:53:55,415361591-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.42k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:53:55,434178206-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:53:55,449399785-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:53:55,458602692-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:53:55,479169732-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=781771
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T17:53:55,495913652-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-15T17:53:55,536789927-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:53:55,543205732-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2081e8b7-dad1-4c8c-a38f-dda524474f17', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2081e8b7-dad1-4c8c-a38f-dda524474f17 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.dfY16N:/tmp/ceph-asok.dfY16N -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:53:57.039+0000 ffff846ac010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:53:57.827+0000 ffff846ac010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:53:57.827+0000 ffff846ac010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:53:57.853091+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:53:57.853091+0000     0       0         0         0         0         0           -           0
2021-05-16T00:53:58.853290+0000     1     256      6005      5749   1436.75   1437.25   0.0417619    0.043386
2021-05-16T00:53:59.853553+0000     2     255     12621     12366   1545.28   1654.25   0.0382729   0.0408596
2021-05-16T00:54:00.853801+0000     3     255     19376     19121   1592.96   1688.75   0.0388651   0.0397715
2021-05-16T00:54:01.854154+0000     4     255     26026     25771    1610.2    1662.5   0.0448089   0.0393947
2021-05-16T00:54:02.854363+0000     5     255     32249     31994   1599.25   1555.75   0.0376123   0.0397422
2021-05-16T00:54:03.854623+0000     6     255     38553     38298    1595.3      1576   0.0405978   0.0398592
2021-05-16T00:54:04.854935+0000     7     255     45086     44831   1600.65   1633.25   0.0375618   0.0397552
2021-05-16T00:54:05.855219+0000     8     255     51106     50851   1588.64      1505   0.0383474   0.0400701
2021-05-16T00:54:06.855471+0000     9     255     57426     57171   1587.64      1580   0.0377578    0.040108
2021-05-16T00:54:07.855737+0000    10     255     64174     63919   1597.53      1687   0.0369077   0.0398708
2021-05-16T00:54:08.855968+0000    11     255     70568     70313   1597.58    1598.5   0.0433357    0.039865
2021-05-16T00:54:09.856213+0000    12     255     76800     76545   1594.25      1558     0.03721   0.0399666
2021-05-16T00:54:10.856497+0000    13     255     82238     81983   1576.17    1359.5   0.0465476   0.0404159
2021-05-16T00:54:11.856765+0000    14     255     88169     87914   1569.46   1482.75   0.0418478   0.0406015
2021-05-16T00:54:12.857007+0000    15     255     94113     93858   1563.88      1486   0.0436735    0.040749
2021-05-16T00:54:13.857270+0000 Total time run:       15.8202
Total reads made:     98415
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1555.21
Average IOPS:         6220
Stddev IOPS:          384.083
Max IOPS:             6755
Min IOPS:             5438
Average Latency(s):   0.0410144
Max latency(s):       0.107698
Min latency(s):       0.00388375

[1;32mlocalhost.localdomain	[2021-05-15T17:54:14,503810138-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 781771


[1;33mlocalhost.localdomain	[2021-05-15T17:54:14,513408137-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:54:38,585717899-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.42k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:54:38,604958468-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:54:47,524625943-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.42k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:54:47,543812322-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:54:56,801521962-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.42k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:54:56,820963909-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:55:06,067978370-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.42k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:55:06,087236549-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:55:15,181178937-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.42k objects, 24 GiB
    usage:   48 GiB used, 252 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:55:15,200475252-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:55:15,216067029-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T17:55:15,222029259-07:00][RUNNING][ROUND 5/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:55:15,230836605-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T17:55:15,248499246-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40299\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.74162\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid dbcd9908-acd1-465d-a3b2-efb3bcb5ef6e\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid dbcd9908-acd1-465d-a3b2-efb3bcb5ef6e\nlast_changed 2021-05-15T17:55:43.688365-0700\ncreated 2021-05-15T17:55:43.688365-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40299/0,v1:10.10.1.2:40300/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.74162 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 53357899-286e-4216-8d3f-d577c3c87994\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 b74286ee-6b48-44e6-aa83-5b854f497679\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 beacb3ab-7d84-4d05-ac8a-1c36af9f1492\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42299\n  w/ user/pass: admin / ab48a1ee-06f0-4529-95af-2a32e40fe423\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:56:02 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40299
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.74162
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid dbcd9908-acd1-465d-a3b2-efb3bcb5ef6e
setting min_mon_release = octopus
epoch 0
fsid dbcd9908-acd1-465d-a3b2-efb3bcb5ef6e
last_changed 2021-05-15T17:55:43.688365-0700
created 2021-05-15T17:55:43.688365-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40299/0,v1:10.10.1.2:40300/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.74162 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 53357899-286e-4216-8d3f-d577c3c87994
0
start osd.0
add osd1 b74286ee-6b48-44e6-aa83-5b854f497679
1
start osd.1
add osd2 beacb3ab-7d84-4d05-ac8a-1c36af9f1492
2
start osd.2


restful urls: https://10.10.1.2:42299
  w/ user/pass: admin / ab48a1ee-06f0-4529-95af-2a32e40fe423


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T17:55:17.169-0700 7f5466a281c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:55:17.169-0700 7f5466a281c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:55:17.185-0700 7f82aa3691c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T17:55:17.185-0700 7f82aa3691c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40299,v1:10.10.1.2:40300] --print /tmp/ceph_monmap.74162 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.74162 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.74162 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42299 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x559aada22000 @  0x7f370de6a680 0x7f370de8b824 0x7f370e626187 0x7f370e62e355 0x7f370e626708 0x7f370e626877 0x7f370e627c24 0x7f370e63fec1 0x7f370e5b25f3 0x7f370e613e97 0x7f370e61bb1a 0x7f370dd1ed84 0x7f370de3a609 0x7f370da0e293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.zqClr9XktX 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 53357899-286e-4216-8d3f-d577c3c87994 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCabaBgahmZAhAAPcWBAHoyqj8+No9sxrDz9A== --osd-uuid 53357899-286e-4216-8d3f-d577c3c87994 
2021-05-15T17:55:54.681-0700 7fb3b0a49f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:55:54.701-0700 7fb3b0a49f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T17:55:54.701-0700 7fb3b0a49f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55c97f554000 @  0x7fb3b1412680 0x7fb3b1433824 0x55c973fa9447 0x55c973fb14b5 0x55c973fa99c8 0x55c973fa9b37 0x55c973faaee4 0x55c973d7bca1 0x55c973f53423 0x55c973d6c2a7 0x55c973d7153a 0x7fb3b0f65d84 0x7fb3b10ea609 0x7fb3b0c53293
2021-05-15T17:55:55.013-0700 7fb3b0a49f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b74286ee-6b48-44e6-aa83-5b854f497679 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5595b8118000 @  0x7f32e4f82680 0x7f32e4fa3824 0x5595ac4e8447 0x5595ac4f04b5 0x5595ac4e89c8 0x5595ac4e8b37 0x5595ac4e9ee4 0x5595ac2baca1 0x5595ac492423 0x5595ac2ab2a7 0x5595ac2b053a 0x7f32e4ad5d84 0x7f32e4c5a609 0x7f32e47c3293
2021-05-15T17:55:55.581-0700 7f32e45b9f00 -1 Falling back to public interface
2021-05-15T17:55:55.837-0700 7f32e45b9f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCbbaBgXFr1EBAAHdCRVTyTO6R03Do6cr2jaA== --osd-uuid b74286ee-6b48-44e6-aa83-5b854f497679 
2021-05-15T17:55:55.961-0700 7fb6a03a2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:55:55.981-0700 7fb6a03a2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T17:55:55.981-0700 7fb6a03a2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5610a75d8000 @  0x7fb6a0d6b680 0x7fb6a0d8c824 0x56109be89447 0x56109be914b5 0x56109be899c8 0x56109be89b37 0x56109be8aee4 0x56109bc5bca1 0x56109be33423 0x56109bc4c2a7 0x56109bc5153a 0x7fb6a08bed84 0x7fb6a0a43609 0x7fb6a05ac293
2021-05-15T17:55:56.297-0700 7fb6a03a2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new beacb3ab-7d84-4d05-ac8a-1c36af9f1492 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55e917448000 @  0x7faf4dfc5680 0x7faf4dfe6824 0x55e90ba59447 0x55e90ba614b5 0x55e90ba599c8 0x55e90ba59b37 0x55e90ba5aee4 0x55e90b82bca1 0x55e90ba03423 0x55e90b81c2a7 0x55e90b82153a 0x7faf4db18d84 0x7faf4dc9d609 0x7faf4d806293
2021-05-15T17:55:56.961-0700 7faf4d5fcf00 -1 Falling back to public interface
2021-05-15T17:55:57.217-0700 7faf4d5fcf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCcbaBgJnwaJxAAUGr+vJCqD02qT35QkCZ/CA== --osd-uuid beacb3ab-7d84-4d05-ac8a-1c36af9f1492 
2021-05-15T17:55:57.329-0700 7f5cc0a84f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:55:57.353-0700 7f5cc0a84f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T17:55:57.353-0700 7f5cc0a84f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x556ef16e4000 @  0x7f5cc144d680 0x7f5cc146e824 0x556ee63d6447 0x556ee63de4b5 0x556ee63d69c8 0x556ee63d6b37 0x556ee63d7ee4 0x556ee61a8ca1 0x556ee6380423 0x556ee61992a7 0x556ee619e53a 0x7f5cc0fa0d84 0x7f5cc1125609 0x7f5cc0c8e293
2021-05-15T17:55:57.729-0700 7f5cc0a84f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x558f02b8e000 @  0x7f0a0ed64680 0x7f0a0ed85824 0x558ef7908447 0x558ef79104b5 0x558ef79089c8 0x558ef7908b37 0x558ef7909ee4 0x558ef76daca1 0x558ef78b2423 0x558ef76cb2a7 0x558ef76d053a 0x7f0a0e8b7d84 0x7f0a0ea3c609 0x7f0a0e5a5293
2021-05-15T17:55:58.341-0700 7f0a0e39bf00 -1 Falling back to public interface
2021-05-15T17:55:58.613-0700 7f0a0e39bf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T17:56:02,686783538-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T17:56:02,708723301-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T17:56:02,790681972-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:56:02,796941806-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T17:56:06,978119361-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:56:06,985196535-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T17:56:10,887568187-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:56:10,893848290-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T17:56:14,732603139-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:56:14,739050175-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T17:56:22,543664653-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:56:22,549954252-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T17:56:26,794163289-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:56:26,800378445-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T17:56:31,782752708-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:56:31,789177510-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T17:56:36,394567495-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:56:36,401175808-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:56:40,618546624-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:56:40,625053985-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T17:56:45,337150166-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:56:45,343578988-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T17:56:49,608831122-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:56:49,615145055-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T17:56:53,311137523-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:56:53,317419185-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.06   90      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.94   89      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  1.01   80      up          osd.2  
                       TOTAL  300 GiB  177 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.94/1.06  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T17:56:57,287972885-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:57:21,436560592-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:57:30,518371344-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:57:39,443998421-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:57:48,707303316-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:57:57,828186480-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   peering
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:57:57,847201006-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:58:06,819428292-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:58:06,838667495-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:58:15,786040193-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:58:15,805040600-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:58:25,198353513-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:58:25,217085584-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:58:34,321946809-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:58:34,341208590-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:58:34,356815073-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:58:34,365939777-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:58:34,386711475-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=794816
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T17:58:34,403722438-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-15T17:58:34,444620049-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:58:34,450892212-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T00:58:36.059+0000 ffff8fc3b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:58:36.851+0000 ffff8fc3b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T00:58:36.851+0000 ffff8fc3b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T00:58:36.871232+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-16T00:58:36.871293+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-16T00:58:36.920254+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:58:36.920254+0000     0       0         0         0         0         0           -           0
2021-05-16T00:58:37.920441+0000     1     255      4789      4534   1133.43    1133.5   0.0964661   0.0534513
2021-05-16T00:58:38.920618+0000     2     255      9338      9083   1135.24   1137.25   0.0512394   0.0547926
2021-05-16T00:58:39.920813+0000     3     255     13871     13616    1134.5   1133.25   0.0442062   0.0555282
2021-05-16T00:58:40.921140+0000     4     255     18519     18264   1141.28      1162   0.0860727   0.0553948
2021-05-16T00:58:41.921480+0000     5     255     23025     22770   1138.25    1126.5   0.0495673   0.0557251
2021-05-16T00:58:42.921845+0000     6     255     27313     27058   1127.14      1072   0.0984734   0.0562667
2021-05-16T00:58:43.922080+0000     7     255     31586     31331   1118.69   1068.25   0.0510614   0.0568075
2021-05-16T00:58:44.922273+0000     8     255     35916     35661   1114.14    1082.5   0.0509999   0.0570361
2021-05-16T00:58:45.922437+0000     9     255     39654     39399   1094.17     934.5    0.100777   0.0578575
2021-05-16T00:58:46.922621+0000    10     255     43563     43308   1082.46    977.25    0.108109   0.0586937
2021-05-16T00:58:47.922843+0000    11     255     48009     47754   1085.08    1111.5   0.0982365   0.0587377
2021-05-16T00:58:48.923055+0000    12     255     51969     51714   1077.14       990   0.0343591   0.0591828
2021-05-16T00:58:49.923381+0000    13     255     56190     55935   1075.43   1055.25   0.0568488   0.0592642
2021-05-16T00:58:50.923618+0000    14     255     60413     60158      1074   1055.75   0.0553887   0.0593901
2021-05-16T00:58:51.923911+0000    15     255     64643     64388   1072.88    1057.5   0.0440896     0.05947
2021-05-16T00:58:52.924338+0000    16     255     68723     68468   1069.55      1020   0.0486561   0.0596505
2021-05-16T00:58:53.924527+0000    17     255     73037     72782   1070.06    1078.5   0.0602491   0.0596219
2021-05-16T00:58:54.924726+0000    18     255     76255     76000    1055.3     804.5   0.0435342   0.0604801
2021-05-16T00:58:55.924929+0000    19     255     80470     80215   1055.21   1053.75   0.0555449   0.0604727
2021-05-16T00:58:56.925117+0000 min lat: 0.00357512 max lat: 0.257486 avg lat: 0.0604517
2021-05-16T00:58:56.925117+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T00:58:56.925117+0000    20     108     84664     84556    1056.7   1085.25   0.0674693   0.0604517
2021-05-16T00:58:57.925387+0000 Total time run:         20.0149
Total writes made:      84664
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     1057.51
Stddev Bandwidth:       82.7774
Max bandwidth (MB/sec): 1162
Min bandwidth (MB/sec): 804.5
Average IOPS:           4230
Stddev IOPS:            331.11
Max IOPS:               4648
Min IOPS:               3218
Average Latency(s):     0.0604229
Stddev Latency(s):      0.023498
Max latency(s):         0.257486
Min latency(s):         0.00357512

[1;32mlocalhost.localdomain	[2021-05-15T17:58:58,541873849-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 794816


[1;33mlocalhost.localdomain	[2021-05-15T17:58:58,551386248-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:59:22,817607018-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 84.67k objects, 21 GiB
    usage:   41 GiB used, 259 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:59:22,836560163-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:59:31,863204837-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 84.67k objects, 21 GiB
    usage:   41 GiB used, 259 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:59:31,882219763-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:59:41,003022964-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 84.67k objects, 21 GiB
    usage:   41 GiB used, 259 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:59:41,022027793-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:59:50,112395061-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 84.67k objects, 21 GiB
    usage:   41 GiB used, 259 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:59:50,131491495-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:59:59,568547028-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 84.67k objects, 21 GiB
    usage:   41 GiB used, 259 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T17:59:59,587475492-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T17:59:59,603401061-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T17:59:59,612348073-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T17:59:59,633499241-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=798293
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T17:59:59,651189590-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-15T17:59:59,692072640-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T17:59:59,698667807-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14b506dd-62e8-402f-8335-011549041711', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14b506dd-62e8-402f-8335-011549041711 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FtK2ZP:/tmp/ceph-asok.FtK2ZP -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:00:01.217+0000 ffffb2d1c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:00:02.086+0000 ffffb2d1c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:00:02.086+0000 ffffb2d1c010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:00:02.112220+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:00:02.112220+0000     0       0         0         0         0         0           -           0
2021-05-16T01:00:03.112404+0000     1     256      6436      6180   1544.49      1545   0.0373987    0.040464
2021-05-16T01:00:04.112638+0000     2     256     12543     12287   1535.44   1526.75    0.042595   0.0410817
2021-05-16T01:00:05.112908+0000     3     255     18686     18431   1535.49      1536   0.0417725   0.0412478
2021-05-16T01:00:06.113092+0000     4     255     24805     24550   1533.98   1529.75    0.041301   0.0413746
2021-05-16T01:00:07.113292+0000     5     255     30838     30583   1528.78   1508.25   0.0408828    0.041564
2021-05-16T01:00:08.113510+0000     6     255     36922     36667   1527.43      1521   0.0423141   0.0416247
2021-05-16T01:00:09.113728+0000     7     255     43436     43181   1541.81    1628.5   0.0379869   0.0412735
2021-05-16T01:00:10.113930+0000     8     255     49912     49657   1551.42      1619   0.0385146   0.0410319
2021-05-16T01:00:11.114132+0000     9     256     55967     55711   1547.17    1513.5   0.0417662   0.0411485
2021-05-16T01:00:12.114327+0000    10     255     62510     62255   1556.02      1636   0.0380435   0.0409332
2021-05-16T01:00:13.114726+0000    11     255     69036     68781   1562.83    1631.5   0.0492521   0.0407425
2021-05-16T01:00:14.115063+0000    12     255     74840     74585   1553.47      1451   0.0415489   0.0409958
2021-05-16T01:00:15.115295+0000    13     255     81051     80796   1553.38   1552.75   0.0402443   0.0410213
2021-05-16T01:00:16.115545+0000 Total time run:       13.6002
Total reads made:     84664
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1556.3
Average IOPS:         6225
Stddev IOPS:          229.995
Max IOPS:             6544
Min IOPS:             5804
Average Latency(s):   0.040983
Max latency(s):       0.14869
Min latency(s):       0.000513822

[1;32mlocalhost.localdomain	[2021-05-15T18:00:16,747948428-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 798293


[1;33mlocalhost.localdomain	[2021-05-15T18:00:16,757153333-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:00:40,967118059-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 84.67k objects, 21 GiB
    usage:   41 GiB used, 259 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:00:40,990038490-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:00:50,241808351-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 84.67k objects, 21 GiB
    usage:   41 GiB used, 259 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:00:50,260899516-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:00:59,349848271-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 84.67k objects, 21 GiB
    usage:   41 GiB used, 259 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:00:59,368729340-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:01:08,423009289-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 84.67k objects, 21 GiB
    usage:   41 GiB used, 259 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:01:08,442293176-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:01:17,738996926-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 84.67k objects, 21 GiB
    usage:   41 GiB used, 259 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:01:17,759965719-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:01:17,781473494-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T18:01:17,791981655-07:00][RUNNING][ROUND 1/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:01:17,802152033-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T18:01:17,821385601-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40016\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.75252\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a97121d5-5eb6-4800-822d-ff4ad53ef16b\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid a97121d5-5eb6-4800-822d-ff4ad53ef16b\nlast_changed 2021-05-15T18:01:46.202715-0700\ncreated 2021-05-15T18:01:46.202715-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40016/0,v1:10.10.1.2:40017/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.75252 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 ae57865f-68d3-4e8d-8102-62bb7a546742\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 bc995a72-4146-4bb4-a111-55ecc82385dd\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 cfe4434c-3328-497d-b0d6-ae1b6574c293\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42016\n  w/ user/pass: admin / 0dfa5ae7-52f7-4358-ba9e-97a830ab9018\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 18:02:05 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40016
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.75252
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a97121d5-5eb6-4800-822d-ff4ad53ef16b
setting min_mon_release = octopus
epoch 0
fsid a97121d5-5eb6-4800-822d-ff4ad53ef16b
last_changed 2021-05-15T18:01:46.202715-0700
created 2021-05-15T18:01:46.202715-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40016/0,v1:10.10.1.2:40017/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.75252 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 ae57865f-68d3-4e8d-8102-62bb7a546742
0
start osd.0
add osd1 bc995a72-4146-4bb4-a111-55ecc82385dd
1
start osd.1
add osd2 cfe4434c-3328-497d-b0d6-ae1b6574c293
2
start osd.2


restful urls: https://10.10.1.2:42016
  w/ user/pass: admin / 0dfa5ae7-52f7-4358-ba9e-97a830ab9018


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T18:01:19.760-0700 7f741f7771c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:01:19.764-0700 7f741f7771c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:01:19.780-0700 7f8178d2c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:01:19.780-0700 7f8178d2c1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40016,v1:10.10.1.2:40017] --print /tmp/ceph_monmap.75252 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.75252 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.75252 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42016 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x564d828ce000 @  0x7f77303c7680 0x7f77303e8824 0x7f7730b83187 0x7f7730b8b355 0x7f7730b83708 0x7f7730b83877 0x7f7730b84c24 0x7f7730b9cec1 0x7f7730b0f5f3 0x7f7730b70e97 0x7f7730b78b1a 0x7f773027bd84 0x7f7730397609 0x7f772ff6b293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.2pgWu4Lj5a 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ae57865f-68d3-4e8d-8102-62bb7a546742 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAEb6BgotT/IxAAGiBeBIkpkKRg9TEYR4nr9w== --osd-uuid ae57865f-68d3-4e8d-8102-62bb7a546742 
2021-05-15T18:01:57.236-0700 7fdc5b10ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:01:57.256-0700 7fdc5b10ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:01:57.256-0700 7fdc5b10ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55abe99ca000 @  0x7fdc5bad8680 0x7fdc5baf9824 0x55abdeea0447 0x55abdeea84b5 0x55abdeea09c8 0x55abdeea0b37 0x55abdeea1ee4 0x55abdec72ca1 0x55abdee4a423 0x55abdec632a7 0x55abdec6853a 0x7fdc5b62bd84 0x7fdc5b7b0609 0x7fdc5b319293
2021-05-15T18:01:57.544-0700 7fdc5b10ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new bc995a72-4146-4bb4-a111-55ecc82385dd -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55f5c8474000 @  0x7f44c905b680 0x7f44c907c824 0x55f5be510447 0x55f5be5184b5 0x55f5be5109c8 0x55f5be510b37 0x55f5be511ee4 0x55f5be2e2ca1 0x55f5be4ba423 0x55f5be2d32a7 0x55f5be2d853a 0x7f44c8baed84 0x7f44c8d33609 0x7f44c889c293
2021-05-15T18:01:58.132-0700 7f44c8692f00 -1 Falling back to public interface
2021-05-15T18:01:58.388-0700 7f44c8692f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAFb6BgDpnZMRAAHZiadz7bfIwTC8PKVyaN4w== --osd-uuid bc995a72-4146-4bb4-a111-55ecc82385dd 
2021-05-15T18:01:58.484-0700 7f4dba927f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:01:58.509-0700 7f4dba927f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:01:58.509-0700 7f4dba927f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5592ffd4e000 @  0x7f4dbb2f0680 0x7f4dbb311824 0x5592f4506447 0x5592f450e4b5 0x5592f45069c8 0x5592f4506b37 0x5592f4507ee4 0x5592f42d8ca1 0x5592f44b0423 0x5592f42c92a7 0x5592f42ce53a 0x7f4dbae43d84 0x7f4dbafc8609 0x7f4dbab31293
2021-05-15T18:01:58.877-0700 7f4dba927f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new cfe4434c-3328-497d-b0d6-ae1b6574c293 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x562f3a33e000 @  0x7fef82afa680 0x7fef82b1b824 0x562f2efc2447 0x562f2efca4b5 0x562f2efc29c8 0x562f2efc2b37 0x562f2efc3ee4 0x562f2ed94ca1 0x562f2ef6c423 0x562f2ed852a7 0x562f2ed8a53a 0x7fef8264dd84 0x7fef827d2609 0x7fef8233b293
2021-05-15T18:01:59.501-0700 7fef82131f00 -1 Falling back to public interface
2021-05-15T18:01:59.761-0700 7fef82131f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAHb6BgJ0HeCxAAmREU6riW+XThqkVsMF/Qig== --osd-uuid cfe4434c-3328-497d-b0d6-ae1b6574c293 
2021-05-15T18:01:59.865-0700 7fd7799c9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:01:59.885-0700 7fd7799c9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:01:59.885-0700 7fd7799c9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x559636cc6000 @  0x7fd77a392680 0x7fd77a3b3824 0x55962b912447 0x55962b91a4b5 0x55962b9129c8 0x55962b912b37 0x55962b913ee4 0x55962b6e4ca1 0x55962b8bc423 0x55962b6d52a7 0x55962b6da53a 0x7fd779ee5d84 0x7fd77a06a609 0x7fd779bd3293
2021-05-15T18:02:00.205-0700 7fd7799c9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x564089d5c000 @  0x7f9c8dd24680 0x7f9c8dd45824 0x56407f113447 0x56407f11b4b5 0x56407f1139c8 0x56407f113b37 0x56407f114ee4 0x56407eee5ca1 0x56407f0bd423 0x56407eed62a7 0x56407eedb53a 0x7f9c8d877d84 0x7f9c8d9fc609 0x7f9c8d565293
2021-05-15T18:02:00.817-0700 7f9c8d35bf00 -1 Falling back to public interface
2021-05-15T18:02:01.085-0700 7f9c8d35bf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T18:02:05,218814960-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T18:02:05,241239043-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T18:02:05,324975945-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:02:05,331406592-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T18:02:09,235286472-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:02:09,241934551-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T18:02:13,122059309-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:02:13,128412243-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T18:02:17,014509037-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:02:17,021110548-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T18:02:24,954809805-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:02:24,961329027-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T18:02:29,234557632-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:02:29,240884387-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T18:02:33,830946622-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:02:33,837336431-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T18:02:38,307619883-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:02:38,314093077-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:02:42,870268670-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:02:42,876782221-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:02:47,080810198-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:02:47,087144768-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T18:02:51,888886476-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:02:51,895630211-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T18:02:55,926677019-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:02:55,932898515-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.08   90      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   89      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   80      up          osd.2  
                       TOTAL  300 GiB  173 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.08  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T18:02:59,606988718-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:03:23,476870688-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:03:34,787856479-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:03:43,983564800-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:03:52,965579990-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:04:02,158161019-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   245 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:04:02,177434650-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:04:11,434895174-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:04:11,454292799-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:04:20,590624527-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:04:20,610072289-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:04:29,783125761-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:04:29,802365119-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:04:38,926821690-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:04:38,946091306-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:04:38,961998095-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:04:38,971208561-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:04:38,992734560-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=811445
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T18:04:39,010000799-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-15T18:04:39,051865306-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:04:39,058299578-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:04:40.553+0000 ffffb9e55010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:04:41.365+0000 ffffb9e55010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:04:41.365+0000 ffffb9e55010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:04:41.388381+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-16T01:04:41.388491+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T01:04:41.601073+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:04:41.601073+0000     0       0         0         0         0         0           -           0
2021-05-16T01:04:42.601352+0000     1     255      1147       892   891.882       892    0.204105    0.224447
2021-05-16T01:04:43.601709+0000     2     255      2220      1965    982.26      1073    0.244067    0.229817
2021-05-16T01:04:44.602065+0000     3     255      3298      3043   1014.05      1078    0.231273    0.232134
2021-05-16T01:04:45.605130+0000     4     255      4359      4104      1025      1061    0.275399      0.2341
2021-05-16T01:04:46.605412+0000     5     255      5443      5188   1036.73      1084    0.239039    0.235548
2021-05-16T01:04:47.605670+0000     6     255      6501      6246   1040.23      1058    0.233272    0.236283
2021-05-16T01:04:48.605886+0000     7     255      7619      7364    1051.3      1118    0.222362     0.23587
2021-05-16T01:04:49.606427+0000     8     255      8679      8424   1052.32      1060    0.221744    0.236416
2021-05-16T01:04:50.606952+0000     9     255      9891      9636   1069.99      1212    0.206755    0.233448
2021-05-16T01:04:51.607495+0000    10     255     11096     10841   1083.42      1205    0.208256    0.231223
2021-05-16T01:04:52.607820+0000    11     255     12288     12033   1093.25      1192    0.232674    0.229218
2021-05-16T01:04:53.608228+0000    12     255     13424     13169   1096.78      1136    0.211626    0.229016
2021-05-16T01:04:54.608682+0000    13     255     14522     14267   1096.83      1098    0.237398    0.229079
2021-05-16T01:04:55.609078+0000    14     255     15757     15502   1106.66      1235    0.207154    0.227663
2021-05-16T01:04:56.609574+0000    15     255     16968     16713   1113.58      1211    0.214162    0.226554
2021-05-16T01:04:57.609895+0000    16     255     18120     17865   1115.96      1152    0.217333    0.226218
2021-05-16T01:04:58.610242+0000    17     255     19352     19097   1122.76      1232    0.204699    0.225012
2021-05-16T01:04:59.610661+0000    18     255     20577     20322   1128.41      1225    0.204828    0.224108
2021-05-16T01:05:00.610945+0000    19     255     21706     21451   1128.42      1129    0.228363    0.224124
2021-05-16T01:05:01.611166+0000 min lat: 0.00967691 max lat: 0.277499 avg lat: 0.223833
2021-05-16T01:05:01.611166+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:05:01.611166+0000    20       6     22750     22744   1136.63      1293   0.0249803    0.223833
2021-05-16T01:05:02.611440+0000 Total time run:         20.0129
Total writes made:      22750
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     1136.77
Stddev Bandwidth:       91.7626
Max bandwidth (MB/sec): 1293
Min bandwidth (MB/sec): 892
Average IOPS:           1136
Stddev IOPS:            91.7626
Max IOPS:               1293
Min IOPS:               892
Average Latency(s):     0.223781
Stddev Latency(s):      0.0205322
Max latency(s):         0.277499
Min latency(s):         0.00967691

[1;32mlocalhost.localdomain	[2021-05-15T18:05:03,299422351-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 811445


[1;33mlocalhost.localdomain	[2021-05-15T18:05:03,309320670-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:05:27,534073721-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.75k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:05:27,553800756-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:05:36,712822800-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.75k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:05:36,732604713-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:05:45,722503136-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.75k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:05:45,742840355-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:05:55,167350983-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.75k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:05:55,187164653-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:06:04,665261033-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.75k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:06:04,684380289-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:06:04,700778879-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:06:04,710070621-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:06:04,731668926-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=814981
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T18:06:04,749509598-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-15T18:06:04,790388438-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:06:04,796734054-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a65e84-46ca-4a67-b5c4-129912f081d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a65e84-46ca-4a67-b5c4-129912f081d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I0YMtk:/tmp/ceph-asok.I0YMtk -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:06:06.304+0000 ffffb791a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:06:07.100+0000 ffffb791a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:06:07.100+0000 ffffb791a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:06:07.123123+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:06:07.123123+0000     0       0         0         0         0         0           -           0
2021-05-16T01:06:08.123665+0000     1     255      1207       952   951.338       952    0.241353    0.225449
2021-05-16T01:06:09.123902+0000     2     255      2189      1934   966.549       982    0.210486     0.23997
2021-05-16T01:06:10.124512+0000     3     255      3173      2918   972.167       984     0.25861    0.248201
2021-05-16T01:06:11.124749+0000     4     255      4181      3926   981.064      1008    0.251304    0.249712
2021-05-16T01:06:12.124999+0000     5     255      5168      4913   982.201       987    0.280564    0.250707
2021-05-16T01:06:13.126059+0000     6     255      6120      5865   976.997       952     0.21511    0.253571
2021-05-16T01:06:14.126295+0000     7     255      7079      6824   974.394       959    0.405839    0.255218
2021-05-16T01:06:15.126515+0000     8     255      8051      7796   974.068       972    0.217433    0.256197
2021-05-16T01:06:16.127000+0000     9     255      8984      8729   969.455       933    0.416669    0.258155
2021-05-16T01:06:17.127620+0000    10     255      9962      9707   970.249       978     0.21288    0.258603
2021-05-16T01:06:18.127930+0000    11     255     10938     10683   970.744       976    0.258978    0.259136
2021-05-16T01:06:19.128705+0000    12     255     11902     11647    970.12       964    0.267089    0.259502
2021-05-16T01:06:20.129078+0000    13     255     12864     12609   969.468       962    0.254638    0.260153
2021-05-16T01:06:21.129803+0000    14     255     13845     13590   970.241       981    0.194026    0.259853
2021-05-16T01:06:22.130306+0000    15     255     14799     14544   969.126       954    0.195773    0.260364
2021-05-16T01:06:23.130743+0000    16     255     15718     15463   965.968       919    0.415544    0.261357
2021-05-16T01:06:24.130981+0000    17     255     16684     16429   965.957       966    0.201099    0.261538
2021-05-16T01:06:25.131209+0000    18     255     17622     17367   964.392       938     0.20994    0.262283
2021-05-16T01:06:26.131424+0000    19     255     18586     18331    964.36       964    0.245978    0.262377
2021-05-16T01:06:27.131630+0000 min lat: 0.114069 max lat: 0.54053 avg lat: 0.262834
2021-05-16T01:06:27.131630+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:06:27.131630+0000    20     255     19524     19269   963.033       938    0.210383    0.262834
2021-05-16T01:06:28.132022+0000    21     255     20493     20238   963.299       969    0.272673    0.263071
2021-05-16T01:06:29.132258+0000    22     255     21475     21220   964.138       982    0.253101    0.263018
2021-05-16T01:06:30.132550+0000    23     255     22422     22167   963.381       947     0.26403    0.263213
2021-05-16T01:06:31.132934+0000 Total time run:       23.4911
Total reads made:     22750
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   968.451
Average IOPS:         968
Stddev IOPS:          20.5359
Max IOPS:             1008
Min IOPS:             919
Average Latency(s):   0.262442
Max latency(s):       0.54053
Min latency(s):       0.0774753

[1;32mlocalhost.localdomain	[2021-05-15T18:06:31,829021387-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 814981


[1;33mlocalhost.localdomain	[2021-05-15T18:06:31,838918827-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:06:55,899809688-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.75k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:06:55,919641724-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:07:05,121873003-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.75k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:07:05,141835905-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:07:14,245868941-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.75k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:07:14,265833011-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:07:23,263658412-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.75k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:07:23,283756485-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:07:32,234814343-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.75k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:07:32,254171869-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:07:32,270494608-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T18:07:32,276718367-07:00][RUNNING][ROUND 2/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:07:32,286187116-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T18:07:32,304421892-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40056\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.76343\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid bc889b22-b6ea-44bf-9dee-0a5a7f0fa1e0\nsetting min_mon_release = octopus\nepoch 0\nfsid bc889b22-b6ea-44bf-9dee-0a5a7f0fa1e0\nlast_changed 2021-05-15T18:08:00.319302-0700\ncreated 2021-05-15T18:08:00.319302-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40056/0,v1:10.10.1.2:40057/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.76343 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 6b41706f-9028-4ccc-b2f9-8481a0ea1811\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 09dd2a33-8bd1-4f53-adca-0bd800139973\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 c0377601-890d-4c90-a1d6-bd1422795742\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42056\n  w/ user/pass: admin / 21f69c2a-77a9-4920-8ecd-7e8f15b9a3bd\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 18:08:19 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40056
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.76343
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid bc889b22-b6ea-44bf-9dee-0a5a7f0fa1e0
setting min_mon_release = octopus
epoch 0
fsid bc889b22-b6ea-44bf-9dee-0a5a7f0fa1e0
last_changed 2021-05-15T18:08:00.319302-0700
created 2021-05-15T18:08:00.319302-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40056/0,v1:10.10.1.2:40057/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.76343 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 6b41706f-9028-4ccc-b2f9-8481a0ea1811
0
start osd.0
add osd1 09dd2a33-8bd1-4f53-adca-0bd800139973
1
start osd.1
add osd2 c0377601-890d-4c90-a1d6-bd1422795742
2
start osd.2


restful urls: https://10.10.1.2:42056
  w/ user/pass: admin / 21f69c2a-77a9-4920-8ecd-7e8f15b9a3bd


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T18:07:34.216-0700 7fef4039b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:07:34.216-0700 7fef4039b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:07:34.232-0700 7f7ecfb321c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:07:34.232-0700 7f7ecfb321c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40056,v1:10.10.1.2:40057] --print /tmp/ceph_monmap.76343 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.76343 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.76343 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42056 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55ff9f884000 @  0x7f8820ff2680 0x7f8821013824 0x7f88217ae187 0x7f88217b6355 0x7f88217ae708 0x7f88217ae877 0x7f88217afc24 0x7f88217c7ec1 0x7f882173a5f3 0x7f882179be97 0x7f88217a3b1a 0x7f8820ea6d84 0x7f8820fc2609 0x7f8820b96293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.oKfhNF8YBY 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6b41706f-9028-4ccc-b2f9-8481a0ea1811 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB6cKBgkQczLxAAw7a1CrUxFI1KmnC/srz++Q== --osd-uuid 6b41706f-9028-4ccc-b2f9-8481a0ea1811 
2021-05-15T18:08:11.416-0700 7f667fae7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:08:11.436-0700 7f667fae7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:08:11.436-0700 7f667fae7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x561b72eac000 @  0x7f66804b0680 0x7f66804d1824 0x561b68099447 0x561b680a14b5 0x561b680999c8 0x561b68099b37 0x561b6809aee4 0x561b67e6bca1 0x561b68043423 0x561b67e5c2a7 0x561b67e6153a 0x7f6680003d84 0x7f6680188609 0x7f667fcf1293
2021-05-15T18:08:11.744-0700 7f667fae7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 09dd2a33-8bd1-4f53-adca-0bd800139973 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55ed9c88c000 @  0x7f50585e6680 0x7f5058607824 0x55ed920ca447 0x55ed920d24b5 0x55ed920ca9c8 0x55ed920cab37 0x55ed920cbee4 0x55ed91e9cca1 0x55ed92074423 0x55ed91e8d2a7 0x55ed91e9253a 0x7f5058139d84 0x7f50582be609 0x7f5057e27293
2021-05-15T18:08:12.368-0700 7f5057c1df00 -1 Falling back to public interface
2021-05-15T18:08:12.624-0700 7f5057c1df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB8cKBgBT2EAxAAZH65sTGjQn/SFX9iu6kEwA== --osd-uuid 09dd2a33-8bd1-4f53-adca-0bd800139973 
2021-05-15T18:08:12.688-0700 7fe5a12dcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:08:12.708-0700 7fe5a12dcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:08:12.708-0700 7fe5a12dcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55f190a64000 @  0x7fe5a1ca5680 0x7fe5a1cc6824 0x55f186371447 0x55f1863794b5 0x55f1863719c8 0x55f186371b37 0x55f186372ee4 0x55f186143ca1 0x55f18631b423 0x55f1861342a7 0x55f18613953a 0x7fe5a17f8d84 0x7fe5a197d609 0x7fe5a14e6293
2021-05-15T18:08:13.012-0700 7fe5a12dcf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c0377601-890d-4c90-a1d6-bd1422795742 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5593d0d9e000 @  0x7f57c9e2e680 0x7f57c9e4f824 0x5593c4f3e447 0x5593c4f464b5 0x5593c4f3e9c8 0x5593c4f3eb37 0x5593c4f3fee4 0x5593c4d10ca1 0x5593c4ee8423 0x5593c4d012a7 0x5593c4d0653a 0x7f57c9981d84 0x7f57c9b06609 0x7f57c966f293
2021-05-15T18:08:13.784-0700 7f57c9465f00 -1 Falling back to public interface
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB9cKBgs1e5ExAA3M2Cji4QQULPH/X3Wkjojw== --osd-uuid c0377601-890d-4c90-a1d6-bd1422795742 
2021-05-15T18:08:14.000-0700 7ffbc5463f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:08:14.020-0700 7ffbc5463f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:08:14.020-0700 7ffbc5463f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55892018e000 @  0x7ffbc5e2c680 0x7ffbc5e4d824 0x558914bed447 0x558914bf54b5 0x558914bed9c8 0x558914bedb37 0x558914beeee4 0x5589149bfca1 0x558914b97423 0x5589149b02a7 0x5589149b553a 0x7ffbc597fd84 0x7ffbc5b04609 0x7ffbc566d293
2021-05-15T18:08:14.068-0700 7f57c9465f00 -1 osd.1 0 log_to_monitors {default=true}
2021-05-15T18:08:14.324-0700 7ffbc5463f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x556021e08000 @  0x7f241fa76680 0x7f241fa97824 0x55601765d447 0x5560176654b5 0x55601765d9c8 0x55601765db37 0x55601765eee4 0x55601742fca1 0x556017607423 0x5560174202a7 0x55601742553a 0x7f241f5c9d84 0x7f241f74e609 0x7f241f2b7293
2021-05-15T18:08:14.912-0700 7f241f0adf00 -1 Falling back to public interface
2021-05-15T18:08:15.164-0700 7f241f0adf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T18:08:19,380209219-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T18:08:19,402879374-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T18:08:19,485288606-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:08:19,491989955-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T18:08:23,444220677-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:08:23,450653720-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T18:08:27,342210687-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:08:27,349589059-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T18:08:31,426765889-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:08:31,432973667-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T18:08:39,143382410-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:08:39,149857235-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T18:08:43,387323515-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:08:43,393744540-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T18:08:48,189841392-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:08:48,198082532-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T18:08:53,065349731-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:08:53,071910385-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:08:57,324624776-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:08:57,330857533-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:09:02,016425228-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:09:02,023241600-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T18:09:06,477042562-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:09:06,483379211-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T18:09:10,325946017-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:09:10,332188180-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   66 KiB   0 B   0 B   0 B  100 GiB     0  1.15   89      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.89   90      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   80      up          osd.2  
                       TOTAL  300 GiB  173 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.89/1.15  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T18:09:14,122493934-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:09:38,193444002-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:09:47,096590180-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:09:56,183510404-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:05,481219076-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:14,550137930-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:14,570016068-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:23,679951819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:23,699599682-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:33,060939041-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:33,080807080-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:42,265891827-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:42,285841269-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:51,397805831-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:51,417554732-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:51,433811150-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:10:51,442975934-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:10:51,464617037-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=828370
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T18:10:51,482414266-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-15T18:10:51,524321906-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:10:51,530722401-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:10:53.066+0000 ffff90a88010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:10:53.866+0000 ffff90a88010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:10:53.870+0000 ffff90a88010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:10:53.890522+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-16T01:10:53.890602+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-16T01:10:54.102289+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:10:54.102289+0000     0       0         0         0         0         0           -           0
2021-05-16T01:10:55.102602+0000     1     255      1234       979   978.843       979    0.209792    0.207239
2021-05-16T01:10:56.103042+0000     2     255      2463      2208   1103.67      1229    0.216217    0.208001
2021-05-16T01:10:57.103626+0000     3     255      3590      3335   1111.23      1127    0.230508    0.213941
2021-05-16T01:10:58.104018+0000     4     255      4764      4509   1126.81      1174    0.208919    0.215506
2021-05-16T01:10:59.104530+0000     5     255      5993      5738   1147.12      1229    0.206491    0.213901
2021-05-16T01:11:00.104927+0000     6     255      7197      6942   1156.52      1204    0.220744    0.213315
2021-05-16T01:11:01.105299+0000     7     255      8308      8053   1149.96      1111    0.211032    0.215978
2021-05-16T01:11:02.105661+0000     8     255      9446      9191   1148.41      1138    0.230576     0.21675
2021-05-16T01:11:03.105896+0000     9     255     10657     10402   1155.33      1211    0.205407    0.216558
2021-05-16T01:11:04.106406+0000    10     255     11810     11555   1155.04      1153    0.227411    0.216528
2021-05-16T01:11:05.106855+0000    11     255     12893     12638   1148.45      1083    0.218567    0.218782
2021-05-16T01:11:06.107106+0000    12     255     14050     13795   1149.14      1157    0.207654    0.218839
2021-05-16T01:11:07.107448+0000    13     255     15291     15036   1156.17      1241    0.196334    0.218049
2021-05-16T01:11:08.108071+0000    14     255     16424     16169   1154.46      1133    0.222395    0.218421
2021-05-16T01:11:09.108581+0000    15     255     17644     17389   1158.79      1220    0.207501    0.217851
2021-05-16T01:11:10.108836+0000    16     255     18864     18609    1162.6      1220    0.200792    0.217289
2021-05-16T01:11:11.109193+0000    17     255     19954     19699    1158.3      1090    0.233966    0.218237
2021-05-16T01:11:12.109686+0000    18     255     21005     20750   1152.31      1051    0.266113    0.219186
2021-05-16T01:11:13.109987+0000    19     255     22023     21768   1145.23      1018    0.237784    0.220871
2021-05-16T01:11:14.110308+0000 min lat: 0.0118284 max lat: 0.292937 avg lat: 0.220677
2021-05-16T01:11:14.110308+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:11:14.110308+0000    20      14     23103     23089      1154      1321   0.0359168    0.220677
2021-05-16T01:11:15.110591+0000 Total time run:         20.0116
Total writes made:      23103
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     1154.48
Stddev Bandwidth:       83.8912
Max bandwidth (MB/sec): 1321
Min bandwidth (MB/sec): 979
Average IOPS:           1154
Stddev IOPS:            83.8912
Max IOPS:               1321
Min IOPS:               979
Average Latency(s):     0.220559
Stddev Latency(s):      0.0214199
Max latency(s):         0.292937
Min latency(s):         0.0112557

[1;32mlocalhost.localdomain	[2021-05-15T18:11:15,773660839-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 828370


[1;33mlocalhost.localdomain	[2021-05-15T18:11:15,783608456-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:11:39,981389806-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.10k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:11:40,001668293-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:11:49,127117433-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.10k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:11:49,147096078-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:11:58,217623925-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.10k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:11:58,237844365-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:12:07,408019766-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.10k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:12:07,427983331-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:12:16,520767903-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.10k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:12:16,540804890-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:12:16,557201320-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:12:16,566342515-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:12:16,588479279-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=831875
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T18:12:16,606597675-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-15T18:12:16,647227210-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:12:16,653537267-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1894433c-e7a9-4572-8dca-1f4e2834532b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1894433c-e7a9-4572-8dca-1f4e2834532b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ciaiy7:/tmp/ceph-asok.ciaiy7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:12:18.101+0000 ffff889fc010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:12:18.925+0000 ffff889fc010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:12:18.925+0000 ffff889fc010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:12:18.947500+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:12:18.947500+0000     0       0         0         0         0         0           -           0
2021-05-16T01:12:19.947800+0000     1     255      1195       940   939.578       940    0.240396    0.229014
2021-05-16T01:12:20.948019+0000     2     255      2243      1988   993.668      1048    0.246384    0.236359
2021-05-16T01:12:21.949529+0000     3     255      3262      3007   1001.61      1019    0.253113    0.240513
2021-05-16T01:12:22.949762+0000     4     255      4214      3959   989.154       952    0.257929    0.247364
2021-05-16T01:12:23.949984+0000     5     255      5191      4936    986.68       977     0.25801    0.250281
2021-05-16T01:12:24.950510+0000     6     256      6164      5908   984.149       972    0.261483    0.252215
2021-05-16T01:12:25.950945+0000     7     255      7112      6857   979.069       949    0.266849    0.254579
2021-05-16T01:12:26.951259+0000     8     255      8070      7815   976.398       958    0.272044    0.255899
2021-05-16T01:12:27.951595+0000     9     255      9047      8792   976.428       977    0.258706    0.256813
2021-05-16T01:12:28.952106+0000    10     255     10023      9768   976.336       976    0.266012    0.257254
2021-05-16T01:12:29.952490+0000    11     255     10964     10709   973.091       941    0.263338      0.2585
2021-05-16T01:12:30.952766+0000    12     255     11944     11689   973.644       980    0.255354    0.258749
2021-05-16T01:12:31.953307+0000    13     255     12930     12675   974.554       986    0.258966    0.258804
2021-05-16T01:12:32.953500+0000    14     255     13892     13637   973.644       962    0.265897    0.259159
2021-05-16T01:12:33.953920+0000    15     255     14867     14612   973.707       975    0.465642     0.25923
2021-05-16T01:12:34.954463+0000    16     255     15849     15594   974.192       982    0.261812    0.259469
2021-05-16T01:12:35.954682+0000    17     255     16828     16573   974.462       979    0.255499    0.259609
2021-05-16T01:12:36.955329+0000    18     255     17777     17522   973.013       949    0.265902    0.260066
2021-05-16T01:12:37.956033+0000    19     255     18758     18503   973.397       981    0.207655    0.259963
2021-05-16T01:12:38.956239+0000 min lat: 0.114514 max lat: 0.53108 avg lat: 0.259981
2021-05-16T01:12:38.956239+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:12:38.956239+0000    20     255     19732     19477   973.417       974    0.194296    0.259981
2021-05-16T01:12:39.956606+0000    21     255     20694     20439   972.857       962    0.263472    0.260389
2021-05-16T01:12:40.956828+0000    22     255     21668     21413   972.899       974    0.251617    0.260495
2021-05-16T01:12:41.957369+0000    23     255     22652     22397   973.359       984    0.253608    0.260592
2021-05-16T01:12:42.957663+0000 Total time run:       23.6117
Total reads made:     23103
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   978.454
Average IOPS:         978
Stddev IOPS:          23.806
Max IOPS:             1048
Min IOPS:             940
Average Latency(s):   0.259696
Max latency(s):       0.53108
Min latency(s):       0.0750821

[1;32mlocalhost.localdomain	[2021-05-15T18:12:43,625741349-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 831875


[1;33mlocalhost.localdomain	[2021-05-15T18:12:43,635605157-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:13:07,694374690-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.10k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:13:07,714387488-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:13:17,269705732-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.10k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:13:17,289607060-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:13:26,414380159-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.10k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:13:26,433999422-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:13:35,393950686-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.10k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:13:35,414146918-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:13:44,688615239-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.10k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:13:44,708440997-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:13:44,725247440-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T18:13:44,731720288-07:00][RUNNING][ROUND 3/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:13:44,740878342-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T18:13:44,758926129-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40924\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.77439\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 089762b8-a1dc-4246-991c-dce518c50104\nsetting min_mon_release = octopus\nepoch 0\nfsid 089762b8-a1dc-4246-991c-dce518c50104\nlast_changed 2021-05-15T18:14:13.809486-0700\ncreated 2021-05-15T18:14:13.809486-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40924/0,v1:10.10.1.2:40925/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.77439 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 e2d7f58a-d82d-4d74-bc43-369ef1009c14\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 e756c69b-649c-45f2-8a35-2e669ac68343\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 5e7143ce-e848-4a73-991a-a029edd6f6e1\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42924\n  w/ user/pass: admin / 5cd61893-920d-46f9-a888-ca9159200e90\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 18:14:33 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40924
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.77439
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 089762b8-a1dc-4246-991c-dce518c50104
setting min_mon_release = octopus
epoch 0
fsid 089762b8-a1dc-4246-991c-dce518c50104
last_changed 2021-05-15T18:14:13.809486-0700
created 2021-05-15T18:14:13.809486-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40924/0,v1:10.10.1.2:40925/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.77439 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 e2d7f58a-d82d-4d74-bc43-369ef1009c14
0
start osd.0
add osd1 e756c69b-649c-45f2-8a35-2e669ac68343
1
start osd.1
add osd2 5e7143ce-e848-4a73-991a-a029edd6f6e1
2
start osd.2


restful urls: https://10.10.1.2:42924
  w/ user/pass: admin / 5cd61893-920d-46f9-a888-ca9159200e90


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T18:13:46.719-0700 7f292ac8c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:13:46.719-0700 7f292ac8c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:13:46.735-0700 7f06e81621c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:13:46.735-0700 7f06e81621c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40924,v1:10.10.1.2:40925] --print /tmp/ceph_monmap.77439 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.77439 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.77439 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42924 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x563c3826a000 @  0x7f4534d6b680 0x7f4534d8c824 0x7f4535527187 0x7f453552f355 0x7f4535527708 0x7f4535527877 0x7f4535528c24 0x7f4535540ec1 0x7f45354b35f3 0x7f4535514e97 0x7f453551cb1a 0x7f4534c1fd84 0x7f4534d3b609 0x7f453490f293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.caRBDRgQVs 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e2d7f58a-d82d-4d74-bc43-369ef1009c14 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDwcaBgdQSvGhAAIiufXcpVecZquO6SFdFz3Q== --osd-uuid e2d7f58a-d82d-4d74-bc43-369ef1009c14 
2021-05-15T18:14:25.087-0700 7f60e8012f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:14:25.107-0700 7f60e8012f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:14:25.107-0700 7f60e8012f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x557b92c74000 @  0x7f60e89db680 0x7f60e89fc824 0x557b87f71447 0x557b87f794b5 0x557b87f719c8 0x557b87f71b37 0x557b87f72ee4 0x557b87d43ca1 0x557b87f1b423 0x557b87d342a7 0x557b87d3953a 0x7f60e852ed84 0x7f60e86b3609 0x7f60e821c293
2021-05-15T18:14:25.403-0700 7f60e8012f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e756c69b-649c-45f2-8a35-2e669ac68343 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55c3e6634000 @  0x7fa0551f1680 0x7fa055212824 0x55c3db9ea447 0x55c3db9f24b5 0x55c3db9ea9c8 0x55c3db9eab37 0x55c3db9ebee4 0x55c3db7bcca1 0x55c3db994423 0x55c3db7ad2a7 0x55c3db7b253a 0x7fa054d44d84 0x7fa054ec9609 0x7fa054a32293
2021-05-15T18:14:26.011-0700 7fa054828f00 -1 Falling back to public interface
2021-05-15T18:14:26.267-0700 7fa054828f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDxcaBg/xKDKhAAYycdqfDjCPO0DQ+A0YjJbg== --osd-uuid e756c69b-649c-45f2-8a35-2e669ac68343 
2021-05-15T18:14:26.355-0700 7f1627a76f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:14:26.375-0700 7f1627a76f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:14:26.375-0700 7f1627a76f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55ae11a98000 @  0x7f162843f680 0x7f1628460824 0x55ae065c4447 0x55ae065cc4b5 0x55ae065c49c8 0x55ae065c4b37 0x55ae065c5ee4 0x55ae06396ca1 0x55ae0656e423 0x55ae063872a7 0x55ae0638c53a 0x7f1627f92d84 0x7f1628117609 0x7f1627c80293
2021-05-15T18:14:26.683-0700 7f1627a76f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5e7143ce-e848-4a73-991a-a029edd6f6e1 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5560f7a9c000 @  0x7f05b8850680 0x7f05b8871824 0x5560ed2f8447 0x5560ed3004b5 0x5560ed2f89c8 0x5560ed2f8b37 0x5560ed2f9ee4 0x5560ed0caca1 0x5560ed2a2423 0x5560ed0bb2a7 0x5560ed0c053a 0x7f05b83a3d84 0x7f05b8528609 0x7f05b8091293
2021-05-15T18:14:27.319-0700 7f05b7e87f00 -1 Falling back to public interface
2021-05-15T18:14:27.575-0700 7f05b7e87f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDzcaBgclkZABAAa8/rQKhRKMrkUqwtcZxagg== --osd-uuid 5e7143ce-e848-4a73-991a-a029edd6f6e1 
2021-05-15T18:14:27.687-0700 7f20e45b2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:14:27.711-0700 7f20e45b2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:14:27.711-0700 7f20e45b2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x557d50f9c000 @  0x7f20e4f7b680 0x7f20e4f9c824 0x557d46224447 0x557d4622c4b5 0x557d462249c8 0x557d46224b37 0x557d46225ee4 0x557d45ff6ca1 0x557d461ce423 0x557d45fe72a7 0x557d45fec53a 0x7f20e4aced84 0x7f20e4c53609 0x7f20e47bc293
2021-05-15T18:14:28.011-0700 7f20e45b2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5569a4390000 @  0x7f6764dc2680 0x7f6764de3824 0x5569999ad447 0x5569999b54b5 0x5569999ad9c8 0x5569999adb37 0x5569999aeee4 0x55699977fca1 0x556999957423 0x5569997702a7 0x55699977553a 0x7f6764915d84 0x7f6764a9a609 0x7f6764603293
2021-05-15T18:14:28.639-0700 7f67643f9f00 -1 Falling back to public interface
2021-05-15T18:14:28.907-0700 7f67643f9f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T18:14:33,035543238-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T18:14:33,057767573-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T18:14:33,140555761-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:14:33,146714153-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T18:14:37,060857933-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:14:37,067176658-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T18:14:41,101990607-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:14:41,108594726-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T18:14:45,240049891-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:14:45,246659980-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T18:14:53,044254183-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:14:53,050728319-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T18:14:57,942708558-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:14:57,950434613-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T18:15:02,163111671-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:15:02,169485919-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T18:15:06,858318020-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:15:06,864839953-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:15:11,849194708-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:15:11,855824165-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:15:16,269104695-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:15:16,275279454-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T18:15:20,769778549-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:15:20,776173711-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T18:15:24,723073773-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:15:24,730187663-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  190 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  190 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  0.98  153      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   69 KiB   0 B   0 B   0 B  100 GiB     0  1.09  152      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  0.93  143      up          osd.2  
                       TOTAL  300 GiB  192 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.93/1.09  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T18:15:28,622474135-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:15:52,696804955-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:16:02,079448773-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:16:11,537979314-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:16:20,507736192-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:16:29,606990754-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   245 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   peering
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:16:29,626845200-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:16:39,016995681-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:16:39,038974909-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:16:48,147278360-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:16:48,167081001-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:16:57,201820238-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:16:57,221884073-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:17:06,321628135-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:17:06,341546924-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:17:06,358219143-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:17:06,367438386-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:17:06,389495822-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=845260
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T18:17:06,407470443-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-15T18:17:06,449201722-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:17:06,455713781-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:17:08.107+0000 ffff8eae7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:17:08.907+0000 ffff8eae7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:17:08.907+0000 ffff8eae7010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:17:08.927781+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-16T01:17:08.927864+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-16T01:17:09.158409+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:17:09.158409+0000     0       0         0         0         0         0           -           0
2021-05-16T01:17:10.158707+0000     1     255      1169       914    913.86       914    0.210588    0.217972
2021-05-16T01:17:11.159270+0000     2     255      2396      2141   1070.12      1227    0.203798    0.213092
2021-05-16T01:17:12.160068+0000     3     255      3622      3367   1121.77      1226    0.201699    0.211779
2021-05-16T01:17:13.160698+0000     4     255      4757      4502    1124.9      1135     0.22996    0.214094
2021-05-16T01:17:14.161240+0000     5     255      5845      5590    1117.4      1088    0.208223    0.218808
2021-05-16T01:17:15.161530+0000     6     255      7002      6747   1123.94      1157    0.225837    0.219042
2021-05-16T01:17:16.161811+0000     7     255      8128      7873   1124.19      1126    0.224939    0.220298
2021-05-16T01:17:17.162314+0000     8     255      9193      8938   1116.73      1065    0.231769    0.222624
2021-05-16T01:17:18.162731+0000     9     255     10293     10038   1114.82      1100    0.225588     0.22386
2021-05-16T01:17:19.163332+0000    10     255     11471     11216   1121.06      1178     0.21021    0.223338
2021-05-16T01:17:20.163713+0000    11     255     12606     12351   1122.29      1135     0.24531     0.22321
2021-05-16T01:17:21.164300+0000    12     255     13694     13439   1119.38      1088    0.220441    0.224384
2021-05-16T01:17:22.164978+0000    13     255     14868     14613   1123.52      1174     0.21452    0.223873
2021-05-16T01:17:23.165490+0000    14     255     16070     15815   1129.08      1202    0.211226    0.223076
2021-05-16T01:17:24.165923+0000    15     255     17128     16873   1124.31      1058    0.256885    0.223995
2021-05-16T01:17:25.166424+0000    16     255     18225     17970   1122.57      1097    0.236526    0.224712
2021-05-16T01:17:26.166959+0000    17     255     19318     19063    1120.8      1093    0.236301    0.225264
2021-05-16T01:17:27.167215+0000    18     255     20422     20167   1119.85      1104    0.221043    0.225693
2021-05-16T01:17:28.167635+0000    19     255     21513     21258   1118.31      1091    0.204645    0.226251
2021-05-16T01:17:29.168235+0000 Total time run:         20.0095
Total writes made:      22678
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     1133.36
Stddev Bandwidth:       71.3468
Max bandwidth (MB/sec): 1227
Min bandwidth (MB/sec): 914
Average IOPS:           1133
Stddev IOPS:            71.3468
Max IOPS:               1227
Min IOPS:               914
Average Latency(s):     0.224483
Stddev Latency(s):      0.0187127
Max latency(s):         0.272016
Min latency(s):         0.0150391

[1;32mlocalhost.localdomain	[2021-05-15T18:17:29,822451021-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 845260


[1;33mlocalhost.localdomain	[2021-05-15T18:17:29,832116485-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:17:54,056841403-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.68k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:17:54,076632113-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:18:03,178444238-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.68k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:18:03,198313201-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:18:12,359586072-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.68k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:18:12,379354592-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:18:21,698907599-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.68k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:18:21,718599651-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:18:30,860226108-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.68k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:18:30,880023958-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:18:30,896428602-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:18:30,906036680-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:18:30,928282213-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=848658
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T18:18:30,946144041-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-15T18:18:30,986994465-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:18:30,993343460-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '648d1e90-7deb-4a51-9561-026252badf72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 648d1e90-7deb-4a51-9561-026252badf72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iWc9hY:/tmp/ceph-asok.iWc9hY -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:18:32.769+0000 ffffa683d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:18:33.585+0000 ffffa683d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:18:33.585+0000 ffffa683d010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:18:33.610733+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:18:33.610733+0000     0       0         0         0         0         0           -           0
2021-05-16T01:18:34.610911+0000     1     255      1214       959   958.689       959    0.233184    0.222371
2021-05-16T01:18:35.611181+0000     2     255      2231      1976   987.707      1017     0.22708    0.236809
2021-05-16T01:18:36.611650+0000     3     255      3225      2970   989.649       994    0.257064    0.243766
2021-05-16T01:18:37.611882+0000     4     255      4192      3937   983.932       967    0.264585    0.248154
2021-05-16T01:18:38.612124+0000     5     255      5140      4885     976.7       948    0.290617    0.251848
2021-05-16T01:18:39.612782+0000     6     255      6107      5852   974.977       967    0.255813     0.25484
2021-05-16T01:18:40.613213+0000     7     255      7078      6823   974.349       971    0.276932    0.255542
2021-05-16T01:18:41.614104+0000     8     255      8075      7820    977.07       997    0.250302    0.256158
2021-05-16T01:18:42.614664+0000     9     255      9025      8770   974.003       950    0.273119    0.257248
2021-05-16T01:18:43.614875+0000    10     255      9985      9730   972.583       960    0.208324    0.257819
2021-05-16T01:18:44.615253+0000    11     255     10952     10697   972.042       967    0.253727    0.258672
2021-05-16T01:18:45.615507+0000    12     255     11899     11644   969.936       947    0.224254    0.259386
2021-05-16T01:18:46.615736+0000    13     255     12816     12561   965.848       917    0.400796    0.260618
2021-05-16T01:18:47.615962+0000    14     255     13766     13511   964.701       950    0.218488    0.261401
2021-05-16T01:18:48.616461+0000    15     255     14755     14500   966.288       989    0.256457    0.261508
2021-05-16T01:18:49.616745+0000    16     255     15690     15435   964.316       935    0.272878    0.262031
2021-05-16T01:18:50.617025+0000    17     255     16667     16412   965.046       977    0.257232    0.262201
2021-05-16T01:18:51.617247+0000    18     255     17633     17378   965.087       966    0.253022    0.262159
2021-05-16T01:18:52.617972+0000    19     255     18606     18351   965.467       973    0.261315    0.262295
2021-05-16T01:18:53.618189+0000 min lat: 0.114247 max lat: 0.529808 avg lat: 0.262352
2021-05-16T01:18:53.618189+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:18:53.618189+0000    20     255     19557     19302   964.733       951    0.191208    0.262352
2021-05-16T01:18:54.618438+0000    21     255     20533     20278   965.258       976    0.242279    0.262501
2021-05-16T01:18:55.618684+0000    22     255     21505     21250   965.554       972    0.196778    0.262309
2021-05-16T01:18:56.619517+0000    23     255     22449     22194   964.582       944    0.226868    0.262661
2021-05-16T01:18:57.619807+0000 Total time run:       23.3848
Total reads made:     22678
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   969.777
Average IOPS:         969
Stddev IOPS:          21.885
Max IOPS:             1017
Min IOPS:             917
Average Latency(s):   0.262126
Max latency(s):       0.529808
Min latency(s):       0.086065

[1;32mlocalhost.localdomain	[2021-05-15T18:18:58,264573756-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 848658


[1;33mlocalhost.localdomain	[2021-05-15T18:18:58,274691545-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:19:22,372794320-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.68k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:19:22,392753594-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:19:31,578022283-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.68k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:19:31,598084145-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:19:40,542409372-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.68k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:19:40,562733088-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:19:49,722035438-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.68k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:19:49,742187074-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:19:58,865848985-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.68k objects, 22 GiB
    usage:   44 GiB used, 256 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:19:58,885805048-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:19:58,902572961-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T18:19:58,908804320-07:00][RUNNING][ROUND 4/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:19:58,918381715-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T18:19:58,936590775-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40545\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.78536\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e026a2ed-e6e5-4323-a564-ad9c1e275b18\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid e026a2ed-e6e5-4323-a564-ad9c1e275b18\nlast_changed 2021-05-15T18:20:27.767650-0700\ncreated 2021-05-15T18:20:27.767650-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40545/0,v1:10.10.1.2:40546/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.78536 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 7c99ac5c-0542-402c-83f9-279b75b53727\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 ec74074d-0253-433b-bedb-ee8c4ca9184a\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 25078864-2793-423c-92ce-11c4fc38280d\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42545\n  w/ user/pass: admin / 0198956c-4db0-4c7d-b492-a3c7e8f9fe88\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 18:20:46 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40545
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.78536
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e026a2ed-e6e5-4323-a564-ad9c1e275b18
setting min_mon_release = octopus
epoch 0
fsid e026a2ed-e6e5-4323-a564-ad9c1e275b18
last_changed 2021-05-15T18:20:27.767650-0700
created 2021-05-15T18:20:27.767650-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40545/0,v1:10.10.1.2:40546/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.78536 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 7c99ac5c-0542-402c-83f9-279b75b53727
0
start osd.0
add osd1 ec74074d-0253-433b-bedb-ee8c4ca9184a
1
start osd.1
add osd2 25078864-2793-423c-92ce-11c4fc38280d
2
start osd.2


restful urls: https://10.10.1.2:42545
  w/ user/pass: admin / 0198956c-4db0-4c7d-b492-a3c7e8f9fe88


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T18:20:00.846-0700 7f64dda911c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:20:00.846-0700 7f64dda911c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:20:00.862-0700 7f34c5bb21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:20:00.862-0700 7f34c5bb21c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40545,v1:10.10.1.2:40546] --print /tmp/ceph_monmap.78536 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.78536 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.78536 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42545 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5575b786c000 @  0x7f9e21b80680 0x7f9e21ba1824 0x7f9e2233c187 0x7f9e22344355 0x7f9e2233c708 0x7f9e2233c877 0x7f9e2233dc24 0x7f9e22355ec1 0x7f9e222c85f3 0x7f9e22329e97 0x7f9e22331b1a 0x7f9e21a34d84 0x7f9e21b50609 0x7f9e21724293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.tNsHlAYi9H 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7c99ac5c-0542-402c-83f9-279b75b53727 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBmc6Bg4l3SBxAA1OuYTOlarBVtJO/cxPFifA== --osd-uuid 7c99ac5c-0542-402c-83f9-279b75b53727 
2021-05-15T18:20:38.787-0700 7f399816ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:20:38.807-0700 7f399816ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:20:38.807-0700 7f399816ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55da812d2000 @  0x7f3998b38680 0x7f3998b59824 0x55da76ec8447 0x55da76ed04b5 0x55da76ec89c8 0x55da76ec8b37 0x55da76ec9ee4 0x55da76c9aca1 0x55da76e72423 0x55da76c8b2a7 0x55da76c9053a 0x7f399868bd84 0x7f3998810609 0x7f3998379293
2021-05-15T18:20:39.179-0700 7f399816ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ec74074d-0253-433b-bedb-ee8c4ca9184a -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x555e8b17e000 @  0x7f2cb251f680 0x7f2cb2540824 0x555e80038447 0x555e800404b5 0x555e800389c8 0x555e80038b37 0x555e80039ee4 0x555e7fe0aca1 0x555e7ffe2423 0x555e7fdfb2a7 0x555e7fe0053a 0x7f2cb2072d84 0x7f2cb21f7609 0x7f2cb1d60293
2021-05-15T18:20:39.779-0700 7f2cb1b56f00 -1 Falling back to public interface
2021-05-15T18:20:40.039-0700 7f2cb1b56f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBnc6BgEV3NHBAAjHrKxCXM2id46OtEAGXx4A== --osd-uuid ec74074d-0253-433b-bedb-ee8c4ca9184a 
2021-05-15T18:20:40.151-0700 7f10fed80f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:20:40.171-0700 7f10fed80f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:20:40.171-0700 7f10fed80f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5653b4634000 @  0x7f10ff749680 0x7f10ff76a824 0x5653aa12d447 0x5653aa1354b5 0x5653aa12d9c8 0x5653aa12db37 0x5653aa12eee4 0x5653a9effca1 0x5653aa0d7423 0x5653a9ef02a7 0x5653a9ef553a 0x7f10ff29cd84 0x7f10ff421609 0x7f10fef8a293
2021-05-15T18:20:40.483-0700 7f10fed80f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 25078864-2793-423c-92ce-11c4fc38280d -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x564ec06fa000 @  0x7f0cacddd680 0x7f0cacdfe824 0x564eb5d11447 0x564eb5d194b5 0x564eb5d119c8 0x564eb5d11b37 0x564eb5d12ee4 0x564eb5ae3ca1 0x564eb5cbb423 0x564eb5ad42a7 0x564eb5ad953a 0x7f0cac930d84 0x7f0cacab5609 0x7f0cac61e293
2021-05-15T18:20:41.171-0700 7f0cac414f00 -1 Falling back to public interface
2021-05-15T18:20:41.423-0700 7f0cac414f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBoc6BgSxplMhAAikNA/fRlpmzLFMy8hxGd3g== --osd-uuid 25078864-2793-423c-92ce-11c4fc38280d 
2021-05-15T18:20:41.499-0700 7f2fd0dfdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:20:41.519-0700 7f2fd0dfdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:20:41.519-0700 7f2fd0dfdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5631844e2000 @  0x7f2fd17c6680 0x7f2fd17e7824 0x563178f5b447 0x563178f634b5 0x563178f5b9c8 0x563178f5bb37 0x563178f5cee4 0x563178d2dca1 0x563178f05423 0x563178d1e2a7 0x563178d2353a 0x7f2fd1319d84 0x7f2fd149e609 0x7f2fd1007293
2021-05-15T18:20:41.835-0700 7f2fd0dfdf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5618c5f2a000 @  0x7f7515f36680 0x7f7515f57824 0x5618ba5cd447 0x5618ba5d54b5 0x5618ba5cd9c8 0x5618ba5cdb37 0x5618ba5ceee4 0x5618ba39fca1 0x5618ba577423 0x5618ba3902a7 0x5618ba39553a 0x7f7515a89d84 0x7f7515c0e609 0x7f7515777293
2021-05-15T18:20:42.463-0700 7f751556df00 -1 Falling back to public interface
2021-05-15T18:20:42.727-0700 7f751556df00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T18:20:46,840536931-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T18:20:46,863443725-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T18:20:46,946300601-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:20:46,952565235-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T18:20:50,934742361-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:20:50,941250696-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T18:20:54,809643966-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:20:54,816024780-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T18:20:58,725090563-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:20:58,731443097-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T18:21:06,552290253-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:21:06,558842261-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T18:21:10,901926568-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:21:10,908581749-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T18:21:15,639367974-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:21:15,645805812-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T18:21:19,976968071-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:21:19,983272192-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:21:24,617605898-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:21:24,625658326-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:21:28,827876477-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:21:28,834340195-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T18:21:33,430425241-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:21:33,436751250-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T18:21:37,392825938-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:21:37,399178216-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.08   90      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   89      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   80      up          osd.2  
                       TOTAL  300 GiB  173 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.08  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T18:21:41,320073542-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:22:05,535595763-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:22:14,581343139-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:22:23,576942222-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:22:32,727042622-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:22:42,133608571-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:22:42,153891593-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:22:51,278555699-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:22:51,298766814-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:23:00,425476277-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:23:00,446098357-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:23:09,455329850-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:23:09,475172276-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:23:18,731515028-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:23:18,751987561-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:23:18,768462499-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:23:18,777942510-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:23:18,800061559-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=862002
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T18:23:18,818146856-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-15T18:23:18,859305545-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:23:18,865529251-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:23:20.534+0000 ffff9519b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:23:21.350+0000 ffff9519b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:23:21.350+0000 ffff9519b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:23:21.372363+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-16T01:23:21.372438+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T01:23:21.581307+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:23:21.581307+0000     0       0         0         0         0         0           -           0
2021-05-16T01:23:22.581542+0000     1     255      1085       830   829.944       830    0.235216    0.236649
2021-05-16T01:23:23.581736+0000     2     255      2182      1927   963.374      1097    0.252853    0.233914
2021-05-16T01:23:24.582029+0000     3     255      3282      3027   1008.81      1100    0.235187    0.234509
2021-05-16T01:23:25.582399+0000     4     255      4454      4199   1049.51      1172    0.192543    0.230771
2021-05-16T01:23:26.583043+0000     5     255      5569      5314   1062.47      1115    0.238969    0.229572
2021-05-16T01:23:27.583517+0000     6     255      6660      6405   1067.14      1091    0.236067    0.230509
2021-05-16T01:23:28.583964+0000     7     255      7761      7506    1071.9      1101    0.224139    0.231039
2021-05-16T01:23:29.584238+0000     8     255      8829      8574   1071.38      1068    0.244367    0.231963
2021-05-16T01:23:30.584786+0000     9     255      9888      9633   1069.94      1059     0.22193    0.233091
2021-05-16T01:23:31.585197+0000    10     255     10959     10704      1070      1071    0.236159    0.233781
2021-05-16T01:23:32.585487+0000    11     255     12042     11787   1071.15      1083    0.239255    0.233867
2021-05-16T01:23:33.585871+0000    12     255     13145     12890   1073.77      1103    0.230882    0.233978
2021-05-16T01:23:34.586144+0000    13     255     14200     13945   1072.31      1055    0.235081    0.234383
2021-05-16T01:23:35.586687+0000    14     255     15211     14956   1067.89      1011    0.306635    0.235555
2021-05-16T01:23:36.586934+0000    15     255     16212     15957   1063.41      1001    0.255607    0.236959
2021-05-16T01:23:37.587495+0000    16     255     17297     17042   1064.72      1085    0.237229    0.237028
2021-05-16T01:23:38.587915+0000    17     255     18353     18098   1064.19      1056    0.230479    0.237273
2021-05-16T01:23:39.588480+0000    18     255     19472     19217    1067.2      1119    0.232367    0.236682
2021-05-16T01:23:40.588965+0000    19     255     20536     20281      1067      1064    0.244272    0.236868
2021-05-16T01:23:41.589167+0000 min lat: 0.010538 max lat: 0.376994 avg lat: 0.236165
2021-05-16T01:23:41.589167+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:23:41.589167+0000    20      28     21586     21558   1077.49      1277   0.0108307    0.236165
2021-05-16T01:23:42.589464+0000 Total time run:         20.0142
Total writes made:      21586
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     1078.54
Stddev Bandwidth:       81.876
Max bandwidth (MB/sec): 1277
Min bandwidth (MB/sec): 830
Average IOPS:           1078
Stddev IOPS:            81.876
Max IOPS:               1277
Min IOPS:               830
Average Latency(s):     0.235912
Stddev Latency(s):      0.0215523
Max latency(s):         0.376994
Min latency(s):         0.010538

[1;32mlocalhost.localdomain	[2021-05-15T18:23:43,256567791-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 862002


[1;33mlocalhost.localdomain	[2021-05-15T18:23:43,266933882-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:24:07,293799199-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.59k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:24:07,314148501-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:24:16,536185992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.59k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:24:16,556675402-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:24:25,653605398-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.59k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:24:25,674178612-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:24:34,826363174-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.59k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:24:34,847138308-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:24:43,921327247-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.59k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:24:43,941772354-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:24:43,958816682-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:24:43,968442096-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:24:43,990957181-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=865484
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T18:24:44,008913934-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-15T18:24:44,049592567-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:24:44,055781706-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f401df9-dba4-4b34-97f4-5b8ee1294416', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f401df9-dba4-4b34-97f4-5b8ee1294416 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.1ifGj9:/tmp/ceph-asok.1ifGj9 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:24:45.725+0000 ffffafa98010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:24:46.521+0000 ffffafa98010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:24:46.521+0000 ffffafa98010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:24:46.544222+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:24:46.544222+0000     0       0         0         0         0         0           -           0
2021-05-16T01:24:47.544437+0000     1     255      1225       970    969.65       970    0.232494    0.222405
2021-05-16T01:24:48.544663+0000     2     255      2288      2033    1016.2      1063    0.231633    0.230745
2021-05-16T01:24:49.544992+0000     3     255      3276      3021   1006.69       988    0.254378    0.239702
2021-05-16T01:24:50.545228+0000     4     255      4296      4041   1009.96      1020    0.253777    0.242598
2021-05-16T01:24:51.545783+0000     5     255      5310      5055   1010.66      1014    0.254882     0.24444
2021-05-16T01:24:52.546028+0000     6     255      6249      5994   998.675       939    0.270884    0.248285
2021-05-16T01:24:53.546833+0000     7     255      7198      6943   991.467       949    0.228414    0.250813
2021-05-16T01:24:54.547055+0000     8     255      8169      7914   988.882       971    0.263943    0.252911
2021-05-16T01:24:55.547281+0000     9     255      9137      8882   986.538       968    0.254861    0.254254
2021-05-16T01:24:56.547517+0000    10     255     10081      9826   982.262       944    0.234377    0.255274
2021-05-16T01:24:57.548071+0000    11     255     11051     10796   981.098       970    0.184584    0.255923
2021-05-16T01:24:58.548309+0000    12     255     12022     11767   980.238       971    0.397054    0.256677
2021-05-16T01:24:59.549071+0000    13     255     12965     12710   977.317       943    0.281009    0.257794
2021-05-16T01:25:00.549301+0000    14     255     13927     13672   976.207       962     0.25156    0.258506
2021-05-16T01:25:01.549618+0000    15     255     14898     14643   975.839       971    0.260425    0.258911
2021-05-16T01:25:02.549830+0000    16     255     15868     15613   975.462       970    0.222784    0.258974
2021-05-16T01:25:03.550573+0000    17     255     16803     16548    973.04       935    0.222184    0.259705
2021-05-16T01:25:04.550829+0000    18     255     17763     17508   972.302       960    0.262648    0.260301
2021-05-16T01:25:05.551072+0000    19     255     18727     18472   971.853       964    0.260604    0.260567
2021-05-16T01:25:06.551621+0000 min lat: 0.114393 max lat: 0.544435 avg lat: 0.260974
2021-05-16T01:25:06.551621+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:25:06.551621+0000    20     255     19668     19413   970.284       941    0.289634    0.260974
2021-05-16T01:25:07.552284+0000    21     255     20651     20396   970.859       983    0.258218    0.261041
2021-05-16T01:25:08.571576+0000    22     116     21586     21470   974.691      1074     0.17715    0.260838
2021-05-16T01:25:09.571828+0000 Total time run:       22.0859
Total reads made:     21586
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   977.367
Average IOPS:         977
Stddev IOPS:          36.9722
Max IOPS:             1074
Min IOPS:             935
Average Latency(s):   0.260189
Max latency(s):       0.544435
Min latency(s):       0.092483

[1;32mlocalhost.localdomain	[2021-05-15T18:25:10,262889164-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 865484


[1;33mlocalhost.localdomain	[2021-05-15T18:25:10,273055600-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:25:34,339890123-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.59k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:25:34,360526961-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:25:43,846795572-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.59k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:25:43,867079866-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:25:53,052018471-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.59k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:25:53,072539114-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:26:02,211311850-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.59k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:26:02,231801123-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:26:11,387474287-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.59k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:26:11,408166424-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:26:11,425125536-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T18:26:11,431466065-07:00][RUNNING][ROUND 5/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:26:11,440807495-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T18:26:11,459207767-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40839\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.79641\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6312076c-2e5b-431e-b531-174e3172459e\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 6312076c-2e5b-431e-b531-174e3172459e\nlast_changed 2021-05-15T18:26:40.147239-0700\ncreated 2021-05-15T18:26:40.147239-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40839/0,v1:10.10.1.2:40840/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.79641 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 2522cf5b-3b25-4773-be31-55d8812f6007\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 c2286f04-e1ad-4151-b81a-42514589bfab\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 287a8a53-ff41-4b8d-85a3-1ff66fb84f35\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42839\n  w/ user/pass: admin / 8685ec0c-db6b-4f9b-9ea7-0f5e987d5430\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 18:26:59 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40839
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.79641
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6312076c-2e5b-431e-b531-174e3172459e
setting min_mon_release = octopus
epoch 0
fsid 6312076c-2e5b-431e-b531-174e3172459e
last_changed 2021-05-15T18:26:40.147239-0700
created 2021-05-15T18:26:40.147239-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40839/0,v1:10.10.1.2:40840/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.79641 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 2522cf5b-3b25-4773-be31-55d8812f6007
0
start osd.0
add osd1 c2286f04-e1ad-4151-b81a-42514589bfab
1
start osd.1
add osd2 287a8a53-ff41-4b8d-85a3-1ff66fb84f35
2
start osd.2


restful urls: https://10.10.1.2:42839
  w/ user/pass: admin / 8685ec0c-db6b-4f9b-9ea7-0f5e987d5430


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T18:26:13.358-0700 7f5f39dc41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:26:13.358-0700 7f5f39dc41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:26:13.374-0700 7f636cc121c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:26:13.374-0700 7f636cc121c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40839,v1:10.10.1.2:40840] --print /tmp/ceph_monmap.79641 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.79641 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.79641 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42839 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x559056c48000 @  0x7f34bd2f1680 0x7f34bd312824 0x7f34bdaad187 0x7f34bdab5355 0x7f34bdaad708 0x7f34bdaad877 0x7f34bdaaec24 0x7f34bdac6ec1 0x7f34bda395f3 0x7f34bda9ae97 0x7f34bdaa2b1a 0x7f34bd1a5d84 0x7f34bd2c1609 0x7f34bce95293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.MEUjegYBZy 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2522cf5b-3b25-4773-be31-55d8812f6007 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDadKBgqCi6JRAAOTONAAzCvdWMLjtTiMqaVw== --osd-uuid 2522cf5b-3b25-4773-be31-55d8812f6007 
2021-05-15T18:26:51.266-0700 7f3b4174df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:26:51.286-0700 7f3b4174df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:26:51.286-0700 7f3b4174df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5600833b4000 @  0x7f3b42116680 0x7f3b42137824 0x56007830f447 0x5600783174b5 0x56007830f9c8 0x56007830fb37 0x560078310ee4 0x5600780e1ca1 0x5600782b9423 0x5600780d22a7 0x5600780d753a 0x7f3b41c69d84 0x7f3b41dee609 0x7f3b41957293
2021-05-15T18:26:51.602-0700 7f3b4174df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c2286f04-e1ad-4151-b81a-42514589bfab -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55753ebd8000 @  0x7f4ff384e680 0x7f4ff386f824 0x557533b73447 0x557533b7b4b5 0x557533b739c8 0x557533b73b37 0x557533b74ee4 0x557533945ca1 0x557533b1d423 0x5575339362a7 0x55753393b53a 0x7f4ff33a1d84 0x7f4ff3526609 0x7f4ff308f293
2021-05-15T18:26:52.214-0700 7f4ff2e85f00 -1 Falling back to public interface
2021-05-15T18:26:52.466-0700 7f4ff2e85f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDbdKBg79GMNhAA9vok5RpmMK9VW1CXF0RH5w== --osd-uuid c2286f04-e1ad-4151-b81a-42514589bfab 
2021-05-15T18:26:52.570-0700 7fe997977f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:26:52.590-0700 7fe997977f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:26:52.590-0700 7fe997977f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56244e498000 @  0x7fe998340680 0x7fe998361824 0x5624436f5447 0x5624436fd4b5 0x5624436f59c8 0x5624436f5b37 0x5624436f6ee4 0x5624434c7ca1 0x56244369f423 0x5624434b82a7 0x5624434bd53a 0x7fe997e93d84 0x7fe998018609 0x7fe997b81293
2021-05-15T18:26:52.902-0700 7fe997977f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 287a8a53-ff41-4b8d-85a3-1ff66fb84f35 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55e916cd0000 @  0x7f6b73113680 0x7f6b73134824 0x55e90b8d6447 0x55e90b8de4b5 0x55e90b8d69c8 0x55e90b8d6b37 0x55e90b8d7ee4 0x55e90b6a8ca1 0x55e90b880423 0x55e90b6992a7 0x55e90b69e53a 0x7f6b72c66d84 0x7f6b72deb609 0x7f6b72954293
2021-05-15T18:26:53.594-0700 7f6b7274af00 -1 Falling back to public interface
2021-05-15T18:26:53.846-0700 7f6b7274af00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDddKBg+dKRERAArV2Fh+QL27XX6Xht92Uydg== --osd-uuid 287a8a53-ff41-4b8d-85a3-1ff66fb84f35 
2021-05-15T18:26:53.966-0700 7f5f33b60f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:26:53.990-0700 7f5f33b60f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:26:53.990-0700 7f5f33b60f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5623afc9e000 @  0x7f5f34529680 0x7f5f3454a824 0x5623a4965447 0x5623a496d4b5 0x5623a49659c8 0x5623a4965b37 0x5623a4966ee4 0x5623a4737ca1 0x5623a490f423 0x5623a47282a7 0x5623a472d53a 0x7f5f3407cd84 0x7f5f34201609 0x7f5f33d6a293
2021-05-15T18:26:54.298-0700 7f5f33b60f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x561f9a106000 @  0x7fe8bff2a680 0x7fe8bff4b824 0x561f8f6fe447 0x561f8f7064b5 0x561f8f6fe9c8 0x561f8f6feb37 0x561f8f6ffee4 0x561f8f4d0ca1 0x561f8f6a8423 0x561f8f4c12a7 0x561f8f4c653a 0x7fe8bfa7dd84 0x7fe8bfc02609 0x7fe8bf76b293
2021-05-15T18:26:54.910-0700 7fe8bf561f00 -1 Falling back to public interface
2021-05-15T18:26:55.174-0700 7fe8bf561f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T18:26:59,312237379-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T18:26:59,335789151-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T18:26:59,419549620-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:26:59,425809060-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T18:27:03,584526008-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:27:03,590845488-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T18:27:07,646191861-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:27:07,654061555-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T18:27:13,560836579-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:27:13,567150043-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T18:27:21,655872574-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:27:21,662235712-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T18:27:26,445784039-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:27:26,452163650-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T18:27:30,869625804-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:27:30,876079198-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T18:27:35,285156109-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:27:35,291587283-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:27:40,125571077-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:27:40,132159658-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:27:44,027923408-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:27:44,034369645-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T18:27:48,383203640-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:27:48,389696969-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 1 pgp_num_target 64 autoscale_mode on last_change 23 lfor 0/0/23 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T18:27:52,349702778-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:27:52,355761904-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  186 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  186 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.00  153      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   69 KiB   0 B   0 B   0 B  100 GiB     0  1.11  152      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.89  143      up          osd.2  
                       TOTAL  300 GiB  188 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.89/1.11  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T18:27:56,047721914-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:28:20,109839257-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:28:29,253667997-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:28:38,525597059-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:28:47,585122300-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:28:56,675611053-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:28:56,696236936-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:29:06,281917758-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:29:06,302746146-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:29:15,492604809-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:29:15,513367500-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:29:24,576390335-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:29:24,596959028-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:29:33,555854921-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:29:33,576531557-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:29:33,593519231-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:29:33,603124524-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:29:33,625848604-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=878899
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T18:29:33,644377505-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-15T18:29:33,685981289-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:29:33,692450225-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:29:35.274+0000 ffffa9e0c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:29:36.078+0000 ffffa9e0c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:29:36.078+0000 ffffa9e0c010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:29:36.103761+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-16T01:29:36.103854+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T01:29:36.326736+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:29:36.326736+0000     0       0         0         0         0         0           -           0
2021-05-16T01:29:37.327033+0000     1     255      1093       838   837.878       838    0.230174    0.235794
2021-05-16T01:29:38.327701+0000     2     255      2186      1931   965.107      1093    0.230463    0.235564
2021-05-16T01:29:39.328115+0000     3     255      3255      3000   999.591      1069     0.22663    0.236122
2021-05-16T01:29:40.328549+0000     4     255      4299      4044   1010.58      1044     0.23564    0.237876
2021-05-16T01:29:41.328774+0000     5     255      5383      5128   1025.21      1084    0.240292    0.238048
2021-05-16T01:29:42.329064+0000     6     255      6495      6240   1039.62      1112    0.220736      0.2371
2021-05-16T01:29:43.329411+0000     7     255      7586      7331   1046.91      1091     0.25013    0.236105
2021-05-16T01:29:44.329720+0000     8     255      8660      8405   1050.25      1074    0.240613    0.236704
2021-05-16T01:29:45.330092+0000     9     255      9833      9578   1063.84      1173    0.247774     0.23368
2021-05-16T01:29:46.330595+0000    10     255     10957     10702    1069.8      1124    0.232633    0.233752
2021-05-16T01:29:47.330928+0000    11     255     11969     11714   1064.52      1012    0.244745    0.235433
2021-05-16T01:29:48.331225+0000    12     255     13065     12810   1067.11      1096    0.232172    0.235259
2021-05-16T01:29:49.331563+0000    13     255     14148     13893   1068.31      1083    0.240882    0.235236
2021-05-16T01:29:50.332091+0000    14     255     15251     14996   1070.74      1103    0.227544    0.235119
2021-05-16T01:29:51.332538+0000    15     255     16379     16124   1074.53      1128    0.203736    0.234816
2021-05-16T01:29:52.333138+0000    16     255     17517     17262   1078.45      1138     0.22056    0.234051
2021-05-16T01:29:53.333638+0000    17     255     18588     18333   1077.98      1071    0.237723    0.234257
2021-05-16T01:29:54.334081+0000    18     255     19674     19419    1078.4      1086    0.216366    0.234338
2021-05-16T01:29:55.334715+0000    19     255     20863     20608   1084.18      1189    0.202303    0.233445
2021-05-16T01:29:56.335293+0000 Total time run:         20.0082
Total writes made:      22072
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     1103.15
Stddev Bandwidth:       72.4984
Max bandwidth (MB/sec): 1189
Min bandwidth (MB/sec): 838
Average IOPS:           1103
Stddev IOPS:            72.4984
Max IOPS:               1189
Min IOPS:               838
Average Latency(s):     0.230665
Stddev Latency(s):      0.020949
Max latency(s):         0.286996
Min latency(s):         0.0104683

[1;32mlocalhost.localdomain	[2021-05-15T18:29:56,983978379-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 878899


[1;33mlocalhost.localdomain	[2021-05-15T18:29:56,994546521-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:30:21,125240002-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.07k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:30:21,146225660-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:30:30,316833643-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.07k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:30:30,337586551-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:30:39,417991169-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.07k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:30:39,438808768-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:30:48,583437277-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.07k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:30:48,604629684-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:30:57,752347320-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.07k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:30:57,773375600-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:30:57,790193399-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:30:57,799715038-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:30:57,822616821-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=882292
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T18:30:57,841489595-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-15T18:30:57,882456549-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:30:57,888666435-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72c89486-754d-4d19-9b81-945d7b9a935e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72c89486-754d-4d19-9b81-945d7b9a935e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CFyeoD:/tmp/ceph-asok.CFyeoD -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:30:59.628+0000 ffffb8dc6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:31:00.416+0000 ffffb8dc6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:31:00.416+0000 ffffb8dc6010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:31:00.441160+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:31:00.441160+0000     0       0         0         0         0         0           -           0
2021-05-16T01:31:01.441825+0000     1     255      1199       944   943.225       944    0.241405    0.228284
2021-05-16T01:31:02.442607+0000     2     256      2192      1936   967.224       992    0.254733    0.242637
2021-05-16T01:31:03.442820+0000     3     255      3180      2925    974.41       989    0.262599     0.24729
2021-05-16T01:31:04.443644+0000     4     255      4178      3923   980.103       998    0.260301    0.249606
2021-05-16T01:31:05.444194+0000     5     255      5173      4918   982.973       995    0.260728    0.251112
2021-05-16T01:31:06.444404+0000     6     255      6149      5894   981.777       976    0.250471    0.253156
2021-05-16T01:31:07.444842+0000     7     255      7120      6865   980.177       971    0.211234    0.253772
2021-05-16T01:31:08.445929+0000     8     255      8089      7834   978.647       969     0.17602    0.254924
2021-05-16T01:31:09.446179+0000     9     255      9012      8757   972.441       923    0.205247    0.257123
2021-05-16T01:31:10.447069+0000    10     255      9995      9740    973.41       983    0.265699    0.257927
2021-05-16T01:31:11.447329+0000    11     255     10979     10724   974.349       984    0.265146    0.258172
2021-05-16T01:31:12.447580+0000    12     255     11954     11699   974.383       975     0.23774    0.258207
2021-05-16T01:31:13.447826+0000    13     255     12899     12644   972.105       945     0.25563    0.259275
2021-05-16T01:31:14.448191+0000    14     255     13873     13618   972.215       974     0.25883    0.259641
2021-05-16T01:31:15.448418+0000    15     255     14837     14582   971.653       964    0.268309    0.259964
2021-05-16T01:31:16.449037+0000    16     255     15784     15529   970.075       947    0.195256    0.260139
2021-05-16T01:31:17.449526+0000    17     255     16735     16480   968.926       951    0.204049    0.260838
2021-05-16T01:31:18.449730+0000    18     255     17680     17425   967.586       945    0.198675    0.261258
2021-05-16T01:31:19.450085+0000    19     255     18624     18369   966.328       944    0.409874    0.261709
2021-05-16T01:31:20.450313+0000 min lat: 0.115262 max lat: 0.531808 avg lat: 0.262408
2021-05-16T01:31:20.450313+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:31:20.450313+0000    20     255     19557     19302   964.651       933    0.202243    0.262408
2021-05-16T01:31:21.450845+0000    21     255     20513     20258   964.215       956    0.199952    0.262599
2021-05-16T01:31:22.451531+0000    22     255     21454     21199    963.13       941    0.208357    0.262969
2021-05-16T01:31:23.452007+0000 Total time run:       22.8127
Total reads made:     22072
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   967.532
Average IOPS:         967
Stddev IOPS:          21.7533
Max IOPS:             998
Min IOPS:             923
Average Latency(s):   0.262436
Max latency(s):       0.531808
Min latency(s):       0.075396

[1;32mlocalhost.localdomain	[2021-05-15T18:31:24,134098039-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 882292


[1;33mlocalhost.localdomain	[2021-05-15T18:31:24,144528787-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:31:48,184842714-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.07k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:31:48,205757964-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:31:57,325327765-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.07k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:31:57,346047322-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:32:06,755986754-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.07k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:32:06,778362787-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:32:15,914290295-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.07k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:32:15,935153800-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:32:24,911339488-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 22.07k objects, 22 GiB
    usage:   43 GiB used, 257 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:32:24,931909859-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:32:24,949008535-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T18:32:24,959299759-07:00][RUNNING][ROUND 1/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:32:24,969125109-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T18:32:24,987677067-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40901\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.80740\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1f73726c-f42e-4ffd-8ed2-fa44ce4afedb\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 1f73726c-f42e-4ffd-8ed2-fa44ce4afedb\nlast_changed 2021-05-15T18:32:54.198492-0700\ncreated 2021-05-15T18:32:54.198492-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40901/0,v1:10.10.1.2:40902/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.80740 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 7c49f8cc-b44f-4948-acfc-c924cd45f3be\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 e8565693-fba5-4475-a907-ffb3d36ca15c\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 01207e1c-fe77-4ca7-a544-d8537355835b\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42901\n  w/ user/pass: admin / 9dea9194-8179-4391-9db4-ee0b857cc729\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 18:33:13 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40901
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.80740
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1f73726c-f42e-4ffd-8ed2-fa44ce4afedb
setting min_mon_release = octopus
epoch 0
fsid 1f73726c-f42e-4ffd-8ed2-fa44ce4afedb
last_changed 2021-05-15T18:32:54.198492-0700
created 2021-05-15T18:32:54.198492-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40901/0,v1:10.10.1.2:40902/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.80740 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 7c49f8cc-b44f-4948-acfc-c924cd45f3be
0
start osd.0
add osd1 e8565693-fba5-4475-a907-ffb3d36ca15c
1
start osd.1
add osd2 01207e1c-fe77-4ca7-a544-d8537355835b
2
start osd.2


restful urls: https://10.10.1.2:42901
  w/ user/pass: admin / 9dea9194-8179-4391-9db4-ee0b857cc729


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T18:32:26.893-0700 7f9e9baae1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:32:26.893-0700 7f9e9baae1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:32:26.909-0700 7f8782bc31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:32:26.909-0700 7f8782bc31c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40901,v1:10.10.1.2:40902] --print /tmp/ceph_monmap.80740 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.80740 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.80740 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42901 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x562394ea0000 @  0x7f758c9c8680 0x7f758c9e9824 0x7f758d184187 0x7f758d18c355 0x7f758d184708 0x7f758d184877 0x7f758d185c24 0x7f758d19dec1 0x7f758d1105f3 0x7f758d171e97 0x7f758d179b1a 0x7f758c87cd84 0x7f758c998609 0x7f758c56c293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.0KAvOURkO0 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7c49f8cc-b44f-4948-acfc-c924cd45f3be -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBQdqBgEl4rJhAAfL3JL4t3sCIGCCrIjLJOQA== --osd-uuid 7c49f8cc-b44f-4948-acfc-c924cd45f3be 
2021-05-15T18:33:05.266-0700 7f32098c6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:33:05.290-0700 7f32098c6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:33:05.290-0700 7f32098c6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56241b76c000 @  0x7f320a28f680 0x7f320a2b0824 0x562410185447 0x56241018d4b5 0x5624101859c8 0x562410185b37 0x562410186ee4 0x56240ff57ca1 0x56241012f423 0x56240ff482a7 0x56240ff4d53a 0x7f3209de2d84 0x7f3209f67609 0x7f3209ad0293
2021-05-15T18:33:05.590-0700 7f32098c6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e8565693-fba5-4475-a907-ffb3d36ca15c -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x564b0cd3e000 @  0x7fcf4cee4680 0x7fcf4cf05824 0x564b01734447 0x564b0173c4b5 0x564b017349c8 0x564b01734b37 0x564b01735ee4 0x564b01506ca1 0x564b016de423 0x564b014f72a7 0x564b014fc53a 0x7fcf4ca37d84 0x7fcf4cbbc609 0x7fcf4c725293
2021-05-15T18:33:06.222-0700 7fcf4c51bf00 -1 Falling back to public interface
2021-05-15T18:33:06.486-0700 7fcf4c51bf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBRdqBgTSNrNRAAkN+v/NMsZ/7CB6GSF83C0Q== --osd-uuid e8565693-fba5-4475-a907-ffb3d36ca15c 
2021-05-15T18:33:06.518-0700 7fceeac3ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:33:06.538-0700 7fceeac3ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:33:06.538-0700 7fceeac3ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56373030a000 @  0x7fceeb608680 0x7fceeb629824 0x5637251b2447 0x5637251ba4b5 0x5637251b29c8 0x5637251b2b37 0x5637251b3ee4 0x563724f84ca1 0x56372515c423 0x563724f752a7 0x563724f7a53a 0x7fceeb15bd84 0x7fceeb2e0609 0x7fceeae49293
2021-05-15T18:33:06.902-0700 7fceeac3ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 01207e1c-fe77-4ca7-a544-d8537355835b -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x557f92d66000 @  0x7f17030fd680 0x7f170311e824 0x557f87c53447 0x557f87c5b4b5 0x557f87c539c8 0x557f87c53b37 0x557f87c54ee4 0x557f87a25ca1 0x557f87bfd423 0x557f87a162a7 0x557f87a1b53a 0x7f1702c50d84 0x7f1702dd5609 0x7f170293e293
2021-05-15T18:33:07.518-0700 7f1702734f00 -1 Falling back to public interface
2021-05-15T18:33:07.774-0700 7f1702734f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBTdqBg9lvyDBAAsRmdtNo04X/V6Sl/I8Ud0g== --osd-uuid 01207e1c-fe77-4ca7-a544-d8537355835b 
2021-05-15T18:33:07.870-0700 7f060a5a2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:33:07.890-0700 7f060a5a2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:33:07.890-0700 7f060a5a2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55cae0c58000 @  0x7f060af6b680 0x7f060af8c824 0x55cad54fe447 0x55cad55064b5 0x55cad54fe9c8 0x55cad54feb37 0x55cad54ffee4 0x55cad52d0ca1 0x55cad54a8423 0x55cad52c12a7 0x55cad52c653a 0x7f060aabed84 0x7f060ac43609 0x7f060a7ac293
2021-05-15T18:33:08.198-0700 7f060a5a2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x557829742000 @  0x7f349c799680 0x7f349c7ba824 0x55781d9ae447 0x55781d9b64b5 0x55781d9ae9c8 0x55781d9aeb37 0x55781d9afee4 0x55781d780ca1 0x55781d958423 0x55781d7712a7 0x55781d77653a 0x7f349c2ecd84 0x7f349c471609 0x7f349bfda293
2021-05-15T18:33:08.826-0700 7f349bdd0f00 -1 Falling back to public interface
2021-05-15T18:33:09.090-0700 7f349bdd0f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T18:33:13,184010556-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T18:33:13,207381213-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T18:33:13,290195301-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:33:13,296511104-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T18:33:17,545128168-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:33:17,552399169-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T18:33:21,429954105-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:33:21,436440127-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T18:33:25,320664606-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:33:25,326890664-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T18:33:33,021630021-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:33:33,028139072-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T18:33:38,222704942-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:33:38,229076825-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T18:33:42,467629205-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:33:42,474335690-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T18:33:47,190308967-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:33:47,196593025-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:33:51,963290392-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:33:51,969820837-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:33:56,342145179-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:33:56,348412433-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T18:34:00,969383497-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:34:00,975848351-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T18:34:04,840949092-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:34:04,852907714-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   66 KiB   0 B   0 B   0 B  100 GiB     0  1.12   88      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.87   91      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  1.01   80      up          osd.2  
                       TOTAL  300 GiB  177 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.87/1.12  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T18:34:08,531515812-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:34:32,557672838-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:34:41,711938989-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:34:50,799101769-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:34:59,805882919-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:35:08,848911683-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:35:08,871581704-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:35:18,024832546-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:35:18,045381087-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:35:27,078030116-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:35:27,099270245-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:35:36,102710380-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:35:36,123406426-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:35:45,382367273-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:35:45,402976544-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:35:45,420022155-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:35:45,430134983-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:35:45,452735092-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=895741
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T18:35:45,471654926-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-15T18:35:45,512978096-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:35:45,519334646-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:35:47.093+0000 ffffb987d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:35:47.909+0000 ffffb987d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:35:47.909+0000 ffffb987d010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:35:47.935178+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-16T01:35:47.935291+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T01:35:48.881665+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:35:48.881665+0000     0       0         0         0         0         0           -           0
2021-05-16T01:35:49.881968+0000     1     221       221         0         0         0           -           0
2021-05-16T01:35:50.882305+0000     2     255       453       198   395.899       396     1.09519     1.13601
2021-05-16T01:35:51.882742+0000     3     255       684       429    571.82       924     1.11144     1.11917
2021-05-16T01:35:52.883237+0000     4     255       907       652   651.765       892     1.14242     1.11958
2021-05-16T01:35:53.883699+0000     5     255      1126       871   696.535       876     1.17234     1.13171
2021-05-16T01:35:54.884006+0000     6     255      1354      1099   732.397       912     1.11867     1.13273
2021-05-16T01:35:55.884315+0000     7     255      1578      1323   755.728       896     1.15344     1.13281
2021-05-16T01:35:56.884669+0000     8     255      1796      1541   770.223       872     1.18083     1.13654
2021-05-16T01:35:57.885411+0000     9     255      2019      1764   783.685       892      1.1469     1.14153
2021-05-16T01:35:58.885885+0000    10     255      2250      1995   797.674       924      1.1094     1.13989
2021-05-16T01:35:59.886202+0000    11     255      2472      2217   805.859       888     1.14634     1.13859
2021-05-16T01:36:00.886577+0000    12     255      2686      2431   810.011       856     1.17452     1.14185
2021-05-16T01:36:01.886856+0000    13     255      2912      2657    817.22       904     1.13425     1.14294
2021-05-16T01:36:02.887243+0000    14     255      3140      2885   823.965       912     1.11935     1.14216
2021-05-16T01:36:03.887632+0000    15     255      3363      3108   828.478       892     1.14438     1.14072
2021-05-16T01:36:04.888142+0000    16     255      3584      3329    831.92       884     1.16704     1.14256
2021-05-16T01:36:05.888652+0000    17     255      3802      3547   834.252       872     1.16731     1.14436
2021-05-16T01:36:06.889039+0000    18     255      4029      3774   838.329       908     1.13224     1.14462
2021-05-16T01:36:07.889584+0000    19     255      4248      3993   840.287       876      1.1583     1.14461
2021-05-16T01:36:08.890232+0000 min lat: 0.0386071 max lat: 1.20292 avg lat: 1.11477
2021-05-16T01:36:08.890232+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:36:08.890232+0000    20       6      4469      4463   892.224      1880   0.0395635     1.11477
2021-05-16T01:36:09.890655+0000 Total time run:         20.0298
Total writes made:      4469
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     892.468
Stddev Bandwidth:       325.771
Max bandwidth (MB/sec): 1880
Min bandwidth (MB/sec): 0
Average IOPS:           223
Stddev IOPS:            81.4427
Max IOPS:               470
Min IOPS:               0
Average Latency(s):     1.11332
Stddev Latency(s):      0.155593
Max latency(s):         1.20292
Min latency(s):         0.0306453

[1;32mlocalhost.localdomain	[2021-05-15T18:36:10,670182636-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 895741


[1;33mlocalhost.localdomain	[2021-05-15T18:36:10,681719610-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:36:34,805192713-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.47k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:36:34,826072720-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:36:46,136655878-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.47k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:36:46,157745681-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:36:55,404745821-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.47k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:36:55,425179367-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:37:04,518632947-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.47k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:37:04,539410022-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:37:13,591506970-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.47k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:37:13,612055780-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:37:13,629721986-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:37:13,639662029-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:37:13,663296729-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=899338
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T18:37:13,682162253-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-15T18:37:13,723509861-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:37:13,729980070-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '46c7cc3c-d4c8-48cb-85d7-a90a868e4589', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 46c7cc3c-d4c8-48cb-85d7-a90a868e4589 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.HRsi1W:/tmp/ceph-asok.HRsi1W -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:37:15.188+0000 ffff8ee16010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:37:16.016+0000 ffff8ee16010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:37:16.020+0000 ffff8ee16010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:37:16.045836+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:37:16.045836+0000     0       0         0         0         0         0           -           0
2021-05-16T01:37:17.046846+0000     1     255       350        95   379.551       380    0.802216    0.732422
2021-05-16T01:37:18.048475+0000     2     255       592       337   673.054       968     1.05326    0.901272
2021-05-16T01:37:19.049021+0000     3     255       826       571   760.482       936     1.08965    0.970121
2021-05-16T01:37:20.049383+0000     4     255      1049       794   793.262       892     1.13596     1.00944
2021-05-16T01:37:21.049700+0000     5     255      1275      1020   815.342       904     1.12874     1.04124
2021-05-16T01:37:22.051366+0000     6     255      1494      1239   825.216       876     1.15991     1.05942
2021-05-16T01:37:23.052567+0000     7     255      1717      1462   834.605       892     1.15026     1.07378
2021-05-16T01:37:24.052905+0000     8     255      1951      1696   847.233       936     1.09486     1.08043
2021-05-16T01:37:25.054826+0000     9     255      2188      1933   858.237       948     1.08464     1.08079
2021-05-16T01:37:26.055129+0000    10     255      2424      2169    866.78       944     1.08738     1.08088
2021-05-16T01:37:27.055507+0000    11     255      2656      2401    872.31       928     1.09814     1.08284
2021-05-16T01:37:28.057382+0000    12     255      2880      2625   874.146       896     1.12946     1.08484
2021-05-16T01:37:29.058739+0000    13     255      3104      2849   875.735       896     1.15766     1.08953
2021-05-16T01:37:30.059165+0000    14     255      3336      3081   879.438       928     1.10988     1.09195
2021-05-16T01:37:31.059936+0000    15     255      3557      3302   879.696       884     1.15517     1.09474
2021-05-16T01:37:32.060170+0000    16     255      3791      3536   883.199       936     1.09567       1.097
2021-05-16T01:37:33.061236+0000    17     255      4021      3766   885.307       920     1.11086     1.09684
2021-05-16T01:37:34.061553+0000    18     255      4224      3969   881.222       812     1.22731     1.10066
2021-05-16T01:37:35.063471+0000    19     255      4430      4175   878.124       824     1.25239     1.10803
2021-05-16T01:37:36.063737+0000 Total time run:       19.6059
Total reads made:     4469
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   911.768
Average IOPS:         227
Stddev IOPS:          31.7994
Max IOPS:             242
Min IOPS:             95
Average Latency(s):   1.09383
Max latency(s):       1.27003
Min latency(s):       0.406543

[1;32mlocalhost.localdomain	[2021-05-15T18:37:36,849094410-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 899338


[1;33mlocalhost.localdomain	[2021-05-15T18:37:36,860274579-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:38:00,987881757-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.47k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:38:01,008676549-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:38:10,136955827-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.47k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:38:10,157782789-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:38:19,336019044-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.47k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:38:19,356819405-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:38:28,312131819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.47k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:38:28,332679039-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:38:37,755567434-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.47k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:38:37,776320893-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:38:37,793547415-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T18:38:37,800103291-07:00][RUNNING][ROUND 2/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:38:37,809923978-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T18:38:37,829068766-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40350\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.81876\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a58cbb58-344c-4039-a2ba-e94cc1d997ab\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid a58cbb58-344c-4039-a2ba-e94cc1d997ab\nlast_changed 2021-05-15T18:39:06.615985-0700\ncreated 2021-05-15T18:39:06.615985-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40350/0,v1:10.10.1.2:40351/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.81876 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 f5b15ed4-6f33-4979-8dff-87b41962134d\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 b27436c7-04d8-4475-9bd1-23add9f6a655\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 f61fb16d-13d2-4231-9613-c4c891b71336\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42350\n  w/ user/pass: admin / dae44e47-7c21-493d-9403-ecc8ad29ba9f\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 18:39:25 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40350
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.81876
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a58cbb58-344c-4039-a2ba-e94cc1d997ab
setting min_mon_release = octopus
epoch 0
fsid a58cbb58-344c-4039-a2ba-e94cc1d997ab
last_changed 2021-05-15T18:39:06.615985-0700
created 2021-05-15T18:39:06.615985-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40350/0,v1:10.10.1.2:40351/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.81876 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 f5b15ed4-6f33-4979-8dff-87b41962134d
0
start osd.0
add osd1 b27436c7-04d8-4475-9bd1-23add9f6a655
1
start osd.1
add osd2 f61fb16d-13d2-4231-9613-c4c891b71336
2
start osd.2


restful urls: https://10.10.1.2:42350
  w/ user/pass: admin / dae44e47-7c21-493d-9403-ecc8ad29ba9f


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T18:38:39.745-0700 7f79006711c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:38:39.745-0700 7f79006711c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:38:39.761-0700 7fa068e3a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:38:39.761-0700 7fa068e3a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40350,v1:10.10.1.2:40351] --print /tmp/ceph_monmap.81876 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.81876 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.81876 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42350 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x562d8ba3a000 @  0x7f29ca8bc680 0x7f29ca8dd824 0x7f29cb078187 0x7f29cb080355 0x7f29cb078708 0x7f29cb078877 0x7f29cb079c24 0x7f29cb091ec1 0x7f29cb0045f3 0x7f29cb065e97 0x7f29cb06db1a 0x7f29ca770d84 0x7f29ca88c609 0x7f29ca460293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.JmyGSIvHAD 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f5b15ed4-6f33-4979-8dff-87b41962134d -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDFd6BgYc/YBRAAANCIPc4IpbShbNMA9Z2wKA== --osd-uuid f5b15ed4-6f33-4979-8dff-87b41962134d 
2021-05-15T18:39:17.729-0700 7f536927df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:39:17.749-0700 7f536927df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:39:17.749-0700 7f536927df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x557afd184000 @  0x7f5369c46680 0x7f5369c67824 0x557af2cdb447 0x557af2ce34b5 0x557af2cdb9c8 0x557af2cdbb37 0x557af2cdcee4 0x557af2aadca1 0x557af2c85423 0x557af2a9e2a7 0x557af2aa353a 0x7f5369799d84 0x7f536991e609 0x7f5369487293
2021-05-15T18:39:18.061-0700 7f536927df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b27436c7-04d8-4475-9bd1-23add9f6a655 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x557719bf8000 @  0x7f81b5cfc680 0x7f81b5d1d824 0x55770ec42447 0x55770ec4a4b5 0x55770ec429c8 0x55770ec42b37 0x55770ec43ee4 0x55770ea14ca1 0x55770ebec423 0x55770ea052a7 0x55770ea0a53a 0x7f81b584fd84 0x7f81b59d4609 0x7f81b553d293
2021-05-15T18:39:18.685-0700 7f81b5333f00 -1 Falling back to public interface
2021-05-15T18:39:18.941-0700 7f81b5333f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDGd6BgYO4JFxAAPFMe1vADQaff4y7ychpmpw== --osd-uuid b27436c7-04d8-4475-9bd1-23add9f6a655 
2021-05-15T18:39:19.041-0700 7fd4e3e82f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:39:19.061-0700 7fd4e3e82f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:39:19.061-0700 7fd4e3e82f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55e5da7ce000 @  0x7fd4e484b680 0x7fd4e486c824 0x55e5d0395447 0x55e5d039d4b5 0x55e5d03959c8 0x55e5d0395b37 0x55e5d0396ee4 0x55e5d0167ca1 0x55e5d033f423 0x55e5d01582a7 0x55e5d015d53a 0x7fd4e439ed84 0x7fd4e4523609 0x7fd4e408c293
2021-05-15T18:39:19.365-0700 7fd4e3e82f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f61fb16d-13d2-4231-9613-c4c891b71336 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55b417fe8000 @  0x7fbd20fd0680 0x7fbd20ff1824 0x55b40cba1447 0x55b40cba94b5 0x55b40cba19c8 0x55b40cba1b37 0x55b40cba2ee4 0x55b40c973ca1 0x55b40cb4b423 0x55b40c9642a7 0x55b40c96953a 0x7fbd20b23d84 0x7fbd20ca8609 0x7fbd20811293
2021-05-15T18:39:19.997-0700 7fbd20607f00 -1 Falling back to public interface
2021-05-15T18:39:20.253-0700 7fbd20607f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDHd6BgIoMmKRAA6UMUvViiT0tlA7M1Vw5zbw== --osd-uuid f61fb16d-13d2-4231-9613-c4c891b71336 
2021-05-15T18:39:20.349-0700 7fdb7c35ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:39:20.369-0700 7fdb7c35ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:39:20.369-0700 7fdb7c35ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x556b9cdac000 @  0x7fdb7cd27680 0x7fdb7cd48824 0x556b912f4447 0x556b912fc4b5 0x556b912f49c8 0x556b912f4b37 0x556b912f5ee4 0x556b910c6ca1 0x556b9129e423 0x556b910b72a7 0x556b910bc53a 0x7fdb7c87ad84 0x7fdb7c9ff609 0x7fdb7c568293
2021-05-15T18:39:20.677-0700 7fdb7c35ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x56318e652000 @  0x7f9bcc0e6680 0x7f9bcc107824 0x56318432f447 0x5631843374b5 0x56318432f9c8 0x56318432fb37 0x563184330ee4 0x563184101ca1 0x5631842d9423 0x5631840f22a7 0x5631840f753a 0x7f9bcbc39d84 0x7f9bcbdbe609 0x7f9bcb927293
2021-05-15T18:39:21.305-0700 7f9bcb71df00 -1 Falling back to public interface
2021-05-15T18:39:21.573-0700 7f9bcb71df00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T18:39:25,704258139-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T18:39:25,727687966-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T18:39:25,808373511-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:39:25,814820616-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T18:39:29,847343266-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:39:29,853685637-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T18:39:33,825401939-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:39:33,831870632-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T18:39:37,835397514-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:39:37,842716495-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T18:39:45,639854482-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:39:45,646176494-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T18:39:49,683902576-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:39:49,690329921-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T18:39:54,361318964-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:39:54,367372999-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T18:39:58,927841661-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:39:58,934470085-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:40:03,399642563-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:40:03,405975916-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:40:07,658605202-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:40:07,665054195-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T18:40:12,505584451-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:40:12,512029213-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T18:40:16,295187726-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:40:16,301290458-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.08   89      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   90      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   80      up          osd.2  
                       TOTAL  300 GiB  173 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.08  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T18:40:20,084644231-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:40:44,109733050-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:40:53,217303736-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:02,450057738-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:11,513938708-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:20,757752068-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:20,778611840-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:29,811536168-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:29,832258500-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:39,089569176-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:39,110460001-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:48,329868590-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:48,350560584-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:57,431043156-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:57,452175055-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:57,469686511-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:41:57,479792008-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:41:57,503579203-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=912588
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T18:41:57,522945280-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-15T18:41:57,565192065-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:41:57,571691552-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:41:59.061+0000 ffff9a95c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:41:59.869+0000 ffff9a95c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:41:59.869+0000 ffff9a95c010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:41:59.895030+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-16T01:41:59.895117+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T01:42:00.876027+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:42:00.876027+0000     0       0         0         0         0         0           -           0
2021-05-16T01:42:01.876244+0000     1     220       220         0         0         0           -           0
2021-05-16T01:42:02.876528+0000     2     255       432       177   353.939       354     1.20447     1.19069
2021-05-16T01:42:03.877058+0000     3     255       644       389   518.515       848     1.20255     1.20408
2021-05-16T01:42:04.877481+0000     4     255       854       599   598.805       840     1.21804      1.2035
2021-05-16T01:42:05.877844+0000     5     255      1069       814   650.984       860     1.21751     1.20393
2021-05-16T01:42:06.878132+0000     6     255      1279      1024   682.445       840     1.20705     1.20458
2021-05-16T01:42:07.878425+0000     7     255      1492      1237   706.631       852     1.20236     1.20691
2021-05-16T01:42:08.878849+0000     8     255      1704      1449   724.259       848     1.19947     1.20613
2021-05-16T01:42:09.879189+0000     9     255      1917      1662    738.42       852     1.19672     1.20644
2021-05-16T01:42:10.879430+0000    10     255      2125      1870   747.757       832      1.2063     1.20621
2021-05-16T01:42:11.879727+0000    11     255      2342      2087   758.665       868     1.17893     1.20603
2021-05-16T01:42:12.880063+0000    12     255      2570      2315   771.417       912     1.13618     1.20198
2021-05-16T01:42:13.880486+0000    13     255      2783      2528   777.589       852     1.19368     1.19813
2021-05-16T01:42:14.880795+0000    14     255      2991      2736   781.457       832      1.2241     1.19908
2021-05-16T01:42:15.881027+0000    15     255      3208      2953   787.212       868     1.19161     1.19983
2021-05-16T01:42:16.881285+0000    16     255      3423      3168   791.747       860     1.18243      1.1987
2021-05-16T01:42:17.881604+0000    17     255      3635      3380   795.041       848     1.20426     1.19874
2021-05-16T01:42:18.881989+0000    18     255      3848      3593   798.187       852     1.19822     1.19945
2021-05-16T01:42:19.882385+0000    19     255      4068      3813   802.475       880     1.16078     1.19857
2021-05-16T01:42:20.882774+0000 min lat: 0.0447756 max lat: 1.24148 avg lat: 1.16291
2021-05-16T01:42:20.882774+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:42:20.882774+0000    20       6      4283      4277   855.118      1856   0.0470404     1.16291
2021-05-16T01:42:21.883084+0000 Total time run:         20.0248
Total writes made:      4283
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     855.539
Stddev Bandwidth:       323.142
Max bandwidth (MB/sec): 1856
Min bandwidth (MB/sec): 0
Average IOPS:           213
Stddev IOPS:            80.825
Max IOPS:               464
Min IOPS:               0
Average Latency(s):     1.16133
Stddev Latency(s):      0.165129
Max latency(s):         1.24148
Min latency(s):         0.0272982

[1;32mlocalhost.localdomain	[2021-05-15T18:42:22,675648061-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 912588


[1;33mlocalhost.localdomain	[2021-05-15T18:42:22,686287199-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:42:46,872987698-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:42:46,894511924-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:42:56,003862351-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:42:56,025660403-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:43:05,178491838-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:43:05,200168563-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:43:14,385322303-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:43:14,406987685-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:43:23,581888906-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:43:23,603007233-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:43:23,620393469-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:43:23,630555251-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:43:23,654237026-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=916103
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T18:43:23,673097642-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-15T18:43:23,714115034-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:43:23,720481235-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2f7c646-e0e3-49f0-8ec4-f3d87e14f5a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0ugzob:/tmp/ceph-asok.0ugzob -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:43:25.416+0000 ffffa4647010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:43:26.212+0000 ffffa4647010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:43:26.216+0000 ffffa4647010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:43:26.241217+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:43:26.241217+0000     0       0         0         0         0         0           -           0
2021-05-16T01:43:27.241883+0000     1     255       347        92   367.692       368    0.813033    0.739167
2021-05-16T01:43:28.243188+0000     2     255       583       328   655.298       944     1.08494    0.914266
2021-05-16T01:43:29.244577+0000     3     255       811       556   740.462       912     1.12153    0.989631
2021-05-16T01:43:30.245859+0000     4     255      1044       789   788.052       932     1.09834     1.02613
2021-05-16T01:43:31.246090+0000     5     255      1285      1030    823.17       964     1.06146     1.03813
2021-05-16T01:43:32.246359+0000     6     255      1513      1258   837.925       912     1.11313     1.04602
2021-05-16T01:43:33.246642+0000     7     255      1733      1478   843.897       880     1.15661     1.06068
2021-05-16T01:43:34.248310+0000     8     255      1954      1699   848.729       884     1.15834      1.0746
2021-05-16T01:43:35.248915+0000     9     255      2179      1924   854.364       900     1.13149     1.08291
2021-05-16T01:43:36.249939+0000    10     255      2394      2139    854.84       860     1.17783     1.09016
2021-05-16T01:43:37.250203+0000    11     255      2615      2360   857.468       884     1.15258      1.0981
2021-05-16T01:43:38.250585+0000    12     255      2835      2580   859.317       880      1.1563     1.10339
2021-05-16T01:43:39.250851+0000    13     255      3051      2796   859.659       864     1.17601     1.10793
2021-05-16T01:43:40.252904+0000    14     255      3275      3020   862.127       896     1.16092     1.11251
2021-05-16T01:43:41.253236+0000    15     255      3495      3240   863.298       880      1.1497      1.1152
2021-05-16T01:43:42.254216+0000    16     255      3718      3463   865.038       892     1.15339     1.11762
2021-05-16T01:43:43.254844+0000    17     255      3937      3682    865.65       876     1.16712     1.12024
2021-05-16T01:43:44.256315+0000    18     255      4151      3896   865.044       856     1.18167     1.12344
2021-05-16T01:43:45.256749+0000    19      19      4283      4264   896.943      1472    0.460661      1.1107
2021-05-16T01:43:46.257024+0000 Total time run:       19.0424
Total reads made:     4283
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   899.679
Average IOPS:         224
Stddev IOPS:          46.5717
Max IOPS:             368
Min IOPS:             92
Average Latency(s):   1.10767
Max latency(s):       1.22949
Min latency(s):       0.393532

[1;32mlocalhost.localdomain	[2021-05-15T18:43:47,020114387-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 916103


[1;33mlocalhost.localdomain	[2021-05-15T18:43:47,031052086-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:44:11,096111215-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:44:11,117707669-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:44:20,324218992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:44:20,346030027-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:44:29,454201152-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:44:29,475618278-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:44:38,541161157-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:44:38,562706495-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:44:47,647785609-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:44:47,669563606-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:44:47,687312313-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T18:44:47,694619039-07:00][RUNNING][ROUND 3/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:44:47,704619884-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T18:44:47,722813568-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40569\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.82982\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 941be755-0093-47dd-85a7-7e9f02a6b118\nsetting min_mon_release = octopus\nepoch 0\nfsid 941be755-0093-47dd-85a7-7e9f02a6b118\nlast_changed 2021-05-15T18:45:16.840933-0700\ncreated 2021-05-15T18:45:16.840933-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40569/0,v1:10.10.1.2:40570/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.82982 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 24a77983-1ce5-4fa1-9467-74932f973bf0\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 9b502956-8213-40b9-8629-f1d2c90fd3aa\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 0608013d-c482-43b0-9e3f-d5e523c299e2\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42569\n  w/ user/pass: admin / 9f660374-2c80-47e5-9156-1d0f4948a601\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 18:45:35 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40569
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.82982
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 941be755-0093-47dd-85a7-7e9f02a6b118
setting min_mon_release = octopus
epoch 0
fsid 941be755-0093-47dd-85a7-7e9f02a6b118
last_changed 2021-05-15T18:45:16.840933-0700
created 2021-05-15T18:45:16.840933-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40569/0,v1:10.10.1.2:40570/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.82982 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 24a77983-1ce5-4fa1-9467-74932f973bf0
0
start osd.0
add osd1 9b502956-8213-40b9-8629-f1d2c90fd3aa
1
start osd.1
add osd2 0608013d-c482-43b0-9e3f-d5e523c299e2
2
start osd.2


restful urls: https://10.10.1.2:42569
  w/ user/pass: admin / 9f660374-2c80-47e5-9156-1d0f4948a601


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T18:44:49.624-0700 7f10380eb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:44:49.624-0700 7f10380eb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:44:49.640-0700 7fecf339f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:44:49.640-0700 7fecf339f1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40569,v1:10.10.1.2:40570] --print /tmp/ceph_monmap.82982 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.82982 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.82982 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42569 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55fcbe154000 @  0x7f740c76e680 0x7f740c78f824 0x7f740cf2a187 0x7f740cf32355 0x7f740cf2a708 0x7f740cf2a877 0x7f740cf2bc24 0x7f740cf43ec1 0x7f740ceb65f3 0x7f740cf17e97 0x7f740cf1fb1a 0x7f740c622d84 0x7f740c73e609 0x7f740c312293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.DSh11XaZw6 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 24a77983-1ce5-4fa1-9467-74932f973bf0 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA3eaBgKdrPGBAAxtWCe4YVq+B53QiBtlg4Qg== --osd-uuid 24a77983-1ce5-4fa1-9467-74932f973bf0 
2021-05-15T18:45:28.040-0700 7f6daf4bbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:45:28.060-0700 7f6daf4bbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:45:28.060-0700 7f6daf4bbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5620bd8c0000 @  0x7f6dafe84680 0x7f6dafea5824 0x5620b2983447 0x5620b298b4b5 0x5620b29839c8 0x5620b2983b37 0x5620b2984ee4 0x5620b2755ca1 0x5620b292d423 0x5620b27462a7 0x5620b274b53a 0x7f6daf9d7d84 0x7f6dafb5c609 0x7f6daf6c5293
2021-05-15T18:45:28.356-0700 7f6daf4bbf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9b502956-8213-40b9-8629-f1d2c90fd3aa -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5598ec956000 @  0x7f6623cf7680 0x7f6623d18824 0x5598e1303447 0x5598e130b4b5 0x5598e13039c8 0x5598e1303b37 0x5598e1304ee4 0x5598e10d5ca1 0x5598e12ad423 0x5598e10c62a7 0x5598e10cb53a 0x7f662384ad84 0x7f66239cf609 0x7f6623538293
2021-05-15T18:45:28.996-0700 7f662332ef00 -1 Falling back to public interface
2021-05-15T18:45:29.252-0700 7f662332ef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA4eaBgHk6tKBAAlt9Btgm1cscEoZFJhaM4RA== --osd-uuid 9b502956-8213-40b9-8629-f1d2c90fd3aa 
2021-05-15T18:45:29.324-0700 7f6be0333f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:45:29.344-0700 7f6be0333f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:45:29.344-0700 7f6be0333f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x560202d38000 @  0x7f6be0cfc680 0x7f6be0d1d824 0x5601f7443447 0x5601f744b4b5 0x5601f74439c8 0x5601f7443b37 0x5601f7444ee4 0x5601f7215ca1 0x5601f73ed423 0x5601f72062a7 0x5601f720b53a 0x7f6be084fd84 0x7f6be09d4609 0x7f6be053d293
2021-05-15T18:45:29.712-0700 7f6be0333f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0608013d-c482-43b0-9e3f-d5e523c299e2 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55e1f7272000 @  0x7fe75ad8c680 0x7fe75adad824 0x55e1eb836447 0x55e1eb83e4b5 0x55e1eb8369c8 0x55e1eb836b37 0x55e1eb837ee4 0x55e1eb608ca1 0x55e1eb7e0423 0x55e1eb5f92a7 0x55e1eb5fe53a 0x7fe75a8dfd84 0x7fe75aa64609 0x7fe75a5cd293
2021-05-15T18:45:30.384-0700 7fe75a3c3f00 -1 Falling back to public interface
2021-05-15T18:45:30.644-0700 7fe75a3c3f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA6eaBgfQi/BBAAcdm5Gz0GBJSpXyV5mP2nmg== --osd-uuid 0608013d-c482-43b0-9e3f-d5e523c299e2 
2021-05-15T18:45:30.736-0700 7fbed4549f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:45:30.760-0700 7fbed4549f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:45:30.760-0700 7fbed4549f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55ac2553a000 @  0x7fbed4f12680 0x7fbed4f33824 0x55ac19f0c447 0x55ac19f144b5 0x55ac19f0c9c8 0x55ac19f0cb37 0x55ac19f0dee4 0x55ac19cdeca1 0x55ac19eb6423 0x55ac19ccf2a7 0x55ac19cd453a 0x7fbed4a65d84 0x7fbed4bea609 0x7fbed4753293
2021-05-15T18:45:31.072-0700 7fbed4549f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x562bfef3a000 @  0x7fec58201680 0x7fec58222824 0x562bf3f0d447 0x562bf3f154b5 0x562bf3f0d9c8 0x562bf3f0db37 0x562bf3f0eee4 0x562bf3cdfca1 0x562bf3eb7423 0x562bf3cd02a7 0x562bf3cd553a 0x7fec57d54d84 0x7fec57ed9609 0x7fec57a42293
2021-05-15T18:45:31.752-0700 7fec57838f00 -1 Falling back to public interface
2021-05-15T18:45:32.028-0700 7fec57838f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T18:45:36,037336004-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T18:45:36,060654117-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T18:45:36,143486015-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:45:36,149952685-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T18:45:40,208201039-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:45:40,215383871-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T18:45:44,074232537-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:45:44,080424089-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T18:45:47,979690111-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:45:47,986206783-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T18:45:55,835283678-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:45:55,842046363-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T18:45:59,991770927-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:45:59,998091005-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T18:46:04,792562785-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:46:04,798697132-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T18:46:09,086389844-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:46:09,092868665-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:46:13,868658229-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:46:13,875130943-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:46:18,775793267-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:46:18,782074218-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T18:46:23,184659701-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:46:23,190979764-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T18:46:27,153420557-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:46:27,159861031-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.06   89      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.94   90      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  1.01   80      up          osd.2  
                       TOTAL  300 GiB  177 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.94/1.06  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T18:46:31,133802852-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:46:55,368928436-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:47:04,593196495-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:47:13,699133270-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:47:22,795767938-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:47:32,720453825-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:47:32,741858950-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:47:43,136254442-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:47:43,157945516-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:47:52,259972372-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:47:52,281435360-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:48:01,351236101-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:48:01,372830361-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:48:10,346672289-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:48:10,367982099-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:48:10,385981697-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:48:10,396209449-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:48:10,420137465-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=929463
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T18:48:10,439373949-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-15T18:48:10,481637320-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:48:10,488106596-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:48:12.154+0000 ffffae8a6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:48:12.942+0000 ffffae8a6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:48:12.942+0000 ffffae8a6010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:48:12.966054+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-16T01:48:12.966149+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T01:48:13.913963+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:48:13.913963+0000     0       0         0         0         0         0           -           0
2021-05-16T01:48:14.914274+0000     1     223       223         0         0         0           -           0
2021-05-16T01:48:15.914821+0000     2     255       445       190   379.863       380     1.14459     1.14094
2021-05-16T01:48:16.915369+0000     3     255       666       411   547.768       884     1.15786     1.14711
2021-05-16T01:48:17.915739+0000     4     255       881       626   625.743       860     1.17708     1.15753
2021-05-16T01:48:18.916044+0000     5     255      1100       845   675.737       876     1.17788     1.16385
2021-05-16T01:48:19.916447+0000     6     255      1320      1065   709.722       880     1.15155     1.16364
2021-05-16T01:48:20.916875+0000     7     255      1537      1282   732.281       868     1.18779     1.16846
2021-05-16T01:48:21.917201+0000     8     255      1759      1504   751.708       888     1.14873     1.16753
2021-05-16T01:48:22.917665+0000     9     255      1980      1725   766.363       884     1.14328     1.16486
2021-05-16T01:48:23.918046+0000    10     255      2200      1945   777.693       880     1.16804     1.16426
2021-05-16T01:48:24.918427+0000    11     255      2420      2165   786.963       880     1.16806     1.16625
2021-05-16T01:48:25.918727+0000    12     255      2631      2376   791.695       844      1.1833     1.16822
2021-05-16T01:48:26.919077+0000    13     255      2851      2596   798.463       880       1.145     1.16914
2021-05-16T01:48:27.919430+0000    14     255      3073      2818   804.836       888     1.15616     1.16793
2021-05-16T01:48:28.919918+0000    15     255      3299      3044   811.419       904     1.12693     1.16671
2021-05-16T01:48:29.920278+0000    16     255      3521      3266   816.185       888     1.13758     1.16383
2021-05-16T01:48:30.920538+0000    17     255      3747      3492   821.336       904     1.13175     1.16321
2021-05-16T01:48:31.920921+0000    18     255      3965      3710   824.132       872     1.17061     1.16269
2021-05-16T01:48:32.921398+0000    19     255      4194      3939   828.945       916     1.10769     1.16164
2021-05-16T01:48:33.921798+0000 min lat: 0.701335 max lat: 1.21929 avg lat: 1.15451
2021-05-16T01:48:33.921798+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:48:33.921798+0000    20     153      4416      4263   852.272      1296    0.701335     1.15451
2021-05-16T01:48:34.922169+0000 Total time run:         20.0242
Total writes made:      4416
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     882.131
Stddev Bandwidth:       246.981
Max bandwidth (MB/sec): 1296
Min bandwidth (MB/sec): 0
Average IOPS:           220
Stddev IOPS:            61.7451
Max IOPS:               324
Min IOPS:               0
Average Latency(s):     1.12672
Stddev Latency(s):      0.158113
Max latency(s):         1.21929
Min latency(s):         0.0230936

[1;32mlocalhost.localdomain	[2021-05-15T18:48:35,693501783-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 929463


[1;33mlocalhost.localdomain	[2021-05-15T18:48:35,704391988-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:48:59,736525961-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.42k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:48:59,758209228-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:49:08,830601537-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.42k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:49:08,852038837-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:49:18,217868496-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.42k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:49:18,241629724-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:49:27,280648183-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.42k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:49:27,301911256-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:49:36,404137216-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.42k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:49:36,426124463-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:49:36,443936824-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:49:36,454225590-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:49:36,478147419-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=932938
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T18:49:36,497188776-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-15T18:49:36,539126131-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:49:36,545407709-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5064a5ff-ad8f-4d7e-8dad-bec5e49fdabc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ylnh3d:/tmp/ceph-asok.ylnh3d -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:49:38.004+0000 ffffb4ced010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:49:38.837+0000 ffffb4ced010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:49:38.837+0000 ffffb4ced010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:49:38.862077+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:49:38.862077+0000     0       0         0         0         0         0           -           0
2021-05-16T01:49:39.863052+0000     1     255       355       100   399.545       400      0.7962    0.720513
2021-05-16T01:49:40.863497+0000     2     255       590       335    669.47       940      1.0742    0.893801
2021-05-16T01:49:41.865711+0000     3     255       826       571   760.371       944     1.08924    0.969913
2021-05-16T01:49:42.865990+0000     4     255      1053       798   797.188       908     1.11878     1.01105
2021-05-16T01:49:43.866464+0000     5     255      1294      1039   830.444       964     1.06301     1.02994
2021-05-16T01:49:44.868373+0000     6     255      1532      1277   850.418       952     1.07618     1.03665
2021-05-16T01:49:45.868949+0000     7     255      1742      1487   848.861       840     1.18955     1.04979
2021-05-16T01:49:46.870607+0000     8     255      1948      1693   845.581       824     1.23502     1.07256
2021-05-16T01:49:47.870919+0000     9     255      2160      1905    845.82       848     1.19242     1.08909
2021-05-16T01:49:48.871354+0000    10     255      2373      2118   846.401       852     1.19846     1.09968
2021-05-16T01:49:49.871696+0000    11     255      2582      2327    845.43       836     1.22996      1.1098
2021-05-16T01:49:50.872821+0000    12     255      2806      2551   849.561       896     1.16791      1.1171
2021-05-16T01:49:51.873194+0000    13     255      3025      2770   851.569       876     1.15576     1.12038
2021-05-16T01:49:52.873502+0000    14     255      3232      2977   849.868       828     1.21986     1.12521
2021-05-16T01:49:53.874308+0000    15     255      3443      3188   849.431       844     1.22049     1.13152
2021-05-16T01:49:54.874524+0000    16     255      3659      3404    850.33       864      1.2035     1.13595
2021-05-16T01:49:55.875910+0000    17     255      3874      3619   850.829       860     1.18366      1.1389
2021-05-16T01:49:56.877471+0000    18     256      4089      3833   851.042       856     1.20572     1.14189
2021-05-16T01:49:57.877757+0000    19     255      4306      4051   852.132       872     1.17737     1.14429
2021-05-16T01:49:58.878252+0000 Total time run:       19.9041
Total reads made:     4416
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   887.453
Average IOPS:         221
Stddev IOPS:          29.5307
Max IOPS:             241
Min IOPS:             100
Average Latency(s):   1.12469
Max latency(s):       1.26743
Min latency(s):       0.394398

[1;32mlocalhost.localdomain	[2021-05-15T18:49:59,926649647-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 932938


[1;33mlocalhost.localdomain	[2021-05-15T18:49:59,937279901-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:50:24,079475095-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.42k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:50:24,101235606-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:50:33,540217873-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.42k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:50:33,562177077-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:50:42,878803021-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.42k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:50:42,900163232-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:50:51,988808533-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.42k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:50:52,010429609-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:51:01,111082583-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.42k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:51:01,132629021-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:51:01,150465203-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T18:51:01,157200292-07:00][RUNNING][ROUND 4/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:51:01,167209842-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T18:51:01,185955960-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40464\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.84088\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e3b49ca7-4f92-41d6-a936-7a15c5e958fd\nsetting min_mon_release = octopus\nepoch 0\nfsid e3b49ca7-4f92-41d6-a936-7a15c5e958fd\nlast_changed 2021-05-15T18:51:30.172567-0700\ncreated 2021-05-15T18:51:30.172567-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40464/0,v1:10.10.1.2:40465/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.84088 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 79577ad5-910b-45f1-8648-1dfa295eaa36\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 e6962985-e44c-4ec9-8db5-912ad802ee4d\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 7fc64843-84fe-4b48-9cf2-4dd6d63005a6\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42464\n  w/ user/pass: admin / d48261ce-507e-4765-a09d-1c7b389b7b7f\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 18:51:49 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40464
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.84088
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e3b49ca7-4f92-41d6-a936-7a15c5e958fd
setting min_mon_release = octopus
epoch 0
fsid e3b49ca7-4f92-41d6-a936-7a15c5e958fd
last_changed 2021-05-15T18:51:30.172567-0700
created 2021-05-15T18:51:30.172567-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40464/0,v1:10.10.1.2:40465/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.84088 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 79577ad5-910b-45f1-8648-1dfa295eaa36
0
start osd.0
add osd1 e6962985-e44c-4ec9-8db5-912ad802ee4d
1
start osd.1
add osd2 7fc64843-84fe-4b48-9cf2-4dd6d63005a6
2
start osd.2


restful urls: https://10.10.1.2:42464
  w/ user/pass: admin / d48261ce-507e-4765-a09d-1c7b389b7b7f


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T18:51:03.095-0700 7ff9911691c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:51:03.095-0700 7ff9911691c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:51:03.111-0700 7f4b165351c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:51:03.111-0700 7f4b165351c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40464,v1:10.10.1.2:40465] --print /tmp/ceph_monmap.84088 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.84088 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.84088 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42464 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55d689046000 @  0x7fe1fb35d680 0x7fe1fb37e824 0x7fe1fbb19187 0x7fe1fbb21355 0x7fe1fbb19708 0x7fe1fbb19877 0x7fe1fbb1ac24 0x7fe1fbb32ec1 0x7fe1fbaa55f3 0x7fe1fbb06e97 0x7fe1fbb0eb1a 0x7fe1fb211d84 0x7fe1fb32d609 0x7fe1faf01293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.y1AVOXeuRb 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 79577ad5-910b-45f1-8648-1dfa295eaa36 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCseqBgCWquJhAAEioYMJhVnvVndzBExI6TUQ== --osd-uuid 79577ad5-910b-45f1-8648-1dfa295eaa36 
2021-05-15T18:51:41.296-0700 7fa43e3c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:51:41.316-0700 7fa43e3c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:51:41.316-0700 7fa43e3c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x564cab1ba000 @  0x7fa43ed8e680 0x7fa43edaf824 0x564c9f9b6447 0x564c9f9be4b5 0x564c9f9b69c8 0x564c9f9b6b37 0x564c9f9b7ee4 0x564c9f788ca1 0x564c9f960423 0x564c9f7792a7 0x564c9f77e53a 0x7fa43e8e1d84 0x7fa43ea66609 0x7fa43e5cf293
2021-05-15T18:51:41.636-0700 7fa43e3c5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e6962985-e44c-4ec9-8db5-912ad802ee4d -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55dd418a0000 @  0x7f48f1d59680 0x7f48f1d7a824 0x55dd35e87447 0x55dd35e8f4b5 0x55dd35e879c8 0x55dd35e87b37 0x55dd35e88ee4 0x55dd35c59ca1 0x55dd35e31423 0x55dd35c4a2a7 0x55dd35c4f53a 0x7f48f18acd84 0x7f48f1a31609 0x7f48f159a293
2021-05-15T18:51:42.224-0700 7f48f1390f00 -1 Falling back to public interface
2021-05-15T18:51:42.480-0700 7f48f1390f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCteqBgtARZNxAA1hM9/uP6kXgmi1X+wZmE3Q== --osd-uuid e6962985-e44c-4ec9-8db5-912ad802ee4d 
2021-05-15T18:51:42.596-0700 7f4746b5ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:51:42.616-0700 7f4746b5ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:51:42.616-0700 7f4746b5ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x559e3df3e000 @  0x7f4747527680 0x7f4747548824 0x559e33619447 0x559e336214b5 0x559e336199c8 0x559e33619b37 0x559e3361aee4 0x559e333ebca1 0x559e335c3423 0x559e333dc2a7 0x559e333e153a 0x7f474707ad84 0x7f47471ff609 0x7f4746d68293
2021-05-15T18:51:42.932-0700 7f4746b5ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7fc64843-84fe-4b48-9cf2-4dd6d63005a6 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5615a4ab8000 @  0x7f6673a27680 0x7f6673a48824 0x561599756447 0x56159975e4b5 0x5615997569c8 0x561599756b37 0x561599757ee4 0x561599528ca1 0x561599700423 0x5615995192a7 0x56159951e53a 0x7f667357ad84 0x7f66736ff609 0x7f6673268293
2021-05-15T18:51:43.616-0700 7f667305ef00 -1 Falling back to public interface
2021-05-15T18:51:43.876-0700 7f667305ef00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCveqBgdOjqERAAo1bovcSaUJxwcveJ5PiQfQ== --osd-uuid 7fc64843-84fe-4b48-9cf2-4dd6d63005a6 
2021-05-15T18:51:43.976-0700 7f24c7ff2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:51:43.996-0700 7f24c7ff2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:51:43.996-0700 7f24c7ff2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b69dc54000 @  0x7f24c89bb680 0x7f24c89dc824 0x55b693948447 0x55b6939504b5 0x55b6939489c8 0x55b693948b37 0x55b693949ee4 0x55b69371aca1 0x55b6938f2423 0x55b69370b2a7 0x55b69371053a 0x7f24c850ed84 0x7f24c8693609 0x7f24c81fc293
2021-05-15T18:51:44.368-0700 7f24c7ff2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x559e648ba000 @  0x7f7366756680 0x7f7366777824 0x559e59c40447 0x559e59c484b5 0x559e59c409c8 0x559e59c40b37 0x559e59c41ee4 0x559e59a12ca1 0x559e59bea423 0x559e59a032a7 0x559e59a0853a 0x7f73662a9d84 0x7f736642e609 0x7f7365f97293
2021-05-15T18:51:45.044-0700 7f7365d8df00 -1 Falling back to public interface
2021-05-15T18:51:45.316-0700 7f7365d8df00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T18:51:49,261285345-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T18:51:49,285354943-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T18:51:49,371143777-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:51:49,377360737-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T18:51:53,468331617-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:51:53,474849634-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T18:51:57,396197412-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:51:57,403406821-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T18:52:01,288532712-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:52:01,295063586-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T18:52:09,135754672-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:52:09,142352682-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T18:52:13,213383784-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:52:13,219866484-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T18:52:18,051177384-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:52:18,057518460-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T18:52:22,443197127-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:52:22,449588326-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:52:27,018201080-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:52:27,024620529-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:52:31,354221100-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:52:31,360575318-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T18:52:35,909576224-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:52:35,916378043-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T18:52:39,658935101-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:52:39,665126406-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  180 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  180 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   66 KiB   0 B   0 B   0 B  100 GiB     0  1.10   89      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.92   90      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  0.98   80      up          osd.2  
                       TOTAL  300 GiB  181 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.92/1.10  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T18:52:43,568116359-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:53:07,805347891-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:53:16,875116939-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:53:26,079346650-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:53:35,271015194-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:53:44,516352802-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:53:44,538166039-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:53:53,720451496-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:53:53,742103515-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:54:02,864595699-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:54:02,885920422-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:54:12,340956665-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:54:12,362489098-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:54:21,704878943-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:54:21,726590626-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:54:21,744462283-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:54:21,754572077-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:54:21,778665028-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=946151
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T18:54:21,798207175-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-15T18:54:21,840568001-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:54:21,847006502-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:54:23.373+0000 ffff892d5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:54:24.161+0000 ffff892d5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:54:24.161+0000 ffff892d5010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:54:24.187592+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-16T01:54:24.187688+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T01:54:25.125264+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:54:25.125264+0000     0       0         0         0         0         0           -           0
2021-05-16T01:54:26.125531+0000     1     215       215         0         0         0           -           0
2021-05-16T01:54:27.125901+0000     2     255       438       183   365.912       366     1.16138     1.18662
2021-05-16T01:54:28.126487+0000     3     255       662       407   542.474       896     1.13783     1.15752
2021-05-16T01:54:29.126842+0000     4     255       881       626   625.778       876     1.17664     1.15504
2021-05-16T01:54:30.127082+0000     5     255      1098       843   674.176       868      1.1857     1.16344
2021-05-16T01:54:31.127345+0000     6     255      1312      1057   704.441       856     1.17459      1.1663
2021-05-16T01:54:32.127661+0000     7     255      1538      1283   732.908       904     1.15331     1.16611
2021-05-16T01:54:33.128010+0000     8     255      1755      1500   749.757       868      1.1787     1.16414
2021-05-16T01:54:34.128316+0000     9     255      1979      1724   765.976       896     1.15733     1.16582
2021-05-16T01:54:35.128833+0000    10     255      2203      1948   778.934       896     1.13075     1.16394
2021-05-16T01:54:36.129221+0000    11     255      2421      2166   787.364       872      1.1694     1.16258
2021-05-16T01:54:37.129518+0000    12     255      2635      2380   793.063       856     1.18423     1.16501
2021-05-16T01:54:38.129864+0000    13     255      2857      2602   800.342       888     1.14887     1.16417
2021-05-16T01:54:39.130273+0000    14     255      3078      2823   806.292       884     1.16913     1.16388
2021-05-16T01:54:40.130712+0000    15     255      3296      3041   810.647       872     1.16758     1.16386
2021-05-16T01:54:41.131143+0000    16     255      3514      3259   814.459       872      1.1804     1.16502
2021-05-16T01:54:42.131491+0000    17     255      3730      3475   817.355       864     1.17177     1.16603
2021-05-16T01:54:43.131721+0000    18     255      3957      3702   822.379       908     1.12608     1.16539
2021-05-16T01:54:44.132077+0000    19     255      4173      3918   824.553       864     1.18065     1.16417
2021-05-16T01:54:45.132534+0000 min lat: 0.0385084 max lat: 1.23776 avg lat: 1.13568
2021-05-16T01:54:45.132534+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:54:45.132534+0000    20       8      4385      4377   875.089      1836   0.0385084     1.13568
2021-05-16T01:54:46.133258+0000 Total time run:         20.0282
Total writes made:      4385
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     875.766
Stddev Bandwidth:       320.067
Max bandwidth (MB/sec): 1836
Min bandwidth (MB/sec): 0
Average IOPS:           218
Stddev IOPS:            80.0571
Max IOPS:               459
Min IOPS:               0
Average Latency(s):     1.13367
Stddev Latency(s):      0.157598
Max latency(s):         1.23776
Min latency(s):         0.0275116

[1;32mlocalhost.localdomain	[2021-05-15T18:54:46,915985438-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 946151


[1;33mlocalhost.localdomain	[2021-05-15T18:54:46,927022150-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:55:11,208020125-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.39k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:55:11,229634980-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:55:20,431337946-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.39k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:55:20,453344548-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:55:29,531541090-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.39k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:55:29,553839642-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:55:38,662885811-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.39k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:55:38,684654847-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:55:48,064387832-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.39k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:55:48,089467574-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:55:48,109202232-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:55:48,119865905-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:55:48,144981698-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=949716
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T18:55:48,164636122-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-15T18:55:48,206096789-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:55:48,212378032-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '87d6692f-2fae-4649-8e01-9a5026d4ddab', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 87d6692f-2fae-4649-8e01-9a5026d4ddab --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m4tHk2:/tmp/ceph-asok.m4tHk2 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T01:55:49.700+0000 ffffa9d92010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:55:50.500+0000 ffffa9d92010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T01:55:50.500+0000 ffffa9d92010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T01:55:50.527693+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T01:55:50.527693+0000     0       0         0         0         0         0           -           0
2021-05-16T01:55:51.528902+0000     1     255       347        92   367.488       368    0.817971    0.731502
2021-05-16T01:55:52.530045+0000     2     255       583       328   655.169       944      1.0865    0.916086
2021-05-16T01:55:53.531122+0000     3     255       819       564   751.095       944     1.08587    0.986357
2021-05-16T01:55:54.532723+0000     4     255      1055       800   798.958       944     1.07849      1.0157
2021-05-16T01:55:55.532995+0000     5     255      1287      1032   824.695       928     1.09541     1.03211
2021-05-16T01:55:56.534940+0000     6     255      1528      1273   847.617       964     1.06617     1.04145
2021-05-16T01:55:57.535759+0000     7     255      1767      1512   862.983       956     1.07071     1.04405
2021-05-16T01:55:58.538387+0000     8     255      2003      1748   872.813       944     1.08942     1.04884
2021-05-16T01:55:59.538964+0000     9     255      2237      1982   879.769       936     1.09588     1.05489
2021-05-16T01:56:00.539216+0000    10     255      2472      2217   885.763       940     1.08413      1.0581
2021-05-16T01:56:01.539476+0000    11     255      2711      2456    892.12       956     1.07066        1.06
2021-05-16T01:56:02.541262+0000    12     255      2931      2676   890.979       880     1.15057     1.06243
2021-05-16T01:56:03.541528+0000    13     255      3152      2897   890.424       884     1.16641     1.07098
2021-05-16T01:56:04.541781+0000    14     255      3366      3111   887.952       856     1.18614     1.07828
2021-05-16T01:56:05.542079+0000    15     255      3580      3325   885.806       856     1.18667     1.08597
2021-05-16T01:56:06.542557+0000    16     255      3796      3541   884.418       864     1.19331     1.09227
2021-05-16T01:56:07.542774+0000    17     255      4008      3753   882.267       848     1.19845     1.09777
2021-05-16T01:56:08.543056+0000    18     256      4218      3962   879.685       836     1.21276     1.10373
2021-05-16T01:56:09.548380+0000    19     145      4385      4240   891.652      1112    0.907998     1.10754
2021-05-16T01:56:10.548699+0000 Total time run:       19.2255
Total reads made:     4385
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   912.331
Average IOPS:         228
Stddev IOPS:          35.4217
Max IOPS:             278
Min IOPS:             92
Average Latency(s):   1.09263
Max latency(s):       1.23412
Min latency(s):       0.407243

[1;32mlocalhost.localdomain	[2021-05-15T18:56:11,330059764-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 949716


[1;33mlocalhost.localdomain	[2021-05-15T18:56:11,340994578-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:56:35,314258879-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.39k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:56:35,335866456-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:56:44,475779117-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.39k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:56:44,497535819-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:56:53,586419393-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.39k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:56:53,608383670-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:57:02,882064679-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.39k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:57:02,904025596-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:57:11,978987894-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.39k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:57:12,000744685-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T18:57:12,018978155-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T18:57:12,025923367-07:00][RUNNING][ROUND 5/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T18:57:12,035982900-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T18:57:12,054689356-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40663\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.85176\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 81b4b863-7ff7-4acd-9c9b-9ec2d6c88ae9\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 81b4b863-7ff7-4acd-9c9b-9ec2d6c88ae9\nlast_changed 2021-05-15T18:57:41.579612-0700\ncreated 2021-05-15T18:57:41.579612-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40663/0,v1:10.10.1.2:40664/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.85176 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 f446346c-ab46-4708-be6d-b043ad706e43\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 9cc6546b-f934-4a11-a519-bed3d79e94f9\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 03284adc-f72a-4ea2-be00-d3fd25c80b9a\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42663\n  w/ user/pass: admin / 529326cd-20f9-4c41-8f7a-5fed453ca2da\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 18:58:00 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40663
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.85176
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 81b4b863-7ff7-4acd-9c9b-9ec2d6c88ae9
setting min_mon_release = octopus
epoch 0
fsid 81b4b863-7ff7-4acd-9c9b-9ec2d6c88ae9
last_changed 2021-05-15T18:57:41.579612-0700
created 2021-05-15T18:57:41.579612-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40663/0,v1:10.10.1.2:40664/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.85176 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 f446346c-ab46-4708-be6d-b043ad706e43
0
start osd.0
add osd1 9cc6546b-f934-4a11-a519-bed3d79e94f9
1
start osd.1
add osd2 03284adc-f72a-4ea2-be00-d3fd25c80b9a
2
start osd.2


restful urls: https://10.10.1.2:42663
  w/ user/pass: admin / 529326cd-20f9-4c41-8f7a-5fed453ca2da


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T18:57:13.983-0700 7f887b2061c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:57:13.983-0700 7f887b2061c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:57:14.003-0700 7f94617101c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T18:57:14.003-0700 7f94617101c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40663,v1:10.10.1.2:40664] --print /tmp/ceph_monmap.85176 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.85176 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.85176 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42663 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55cbe3842000 @  0x7fbabe1e3680 0x7fbabe204824 0x7fbabe99f187 0x7fbabe9a7355 0x7fbabe99f708 0x7fbabe99f877 0x7fbabe9a0c24 0x7fbabe9b8ec1 0x7fbabe92b5f3 0x7fbabe98ce97 0x7fbabe994b1a 0x7fbabe097d84 0x7fbabe1b3609 0x7fbabdd87293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.x1hhzHZbWA 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f446346c-ab46-4708-be6d-b043ad706e43 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAgfKBgm6iHBRAAY2O6xagwQGo112WDkGm6ZA== --osd-uuid f446346c-ab46-4708-be6d-b043ad706e43 
2021-05-15T18:57:52.723-0700 7f9be3c94f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:57:52.743-0700 7f9be3c94f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T18:57:52.743-0700 7f9be3c94f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55ab76690000 @  0x7f9be465d680 0x7f9be467e824 0x55ab6c713447 0x55ab6c71b4b5 0x55ab6c7139c8 0x55ab6c713b37 0x55ab6c714ee4 0x55ab6c4e5ca1 0x55ab6c6bd423 0x55ab6c4d62a7 0x55ab6c4db53a 0x7f9be41b0d84 0x7f9be4335609 0x7f9be3e9e293
2021-05-15T18:57:53.039-0700 7f9be3c94f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9cc6546b-f934-4a11-a519-bed3d79e94f9 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55bdf08a6000 @  0x7f48652e0680 0x7f4865301824 0x55bde5f19447 0x55bde5f214b5 0x55bde5f199c8 0x55bde5f19b37 0x55bde5f1aee4 0x55bde5cebca1 0x55bde5ec3423 0x55bde5cdc2a7 0x55bde5ce153a 0x7f4864e33d84 0x7f4864fb8609 0x7f4864b21293
2021-05-15T18:57:53.723-0700 7f4864917f00 -1 Falling back to public interface
2021-05-15T18:57:53.983-0700 7f4864917f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAhfKBgbRNVGBAAJ3/Ay2tanOr1BvyRsuBGtQ== --osd-uuid 9cc6546b-f934-4a11-a519-bed3d79e94f9 
2021-05-15T18:57:54.055-0700 7f92e51f3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:57:54.075-0700 7f92e51f3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T18:57:54.075-0700 7f92e51f3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55705bfcc000 @  0x7f92e5bbc680 0x7f92e5bdd824 0x557050342447 0x55705034a4b5 0x5570503429c8 0x557050342b37 0x557050343ee4 0x557050114ca1 0x5570502ec423 0x5570501052a7 0x55705010a53a 0x7f92e570fd84 0x7f92e5894609 0x7f92e53fd293
2021-05-15T18:57:54.383-0700 7f92e51f3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 03284adc-f72a-4ea2-be00-d3fd25c80b9a -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x560777630000 @  0x7efcf0fb8680 0x7efcf0fd9824 0x56076c1aa447 0x56076c1b24b5 0x56076c1aa9c8 0x56076c1aab37 0x56076c1abee4 0x56076bf7cca1 0x56076c154423 0x56076bf6d2a7 0x56076bf7253a 0x7efcf0b0bd84 0x7efcf0c90609 0x7efcf07f9293
2021-05-15T18:57:55.011-0700 7efcf05eff00 -1 Falling back to public interface
2021-05-15T18:57:55.263-0700 7efcf05eff00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAifKBgxw11KhAADHGSL+HNBr/IZlrwOqArWw== --osd-uuid 03284adc-f72a-4ea2-be00-d3fd25c80b9a 
2021-05-15T18:57:55.363-0700 7f8054b6bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:57:55.387-0700 7f8054b6bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T18:57:55.387-0700 7f8054b6bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55af7314a000 @  0x7f8055534680 0x7f8055555824 0x55af682fd447 0x55af683054b5 0x55af682fd9c8 0x55af682fdb37 0x55af682feee4 0x55af680cfca1 0x55af682a7423 0x55af680c02a7 0x55af680c553a 0x7f8055087d84 0x7f805520c609 0x7f8054d75293
2021-05-15T18:57:55.695-0700 7f8054b6bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x562acaddc000 @  0x7f8318082680 0x7f83180a3824 0x562abf524447 0x562abf52c4b5 0x562abf5249c8 0x562abf524b37 0x562abf525ee4 0x562abf2f6ca1 0x562abf4ce423 0x562abf2e72a7 0x562abf2ec53a 0x7f8317bd5d84 0x7f8317d5a609 0x7f83178c3293
2021-05-15T18:57:56.323-0700 7f83176b9f00 -1 Falling back to public interface
2021-05-15T18:57:56.595-0700 7f83176b9f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T18:58:00,685640829-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T18:58:00,709848393-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T18:58:00,792364795-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:58:00,798995043-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T18:58:04,595007659-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:58:04,601212673-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T18:58:08,513432407-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:58:08,519663781-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T18:58:12,401052440-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:58:12,407378450-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T18:58:20,650982513-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:58:20,658489705-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T18:58:25,438396982-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:58:25,444641424-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T18:58:29,845559215-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:58:29,851876013-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T18:58:34,430579261-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:58:34,436988610-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:58:38,664677488-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:58:38,671087243-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T18:58:43,481733480-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:58:43,488220032-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T18:58:48,079354184-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:58:48,085796734-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T18:58:51,804661207-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T18:58:51,810992393-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   66 KiB   0 B   0 B   0 B  100 GiB     0  1.12   89      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.87   90      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  1.01   80      up          osd.2  
                       TOTAL  300 GiB  178 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.87/1.12  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T18:58:55,511795451-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T18:59:19,576022270-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:59:30,775647386-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:59:39,772909402-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:59:48,868387572-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:59:58,172285457-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   248 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   peering
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T18:59:58,195812037-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:00:07,120280802-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:00:07,142132137-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:00:16,204320374-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:00:16,226509537-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:00:25,508172590-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:00:25,530297466-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:00:34,814575496-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:00:34,836575401-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:00:34,854826961-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:00:34,864843690-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:00:34,888607356-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=962990
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T19:00:34,908408440-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-15T19:00:34,951789769-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:00:34,958411719-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:00:36.613+0000 ffffb7a6b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:00:37.405+0000 ffffb7a6b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:00:37.405+0000 ffffb7a6b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:00:37.431185+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-16T02:00:37.431263+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-16T02:00:38.418499+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:00:38.418499+0000     0       0         0         0         0         0           -           0
2021-05-16T02:00:39.418831+0000     1     214       214         0         0         0           -           0
2021-05-16T02:00:40.419238+0000     2     255       433       178   355.901       356     1.15778     1.17852
2021-05-16T02:00:41.419473+0000     3     255       651       396   527.861       872     1.16926     1.16432
2021-05-16T02:00:42.419741+0000     4     255       871       616   615.837       880     1.17186     1.17185
2021-05-16T02:00:43.419966+0000     5     255      1082       827    661.43       844     1.20675     1.17922
2021-05-16T02:00:44.420255+0000     6     255      1303      1048   698.483       884     1.15973     1.17899
2021-05-16T02:00:45.420543+0000     7     255      1513      1258   718.666       840     1.20387     1.17915
2021-05-16T02:00:46.420955+0000     8     255      1715      1460   729.792       808     1.25509     1.18767
2021-05-16T02:00:47.421494+0000     9     255      1926      1671   742.435       844     1.23111     1.19573
2021-05-16T02:00:48.421902+0000    10     255      2147      1892   756.556       884      1.1615     1.19454
2021-05-16T02:00:49.422219+0000    11     255      2363      2108   766.299       864     1.19008     1.19195
2021-05-16T02:00:50.422479+0000    12     255      2578      2323   774.088       860     1.18725     1.19259
2021-05-16T02:00:51.422910+0000    13     255      2794      2539   780.977       864     1.17657     1.19128
2021-05-16T02:00:52.423395+0000    14     255      3010      2755   786.878       864     1.17845     1.19103
2021-05-16T02:00:53.423790+0000    15     255      3224      2969   791.464       856     1.17404     1.18956
2021-05-16T02:00:54.424120+0000    16     255      3435      3180    794.73       844     1.24171     1.19116
2021-05-16T02:00:55.424511+0000    17     255      3641      3386   796.433       824     1.25075     1.19336
2021-05-16T02:00:56.424799+0000    18     255      3854      3599   799.506       852     1.21454     1.19554
2021-05-16T02:00:57.425211+0000    19     255      4066      3811    802.04       848     1.20646     1.19609
2021-05-16T02:00:58.425587+0000 min lat: 0.0362733 max lat: 1.28358 avg lat: 1.16287
2021-05-16T02:00:58.425587+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:00:58.425587+0000    20       6      4278      4272   854.105      1844   0.0362733     1.16287
2021-05-16T02:00:59.425902+0000 Total time run:         20.0212
Total writes made:      4278
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     854.694
Stddev Bandwidth:       321.023
Max bandwidth (MB/sec): 1844
Min bandwidth (MB/sec): 0
Average IOPS:           213
Stddev IOPS:            80.2557
Max IOPS:               461
Min IOPS:               0
Average Latency(s):     1.16129
Stddev Latency(s):      0.166081
Max latency(s):         1.28358
Min latency(s):         0.0250594

[1;32mlocalhost.localdomain	[2021-05-15T19:01:00,190355173-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 962990


[1;33mlocalhost.localdomain	[2021-05-15T19:01:00,201609898-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:01:24,336566782-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:01:24,358799676-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:01:33,704048299-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:01:33,726497199-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:01:42,816282332-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:01:42,838610581-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:01:51,947732074-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:01:51,970115698-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:02:01,239239891-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:02:01,264956588-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:02:01,284774774-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:02:01,295984867-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:02:01,320676665-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=966552
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T19:02:01,340646842-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-15T19:02:01,382504253-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:02:01,388911381-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0d2096e9-ae30-4686-861b-8d78da4be210', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0d2096e9-ae30-4686-861b-8d78da4be210 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.7aIMDA:/tmp/ceph-asok.7aIMDA -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:02:02.868+0000 ffff94be8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:02:03.680+0000 ffff94be8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:02:03.680+0000 ffff94be8010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:02:03.708806+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:02:03.708806+0000     0       0         0         0         0         0           -           0
2021-05-16T02:02:04.711480+0000     1     255       348        93   370.951       372    0.812337    0.731323
2021-05-16T02:02:05.712914+0000     2     255       584       329   656.601       944     1.08323      0.9174
2021-05-16T02:02:06.714890+0000     3     255       819       564    750.44       940     1.09105    0.987003
2021-05-16T02:02:07.717862+0000     4     255      1060       805   803.151       964     1.06964     1.01377
2021-05-16T02:02:08.718141+0000     5     255      1274      1019   813.656       856      1.1681      1.0348
2021-05-16T02:02:09.718506+0000     6     255      1491      1236   822.649       868     1.18112     1.06325
2021-05-16T02:02:10.718771+0000     7     255      1705      1450   827.376       856      1.1816     1.07945
2021-05-16T02:02:11.726541+0000     8     255      1917      1662   829.146       848     1.22451     1.09559
2021-05-16T02:02:12.726835+0000     9     255      2130      1875   831.653       852     1.20541     1.10816
2021-05-16T02:02:13.727794+0000    10     255      2344      2089   834.004       856     1.20011     1.11797
2021-05-16T02:02:14.728040+0000    11     255      2563      2308   837.796       876      1.1645     1.12319
2021-05-16T02:02:15.729358+0000    12     255      2774      2519    838.22       844     1.21448     1.12919
2021-05-16T02:02:16.729674+0000    13     255      2980      2725   837.108       824     1.22986     1.13642
2021-05-16T02:02:17.730983+0000    14     255      3193      2938   838.092       852     1.20601      1.1427
2021-05-16T02:02:18.731224+0000    15     255      3410      3155   840.069       868     1.18602     1.14571
2021-05-16T02:02:19.732646+0000    16     255      3627      3372   841.738       868     1.18367     1.14881
2021-05-16T02:02:20.732874+0000    17     255      3857      3602   846.324       920     1.11514     1.14816
2021-05-16T02:02:21.733097+0000    18     255      4086      3831   850.179       916     1.11422     1.14639
2021-05-16T02:02:22.775547+0000    19     200      4278      4078   855.514       988     1.07983     1.14622
2021-05-16T02:02:23.775810+0000 Total time run:       19.3473
Total reads made:     4278
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   884.463
Average IOPS:         221
Stddev IOPS:          31.6126
Max IOPS:             247
Min IOPS:             93
Average Latency(s):   1.1277
Max latency(s):       1.2425
Min latency(s):       0.42025

[1;32mlocalhost.localdomain	[2021-05-15T19:02:24,556074100-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 966552


[1;33mlocalhost.localdomain	[2021-05-15T19:02:24,567240715-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:02:48,676305862-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:02:48,698671392-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:02:57,885392348-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:02:57,907650475-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:03:06,997518811-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:03:07,020152707-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:03:16,420836008-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:03:16,445795496-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:03:25,504788572-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.28k objects, 17 GiB
    usage:   33 GiB used, 267 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:03:25,527436465-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:03:25,546186769-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T19:03:25,556938323-07:00][RUNNING][ROUND 1/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:03:25,567541036-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T19:03:25,586631657-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40209\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.86262\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid d518c77c-7f27-47d0-97dd-12cc922602ba\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid d518c77c-7f27-47d0-97dd-12cc922602ba\nlast_changed 2021-05-15T19:04:00.758834-0700\ncreated 2021-05-15T19:04:00.758834-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40209/0,v1:10.10.1.2:40210/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.86262 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 6b8f996b-95ca-4aac-9e16-1db5563e40e3\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 0614de70-6100-478a-b9e5-68391299d2af\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 678f002d-5258-4aa2-acca-8250a67a2b6d\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42209\n  w/ user/pass: admin / 4bcd0c3e-c3ad-4713-bb37-1e46f4e62fa1\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 19:04:19 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40209
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.86262
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid d518c77c-7f27-47d0-97dd-12cc922602ba
setting min_mon_release = octopus
epoch 0
fsid d518c77c-7f27-47d0-97dd-12cc922602ba
last_changed 2021-05-15T19:04:00.758834-0700
created 2021-05-15T19:04:00.758834-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40209/0,v1:10.10.1.2:40210/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.86262 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 6b8f996b-95ca-4aac-9e16-1db5563e40e3
0
start osd.0
add osd1 0614de70-6100-478a-b9e5-68391299d2af
1
start osd.1
add osd2 678f002d-5258-4aa2-acca-8250a67a2b6d
2
start osd.2


restful urls: https://10.10.1.2:42209
  w/ user/pass: admin / 4bcd0c3e-c3ad-4713-bb37-1e46f4e62fa1


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T19:03:27.506-0700 7f5ca631a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:03:27.506-0700 7f5ca631a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:03:27.522-0700 7f91e0ebc1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:03:27.522-0700 7f91e0ebc1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40209,v1:10.10.1.2:40210] --print /tmp/ceph_monmap.86262 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.86262 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.86262 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42209 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5603d4830000 @  0x7fd69832e680 0x7fd69834f824 0x7fd698aea187 0x7fd698af2355 0x7fd698aea708 0x7fd698aea877 0x7fd698aebc24 0x7fd698b03ec1 0x7fd698a765f3 0x7fd698ad7e97 0x7fd698adfb1a 0x7fd6981e2d84 0x7fd6982fe609 0x7fd697ed2293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.60KuLPEXm5 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6b8f996b-95ca-4aac-9e16-1db5563e40e3 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCbfaBgAEBuChAAxnEj5AC0OEs56L7O8jeNGg== --osd-uuid 6b8f996b-95ca-4aac-9e16-1db5563e40e3 
2021-05-15T19:04:11.807-0700 7efc990c7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T19:04:11.827-0700 7efc990c7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T19:04:11.827-0700 7efc990c7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55cad30c8000 @  0x7efc99a90680 0x7efc99ab1824 0x55cac8b01447 0x55cac8b094b5 0x55cac8b019c8 0x55cac8b01b37 0x55cac8b02ee4 0x55cac88d3ca1 0x55cac8aab423 0x55cac88c42a7 0x55cac88c953a 0x7efc995e3d84 0x7efc99768609 0x7efc992d1293
2021-05-15T19:04:12.131-0700 7efc990c7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0614de70-6100-478a-b9e5-68391299d2af -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55d2214b8000 @  0x7fe0fd1c7680 0x7fe0fd1e8824 0x55d216f43447 0x55d216f4b4b5 0x55d216f439c8 0x55d216f43b37 0x55d216f44ee4 0x55d216d15ca1 0x55d216eed423 0x55d216d062a7 0x55d216d0b53a 0x7fe0fcd1ad84 0x7fe0fce9f609 0x7fe0fca08293
2021-05-15T19:04:12.707-0700 7fe0fc7fef00 -1 Falling back to public interface
2021-05-15T19:04:12.963-0700 7fe0fc7fef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCcfaBg7OQvGBAArcnJYa+adY+Wly+H/WZKKg== --osd-uuid 0614de70-6100-478a-b9e5-68391299d2af 
2021-05-15T19:04:13.063-0700 7f62159f2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T19:04:13.083-0700 7f62159f2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T19:04:13.083-0700 7f62159f2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x561311576000 @  0x7f62163bb680 0x7f62163dc824 0x561306be1447 0x561306be94b5 0x561306be19c8 0x561306be1b37 0x561306be2ee4 0x5613069b3ca1 0x561306b8b423 0x5613069a42a7 0x5613069a953a 0x7f6215f0ed84 0x7f6216093609 0x7f6215bfc293
2021-05-15T19:04:13.451-0700 7f62159f2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 678f002d-5258-4aa2-acca-8250a67a2b6d -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5571d7542000 @  0x7f71ca584680 0x7f71ca5a5824 0x5571cc59e447 0x5571cc5a64b5 0x5571cc59e9c8 0x5571cc59eb37 0x5571cc59fee4 0x5571cc370ca1 0x5571cc548423 0x5571cc3612a7 0x5571cc36653a 0x7f71ca0d7d84 0x7f71ca25c609 0x7f71c9dc5293
2021-05-15T19:04:14.107-0700 7f71c9bbbf00 -1 Falling back to public interface
2021-05-15T19:04:14.367-0700 7f71c9bbbf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCdfaBgNFusLxAA7BEWJJnpD+jvpk72YBcYyw== --osd-uuid 678f002d-5258-4aa2-acca-8250a67a2b6d 
2021-05-15T19:04:14.463-0700 7fb309557f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T19:04:14.483-0700 7fb309557f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T19:04:14.483-0700 7fb309557f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5628734ea000 @  0x7fb309f20680 0x7fb309f41824 0x5628694f1447 0x5628694f94b5 0x5628694f19c8 0x5628694f1b37 0x5628694f2ee4 0x5628692c3ca1 0x56286949b423 0x5628692b42a7 0x5628692b953a 0x7fb309a73d84 0x7fb309bf8609 0x7fb309761293
2021-05-15T19:04:14.799-0700 7fb309557f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55ade47fc000 @  0x7f6537095680 0x7f65370b6824 0x55adda766447 0x55adda76e4b5 0x55adda7669c8 0x55adda766b37 0x55adda767ee4 0x55adda538ca1 0x55adda710423 0x55adda5292a7 0x55adda52e53a 0x7f6536be8d84 0x7f6536d6d609 0x7f65368d6293
2021-05-15T19:04:15.371-0700 7f65366ccf00 -1 Falling back to public interface
2021-05-15T19:04:15.631-0700 7f65366ccf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T19:04:19,807303847-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T19:04:19,833138522-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T19:04:19,930638560-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:04:19,937437060-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T19:04:23,746132771-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:04:23,752620702-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T19:04:27,599386210-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:04:27,605724145-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T19:04:31,336099978-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:04:31,342632167-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T19:04:39,577072293-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:04:39,583652940-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T19:04:44,111419023-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:04:44,117914666-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T19:04:48,716136550-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:04:48,722636244-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T19:04:53,543979873-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:04:53,550616209-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T19:04:58,520637372-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:04:58,526602494-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T19:05:03,560528050-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:05:03,566859819-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T19:05:07,786789776-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:05:07,792911960-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T19:05:11,602608131-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:05:11,609122917-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  186 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  186 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.00  153      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   69 KiB   0 B   0 B   0 B  100 GiB     0  1.11  152      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.89  143      up          osd.2  
                       TOTAL  300 GiB  188 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.89/1.11  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T19:05:15,351615949-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:05:39,502794340-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:05:48,461574624-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:05:57,568492117-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:06,622831210-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:15,790648767-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:15,812670737-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:24,932640487-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:24,955408084-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:34,182801640-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:34,205441610-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:43,403820612-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:43,426446089-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:52,395116881-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:52,417443042-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:52,435840379-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:06:52,446394033-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:06:52,471268199-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=980047
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T19:06:52,491384498-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-15T19:06:52,533828324-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:06:52,540279019-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:06:54.009+0000 ffffaa3fd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:06:54.806+0000 ffffaa3fd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:06:54.806+0000 ffffaa3fd010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:06:54.840983+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-16T02:06:54.841110+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-16T02:06:58.830723+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:06:58.830723+0000     0       0         0         0         0         0           -           0
2021-05-16T02:06:59.831047+0000     1      28        28         0         0         0           -           0
2021-05-16T02:07:00.831418+0000     2      55        55         0         0         0           -           0
2021-05-16T02:07:01.831677+0000     3      82        82         0         0         0           -           0
2021-05-16T02:07:02.831922+0000     4     110       110         0         0         0           -           0
2021-05-16T02:07:03.832133+0000     5     137       137         0         0         0           -           0
2021-05-16T02:07:04.832393+0000     6     164       164         0         0         0           -           0
2021-05-16T02:07:05.832744+0000     7     190       190         0         0         0           -           0
2021-05-16T02:07:06.832992+0000     8     216       216         0         0         0           -           0
2021-05-16T02:07:07.833245+0000     9     243       243         0         0         0           -           0
2021-05-16T02:07:08.833484+0000    10     255       271        16   25.5934      25.6     9.46865     9.47823
2021-05-16T02:07:09.833712+0000    11     255       296        41   59.6211       400     9.54363     9.48646
2021-05-16T02:07:10.833929+0000    12     255       324        69   91.9767       448     9.48585     9.49936
2021-05-16T02:07:11.834125+0000    13     255       352        97   119.355       448     9.48026     9.49611
2021-05-16T02:07:12.834316+0000    14     255       379       124    141.68       432     9.50758     9.49562
2021-05-16T02:07:13.834536+0000    15     255       405       150   159.961       416     9.54557     9.51066
2021-05-16T02:07:14.834784+0000    16     255       434       179   178.956       464     9.45518     9.50919
2021-05-16T02:07:15.835051+0000    17     255       460       205   192.894       416     9.42191     9.49876
2021-05-16T02:07:16.835286+0000    18     255       484       229   203.506       384     9.49726     9.49751
2021-05-16T02:07:17.835498+0000    19     255       509       254   213.843       400     9.64128     9.50707
2021-05-16T02:07:18.835742+0000 min lat: 9.39402 max lat: 9.74087 avg lat: 9.52517
2021-05-16T02:07:18.835742+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:07:18.835742+0000    20     255       535       280   223.946       416      9.7392     9.52517
2021-05-16T02:07:19.836063+0000 Total time run:         20.1003
Total writes made:      536
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     426.659
Stddev Bandwidth:       216.145
Max bandwidth (MB/sec): 464
Min bandwidth (MB/sec): 0
Average IOPS:           26
Stddev IOPS:            13.537
Max IOPS:               29
Min IOPS:               0
Average Latency(s):     7.33976
Stddev Latency(s):      2.99183
Max latency(s):         9.74087
Min latency(s):         0.102303

[1;32mlocalhost.localdomain	[2021-05-15T19:07:21,018117823-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 980047


[1;33mlocalhost.localdomain	[2021-05-15T19:07:21,029736988-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:07:45,243085569-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 537 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:07:45,265600327-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:07:54,511037544-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 537 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:07:54,533237998-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:08:03,630462825-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 537 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:08:03,652289911-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:08:12,769720024-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 537 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:08:12,791919608-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:08:22,212057392-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 537 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:08:22,234194896-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:08:22,252821336-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:08:22,263670090-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:08:22,288658845-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=983619
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T19:08:22,308801473-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-15T19:08:22,350564812-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:08:22,356815642-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd81ea741-dad3-4ab6-b630-28f29fbfb3bd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d81ea741-dad3-4ab6-b630-28f29fbfb3bd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sptgBW:/tmp/ceph-asok.sptgBW -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:08:23.881+0000 ffffae452010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:08:24.693+0000 ffffae452010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:08:24.693+0000 ffffae452010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:08:24.728020+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:08:24.728020+0000     0       0         0         0         0         0           -           0
2021-05-16T02:08:25.728263+0000     1      89        89         0         0         0           -           0
2021-05-16T02:08:26.728532+0000     2     170       170         0         0         0           -           0
2021-05-16T02:08:27.728785+0000     3     255       255         0         0         0           -           0
2021-05-16T02:08:28.729078+0000     4     255       310        55   219.931       220     3.41572     3.22534
2021-05-16T02:08:29.731664+0000     5     255       364       109   348.532       864     3.75225     3.41638
2021-05-16T02:08:30.733983+0000     6     255       421       166   442.213       912     4.06924     3.59326
2021-05-16T02:08:31.734307+0000     7     255       476       221   504.675       880     4.42678     3.74977
2021-05-16T02:08:32.734634+0000     8     255       534       279   557.525       928     4.59138     3.91541
2021-05-16T02:08:34.270733+0000 Total time run:       9.54282
Total reads made:     536
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   898.686
Average IOPS:         56
Stddev IOPS:          28.5404
Max IOPS:             58
Min IOPS:             0
Average Latency(s):   3.50119
Max latency(s):       4.62422
Min latency(s):       1.51882

[1;32mlocalhost.localdomain	[2021-05-15T19:08:35,494445434-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 983619


[1;33mlocalhost.localdomain	[2021-05-15T19:08:35,505826044-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:08:59,734969823-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 537 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:08:59,757348148-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:09:08,828869296-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 537 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:09:08,851073381-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:09:18,067756611-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 537 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:09:18,090338715-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:09:27,147223118-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 537 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:09:27,171680422-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:09:36,162710106-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 537 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:09:36,185057585-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:09:36,203735288-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T19:09:36,211138585-07:00][RUNNING][ROUND 2/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:09:36,221864693-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T19:09:36,240992938-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40914\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.87352\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid cbf1113c-b233-4058-9077-473df27187c8\nsetting min_mon_release = octopus\nepoch 0\nfsid cbf1113c-b233-4058-9077-473df27187c8\nlast_changed 2021-05-15T19:10:06.342383-0700\ncreated 2021-05-15T19:10:06.342383-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40914/0,v1:10.10.1.2:40915/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.87352 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 0489f0ce-b9ae-41ac-9a5b-948c23d170eb\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 0093967a-b832-478e-9979-fd10f5c9ef7a\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 8a34bd86-1cc2-4c0c-b11d-4f79edd30605\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42914\n  w/ user/pass: admin / 2bffafdf-28e1-47c9-a3c7-9ca93750beb8\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 19:10:25 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40914
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.87352
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid cbf1113c-b233-4058-9077-473df27187c8
setting min_mon_release = octopus
epoch 0
fsid cbf1113c-b233-4058-9077-473df27187c8
last_changed 2021-05-15T19:10:06.342383-0700
created 2021-05-15T19:10:06.342383-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40914/0,v1:10.10.1.2:40915/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.87352 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 0489f0ce-b9ae-41ac-9a5b-948c23d170eb
0
start osd.0
add osd1 0093967a-b832-478e-9979-fd10f5c9ef7a
1
start osd.1
add osd2 8a34bd86-1cc2-4c0c-b11d-4f79edd30605
2
start osd.2


restful urls: https://10.10.1.2:42914
  w/ user/pass: admin / 2bffafdf-28e1-47c9-a3c7-9ca93750beb8


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T19:09:38.146-0700 7f044ca2c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:09:38.146-0700 7f044ca2c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:09:38.162-0700 7fde225441c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:09:38.162-0700 7fde225441c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40914,v1:10.10.1.2:40915] --print /tmp/ceph_monmap.87352 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.87352 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.87352 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42914 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x56058820a000 @  0x7f6bc90de680 0x7f6bc90ff824 0x7f6bc989a187 0x7f6bc98a2355 0x7f6bc989a708 0x7f6bc989a877 0x7f6bc989bc24 0x7f6bc98b3ec1 0x7f6bc98265f3 0x7f6bc9887e97 0x7f6bc988fb1a 0x7f6bc8f92d84 0x7f6bc90ae609 0x7f6bc8c82293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.PT3pPxRngB 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0489f0ce-b9ae-41ac-9a5b-948c23d170eb -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAIf6BgT0pyMRAA5nOG9AkpBry5//80e5dBJQ== --osd-uuid 0489f0ce-b9ae-41ac-9a5b-948c23d170eb 
2021-05-15T19:10:17.458-0700 7fdaed71bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T19:10:17.478-0700 7fdaed71bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T19:10:17.478-0700 7fdaed71bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b8a08de000 @  0x7fdaee0e4680 0x7fdaee105824 0x55b89504b447 0x55b8950534b5 0x55b89504b9c8 0x55b89504bb37 0x55b89504cee4 0x55b894e1dca1 0x55b894ff5423 0x55b894e0e2a7 0x55b894e1353a 0x7fdaedc37d84 0x7fdaeddbc609 0x7fdaed925293
2021-05-15T19:10:17.782-0700 7fdaed71bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0093967a-b832-478e-9979-fd10f5c9ef7a -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55b07ebe6000 @  0x7fcc03380680 0x7fcc033a1824 0x55b073945447 0x55b07394d4b5 0x55b0739459c8 0x55b073945b37 0x55b073946ee4 0x55b073717ca1 0x55b0738ef423 0x55b0737082a7 0x55b07370d53a 0x7fcc02ed3d84 0x7fcc03058609 0x7fcc02bc1293
2021-05-15T19:10:18.414-0700 7fcc029b7f00 -1 Falling back to public interface
2021-05-15T19:10:18.670-0700 7fcc029b7f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAKf6Bgrd7PBhAAsWxM87Ug0ylGRrcoEbpEzQ== --osd-uuid 0093967a-b832-478e-9979-fd10f5c9ef7a 
2021-05-15T19:10:18.770-0700 7fd617993f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T19:10:18.794-0700 7fd617993f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T19:10:18.794-0700 7fd617993f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5646f7afe000 @  0x7fd61835c680 0x7fd61837d824 0x5646eda4b447 0x5646eda534b5 0x5646eda4b9c8 0x5646eda4bb37 0x5646eda4cee4 0x5646ed81dca1 0x5646ed9f5423 0x5646ed80e2a7 0x5646ed81353a 0x7fd617eafd84 0x7fd618034609 0x7fd617b9d293
2021-05-15T19:10:19.158-0700 7fd617993f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8a34bd86-1cc2-4c0c-b11d-4f79edd30605 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55c8edc42000 @  0x7f1aabd53680 0x7f1aabd74824 0x55c8e36ce447 0x55c8e36d64b5 0x55c8e36ce9c8 0x55c8e36ceb37 0x55c8e36cfee4 0x55c8e34a0ca1 0x55c8e3678423 0x55c8e34912a7 0x55c8e349653a 0x7f1aab8a6d84 0x7f1aaba2b609 0x7f1aab594293
2021-05-15T19:10:19.834-0700 7f1aab38af00 -1 Falling back to public interface
2021-05-15T19:10:20.094-0700 7f1aab38af00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQALf6Bgi25iHxAApKMPFJ77ugdRtbw78xkv4Q== --osd-uuid 8a34bd86-1cc2-4c0c-b11d-4f79edd30605 
2021-05-15T19:10:20.206-0700 7f1c3b258f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T19:10:20.230-0700 7f1c3b258f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T19:10:20.230-0700 7f1c3b258f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55608115e000 @  0x7f1c3bc21680 0x7f1c3bc42824 0x556075366447 0x55607536e4b5 0x5560753669c8 0x556075366b37 0x556075367ee4 0x556075138ca1 0x556075310423 0x5560751292a7 0x55607512e53a 0x7f1c3b774d84 0x7f1c3b8f9609 0x7f1c3b462293
2021-05-15T19:10:20.546-0700 7f1c3b258f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55e659dba000 @  0x7f88fa2c6680 0x7f88fa2e7824 0x55e64eba4447 0x55e64ebac4b5 0x55e64eba49c8 0x55e64eba4b37 0x55e64eba5ee4 0x55e64e976ca1 0x55e64eb4e423 0x55e64e9672a7 0x55e64e96c53a 0x7f88f9e19d84 0x7f88f9f9e609 0x7f88f9b07293
2021-05-15T19:10:21.198-0700 7f88f98fdf00 -1 Falling back to public interface
2021-05-15T19:10:21.470-0700 7f88f98fdf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T19:10:25,558600333-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T19:10:25,582573954-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T19:10:25,667336389-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:10:25,673676560-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T19:10:29,624070030-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:10:29,630554825-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T19:10:33,599372632-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:10:33,605747353-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T19:10:37,593249656-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:10:37,600767977-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T19:10:45,468036241-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:10:45,474575646-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T19:10:49,438502381-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:10:49,445090904-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T19:10:54,074254511-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:10:54,080567572-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T19:10:58,342994471-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:10:58,349311283-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T19:11:03,226750303-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:11:03,233420740-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T19:11:07,524479276-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:11:07,530760096-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T19:11:12,103094468-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:11:12,109682420-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T19:11:16,009439644-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:11:16,015914672-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  172 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.08   89      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   90      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.96   80      up          osd.2  
                       TOTAL  300 GiB  173 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.08  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T19:11:19,836909145-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:11:43,915369655-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:11:53,003063362-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:02,319752403-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:11,590751112-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:20,775847546-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:20,798056834-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:29,910820568-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:29,933265725-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:39,068337717-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:39,090558300-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:48,314954869-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:48,337354463-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:57,455785837-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:57,477988611-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:57,496416104-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:12:57,507107260-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:12:57,532446759-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=996894
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T19:12:57,552655104-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-15T19:12:57,595745344-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:12:57,602423879-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:12:59.132+0000 ffff843c7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:12:59.952+0000 ffff843c7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:12:59.956+0000 ffff843c7010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:12:59.984494+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-16T02:12:59.984609+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-16T02:13:03.908781+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:13:03.908781+0000     0       0         0         0         0         0           -           0
2021-05-16T02:13:04.909026+0000     1      28        28         0         0         0           -           0
2021-05-16T02:13:05.909279+0000     2      57        57         0         0         0           -           0
2021-05-16T02:13:06.909514+0000     3      84        84         0         0         0           -           0
2021-05-16T02:13:07.909708+0000     4     111       111         0         0         0           -           0
2021-05-16T02:13:08.909900+0000     5     139       139         0         0         0           -           0
2021-05-16T02:13:09.910107+0000     6     166       166         0         0         0           -           0
2021-05-16T02:13:10.910331+0000     7     195       195         0         0         0           -           0
2021-05-16T02:13:11.910615+0000     8     221       221         0         0         0           -           0
2021-05-16T02:13:12.910872+0000     9     250       250         0         0         0           -           0
2021-05-16T02:13:13.911149+0000    10     255       278        23   36.7919      36.8      9.2066     9.21815
2021-05-16T02:13:14.911468+0000    11     255       304        49   71.2564       416     9.31527     9.23327
2021-05-16T02:13:15.911721+0000    12     255       332        77   102.643       448     9.30023     9.26081
2021-05-16T02:13:16.912036+0000    13     255       360       105     129.2       448     9.26761     9.26608
2021-05-16T02:13:17.912281+0000    14     255       388       133   151.964       448     9.26297     9.26695
2021-05-16T02:13:18.912548+0000    15     255       417       162   172.759       464      9.2232     9.26418
2021-05-16T02:13:19.912827+0000    16     255       445       190   189.954       448     9.19092     9.25392
2021-05-16T02:13:20.913070+0000    17     255       471       216   203.245       416     9.23063      9.2474
2021-05-16T02:13:21.913320+0000    18     255       497       242   215.059       416     9.30661     9.25062
2021-05-16T02:13:22.913615+0000    19     255       524       269   226.471       432     9.37295     9.25835
2021-05-16T02:13:23.913897+0000 min lat: 0.104551 max lat: 9.46904 avg lat: 7.2095
2021-05-16T02:13:23.913897+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:13:23.913897+0000    20       2       550       548   438.292      4464    0.104551      7.2095
2021-05-16T02:13:24.914189+0000 Total time run:         20.072
Total writes made:      550
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     438.421
Stddev Bandwidth:       975.855
Max bandwidth (MB/sec): 4464
Min bandwidth (MB/sec): 0
Average IOPS:           27
Stddev IOPS:            60.9972
Max IOPS:               279
Min IOPS:               0
Average Latency(s):     7.18366
Stddev Latency(s):      2.90252
Max latency(s):         9.46904
Min latency(s):         0.101565

[1;32mlocalhost.localdomain	[2021-05-15T19:13:26,082457218-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 996894


[1;33mlocalhost.localdomain	[2021-05-15T19:13:26,093730608-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:13:50,361227253-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:13:50,383755114-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:13:59,726856070-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:13:59,749429433-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:14:08,870021737-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:14:08,892694927-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:14:17,919015629-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:14:17,941962645-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:14:27,079664597-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:14:27,103543393-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:14:27,124768052-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:14:27,140683028-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:14:27,169419794-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=1000497
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-15T19:14:27,191898099-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-15T19:14:27,241347156-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:14:27,248449288-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '63d517e1-a002-4d16-b39e-f0f0fcc439ae', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 63d517e1-a002-4d16-b39e-f0f0fcc439ae --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XzMHmj:/tmp/ceph-asok.XzMHmj -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:14:28.855+0000 ffff8d2b9010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:14:29.651+0000 ffff8d2b9010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:14:29.651+0000 ffff8d2b9010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:14:29.688850+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:14:29.688850+0000     0       0         0         0         0         0           -           0
2021-05-16T02:14:30.689196+0000     1      94        94         0         0         0           -           0
2021-05-16T02:14:31.689827+0000     2     178       178         0         0         0           -           0
2021-05-16T02:14:32.692050+0000     3     255       262         7   37.2912   37.3333     2.99593     2.95597
2021-05-16T02:14:33.692360+0000     4     255       317        62   247.771       880     3.36894     3.15434
2021-05-16T02:14:34.699331+0000     5     255       369       114   364.023       832     3.78896     3.34992
2021-05-16T02:14:35.699572+0000     6     255       424       169   449.849       880      4.1128     3.54753
2021-05-16T02:14:36.699948+0000     7     255       475       220   502.048       816     4.51435     3.72896
2021-05-16T02:14:37.700712+0000     8     255       531       276    551.17       896     4.77159     3.92119
2021-05-16T02:14:38.805584+0000     9     128       550       422   740.601      2336     3.14743     3.97146
2021-05-16T02:14:39.805835+0000 Total time run:       9.85571
Total reads made:     550
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   892.884
Average IOPS:         55
Stddev IOPS:          45.401
Max IOPS:             146
Min IOPS:             0
Average Latency(s):   3.58429
Max latency(s):       4.79762
Min latency(s):       1.48546

[1;32mlocalhost.localdomain	[2021-05-15T19:14:41,042604707-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 1000497


[1;33mlocalhost.localdomain	[2021-05-15T19:14:41,054990986-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:15:05,161743583-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:15:05,185055519-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:15:14,214515828-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:15:14,237621808-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:15:23,166128401-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:15:23,189045948-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:15:32,259381473-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:15:32,282490257-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:15:41,564629187-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:15:41,589549352-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:15:41,613321610-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T19:15:41,623631636-07:00][RUNNING][ROUND 3/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:15:41,635672569-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T19:15:41,656398358-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40853\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.88449\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6eaf3a3b-e88d-470c-8719-89752fbd511e\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 6eaf3a3b-e88d-470c-8719-89752fbd511e\nlast_changed 2021-05-15T19:16:11.368444-0700\ncreated 2021-05-15T19:16:11.368444-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40853/0,v1:10.10.1.2:40854/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.88449 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 b0a8296b-d743-4490-8c85-8a803323d924\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 f15c13d1-8674-4070-a73e-caeca5a816c3\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 80074338-cf26-4bc2-a71b-40b9878cdeab\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42853\n  w/ user/pass: admin / c01e4af0-df7a-455e-8459-46fc4256d039\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 19:16:30 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40853
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.88449
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6eaf3a3b-e88d-470c-8719-89752fbd511e
setting min_mon_release = octopus
epoch 0
fsid 6eaf3a3b-e88d-470c-8719-89752fbd511e
last_changed 2021-05-15T19:16:11.368444-0700
created 2021-05-15T19:16:11.368444-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40853/0,v1:10.10.1.2:40854/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.88449 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 b0a8296b-d743-4490-8c85-8a803323d924
0
start osd.0
add osd1 f15c13d1-8674-4070-a73e-caeca5a816c3
1
start osd.1
add osd2 80074338-cf26-4bc2-a71b-40b9878cdeab
2
start osd.2


restful urls: https://10.10.1.2:42853
  w/ user/pass: admin / c01e4af0-df7a-455e-8459-46fc4256d039


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T19:15:43.565-0700 7fc8ad2a81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:15:43.565-0700 7fc8ad2a81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:15:43.581-0700 7f1c9c65a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:15:43.581-0700 7f1c9c65a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40853,v1:10.10.1.2:40854] --print /tmp/ceph_monmap.88449 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.88449 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.88449 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42853 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55fc80f78000 @  0x7fcc643ad680 0x7fcc643ce824 0x7fcc64b69187 0x7fcc64b71355 0x7fcc64b69708 0x7fcc64b69877 0x7fcc64b6ac24 0x7fcc64b82ec1 0x7fcc64af55f3 0x7fcc64b56e97 0x7fcc64b5eb1a 0x7fcc64261d84 0x7fcc6437d609 0x7fcc63f51293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.oIVifpyFOG 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b0a8296b-d743-4490-8c85-8a803323d924 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB1gKBgZ/J4OBAA60XOcahPrfp/ZfRpZSmOvA== --osd-uuid b0a8296b-d743-4490-8c85-8a803323d924 
2021-05-15T19:16:22.581-0700 7ff6f40adf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T19:16:22.601-0700 7ff6f40adf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T19:16:22.601-0700 7ff6f40adf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x561233ee8000 @  0x7ff6f4a76680 0x7ff6f4a97824 0x56122854e447 0x5612285564b5 0x56122854e9c8 0x56122854eb37 0x56122854fee4 0x561228320ca1 0x5612284f8423 0x5612283112a7 0x56122831653a 0x7ff6f45c9d84 0x7ff6f474e609 0x7ff6f42b7293
2021-05-15T19:16:22.961-0700 7ff6f40adf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f15c13d1-8674-4070-a73e-caeca5a816c3 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x558ef84ba000 @  0x7f2754bd7680 0x7f2754bf8824 0x558eeca56447 0x558eeca5e4b5 0x558eeca569c8 0x558eeca56b37 0x558eeca57ee4 0x558eec828ca1 0x558eeca00423 0x558eec8192a7 0x558eec81e53a 0x7f275472ad84 0x7f27548af609 0x7f2754418293
2021-05-15T19:16:23.573-0700 7f275420ef00 -1 Falling back to public interface
2021-05-15T19:16:23.833-0700 7f275420ef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB3gKBg3w4+EBAABSwobAE5dp/C5Ra7S+CFYQ== --osd-uuid f15c13d1-8674-4070-a73e-caeca5a816c3 
2021-05-15T19:16:23.937-0700 7ff9f2c64f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T19:16:23.957-0700 7ff9f2c64f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T19:16:23.957-0700 7ff9f2c64f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x561ad1bc8000 @  0x7ff9f362d680 0x7ff9f364e824 0x561ac6134447 0x561ac613c4b5 0x561ac61349c8 0x561ac6134b37 0x561ac6135ee4 0x561ac5f06ca1 0x561ac60de423 0x561ac5ef72a7 0x561ac5efc53a 0x7ff9f3180d84 0x7ff9f3305609 0x7ff9f2e6e293
2021-05-15T19:16:24.257-0700 7ff9f2c64f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 80074338-cf26-4bc2-a71b-40b9878cdeab -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x562b3cbaa000 @  0x7f881e536680 0x7f881e557824 0x562b3203a447 0x562b320424b5 0x562b3203a9c8 0x562b3203ab37 0x562b3203bee4 0x562b31e0cca1 0x562b31fe4423 0x562b31dfd2a7 0x562b31e0253a 0x7f881e089d84 0x7f881e20e609 0x7f881dd77293
2021-05-15T19:16:24.893-0700 7f881db6df00 -1 Falling back to public interface
2021-05-15T19:16:25.165-0700 7f881db6df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB4gKBg+eVYIhAAutudVxYnoVNi+QZEs4mDiw== --osd-uuid 80074338-cf26-4bc2-a71b-40b9878cdeab 
2021-05-15T19:16:25.245-0700 7fa293d03f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T19:16:25.265-0700 7fa293d03f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T19:16:25.265-0700 7fa293d03f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5586de1a8000 @  0x7fa2946cc680 0x7fa2946ed824 0x5586d3bcf447 0x5586d3bd74b5 0x5586d3bcf9c8 0x5586d3bcfb37 0x5586d3bd0ee4 0x5586d39a1ca1 0x5586d3b79423 0x5586d39922a7 0x5586d399753a 0x7fa29421fd84 0x7fa2943a4609 0x7fa293f0d293
2021-05-15T19:16:25.577-0700 7fa293d03f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x562c713c4000 @  0x7f8e2bd49680 0x7f8e2bd6a824 0x562c65c34447 0x562c65c3c4b5 0x562c65c349c8 0x562c65c34b37 0x562c65c35ee4 0x562c65a06ca1 0x562c65bde423 0x562c659f72a7 0x562c659fc53a 0x7f8e2b89cd84 0x7f8e2ba21609 0x7f8e2b58a293
2021-05-15T19:16:26.189-0700 7f8e2b380f00 -1 Falling back to public interface
2021-05-15T19:16:26.453-0700 7f8e2b380f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T19:16:30,586917599-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T19:16:30,611469898-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T19:16:30,694991889-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:16:30,701184922-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T19:16:34,859480586-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:16:34,865999910-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T19:16:38,857073763-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:16:38,863629378-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T19:16:42,750852165-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:16:42,757303362-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T19:16:50,663596372-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:16:50,670193859-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T19:16:54,972980012-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:16:54,979300882-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T19:16:59,289692102-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:16:59,296300301-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T19:17:04,333610228-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:17:04,340196703-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T19:17:08,603502399-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:17:08,609907418-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T19:17:13,206637433-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:17:13,212927157-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T19:17:17,702077021-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:17:17,708431931-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T19:17:21,529241170-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:17:21,535745971-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  180 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  180 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   66 KiB   0 B   0 B   0 B  100 GiB     0  1.10   88      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.92   91      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  0.98   80      up          osd.2  
                       TOTAL  300 GiB  181 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.92/1.10  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T19:17:25,399247645-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:17:49,600248183-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:17:58,797967018-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:18:07,900596518-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:18:17,019184707-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:18:26,331272456-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   peering
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:18:26,354364864-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:18:35,504880455-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:18:35,527722335-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:18:44,634276862-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:18:44,657430525-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:18:53,822586097-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:18:53,846140508-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:19:03,047847277-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:19:03,070578407-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:19:03,089782088-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:19:03,100393721-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:19:03,126583184-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=1013629
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T19:19:03,147142482-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-15T19:19:03,189997461-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:19:03,196438019-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:19:04.899+0000 ffffb4ebf010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:19:05.719+0000 ffffb4ebf010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:19:05.719+0000 ffffb4ebf010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:19:05.751918+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-16T02:19:05.752027+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T02:19:09.720784+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:19:09.720784+0000     0       0         0         0         0         0           -           0
2021-05-16T02:19:10.721001+0000     1      28        28         0         0         0           -           0
2021-05-16T02:19:11.721224+0000     2      57        57         0         0         0           -           0
2021-05-16T02:19:12.721484+0000     3      85        85         0         0         0           -           0
2021-05-16T02:19:13.721740+0000     4     113       113         0         0         0           -           0
2021-05-16T02:19:14.722057+0000     5     140       140         0         0         0           -           0
2021-05-16T02:19:15.722313+0000     6     166       166         0         0         0           -           0
2021-05-16T02:19:16.722558+0000     7     195       195         0         0         0           -           0
2021-05-16T02:19:17.722803+0000     8     223       223         0         0         0           -           0
2021-05-16T02:19:18.723056+0000     9     252       252         0         0         0           -           0
2021-05-16T02:19:19.723342+0000    10     255       280        25   39.9903        40     9.15167     9.15323
2021-05-16T02:19:20.723541+0000    11     255       308        53   77.0726       448     9.14493     9.15667
2021-05-16T02:19:21.723769+0000    12     255       337        82   109.307       464     9.13078     9.15141
2021-05-16T02:19:22.724011+0000    13     255       365       110   135.352       448     9.13066     9.14554
2021-05-16T02:19:23.724283+0000    14     255       392       137   156.534       432     9.13301     9.14799
2021-05-16T02:19:24.724502+0000    15     255       421       166   177.024       464     9.04944     9.14058
2021-05-16T02:19:25.724696+0000    16     255       448       193   192.955       432     9.07963     9.12928
2021-05-16T02:19:26.724904+0000    17     255       476       221   207.951       448     9.12223     9.12537
2021-05-16T02:19:27.725166+0000    18     255       502       247   219.504       416     9.19114     9.12878
2021-05-16T02:19:28.725365+0000    19     255       530       275   231.525       448      9.2239     9.13769
2021-05-16T02:19:29.725651+0000 min lat: 9.04737 max lat: 9.2458 avg lat: 9.14428
2021-05-16T02:19:29.725651+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:19:29.725651+0000    20     255       558       303   242.343       448     9.21078     9.14428
2021-05-16T02:19:30.725917+0000 Total time run:         20.1129
Total writes made:      559
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     444.69
Stddev Bandwidth:       226.519
Max bandwidth (MB/sec): 464
Min bandwidth (MB/sec): 0
Average IOPS:           27
Stddev IOPS:            14.1793
Max IOPS:               29
Min IOPS:               0
Average Latency(s):     7.10102
Stddev Latency(s):      2.86831
Max latency(s):         9.25368
Min latency(s):         0.095727

[1;32mlocalhost.localdomain	[2021-05-15T19:19:31,898510195-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 1013629


[1;33mlocalhost.localdomain	[2021-05-15T19:19:31,910021547-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:19:55,974920727-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 560 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:19:55,998267771-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:20:05,161700411-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 560 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:20:05,184831357-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:20:14,536831426-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 560 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:20:14,560124100-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:20:23,635915770-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 560 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:20:23,658972603-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:20:32,768749826-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 560 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:20:32,792083396-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:20:32,811406735-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:20:32,822399132-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:20:32,848392703-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=1017315
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-15T19:20:32,869241713-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-15T19:20:32,911280036-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:20:32,917793611-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4240bd31-c6af-4cf2-bf1b-6e19a4003d14', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4240bd31-c6af-4cf2-bf1b-6e19a4003d14 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.oTG10V:/tmp/ceph-asok.oTG10V -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:20:34.506+0000 ffff98428010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:20:35.330+0000 ffff98428010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:20:35.330+0000 ffff98428010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:20:35.369667+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:20:35.369667+0000     0       0         0         0         0         0           -           0
2021-05-16T02:20:36.369919+0000     1      88        88         0         0         0           -           0
2021-05-16T02:20:37.370153+0000     2     178       178         0         0         0           -           0
2021-05-16T02:20:38.382171+0000     3     255       260         5    26.554   26.6667     3.00327     2.96505
2021-05-16T02:20:39.385299+0000     4     255       316        61   243.036       896     3.35437     3.16173
2021-05-16T02:20:40.385537+0000     5     255       372       117   373.199       896     3.65777     3.32773
2021-05-16T02:20:41.390825+0000     6     255       431       176   467.667       944     4.04547     3.50403
2021-05-16T02:20:42.391111+0000     7     255       487       232   528.649       896     4.37294     3.67013
2021-05-16T02:20:43.391417+0000     8     255       546       291   580.406       944     4.46557       3.831
2021-05-16T02:20:44.506302+0000     9     103       559       456   798.524      2640      2.6805     3.76423
2021-05-16T02:20:45.506591+0000 Total time run:       9.71873
Total reads made:     559
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   920.285
Average IOPS:         57
Stddev IOPS:          51.1072
Max IOPS:             165
Min IOPS:             0
Average Latency(s):   3.45367
Max latency(s):       4.50018
Min latency(s):       1.49859

[1;32mlocalhost.localdomain	[2021-05-15T19:20:46,684956708-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 1017315


[1;33mlocalhost.localdomain	[2021-05-15T19:20:46,696461908-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:21:10,728524309-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 560 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:21:10,751895654-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:21:19,839040336-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 560 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:21:19,862226910-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:21:29,371780983-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 560 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:21:29,395215864-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:21:38,572502114-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 560 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:21:38,595810538-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:21:47,575702223-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 560 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:21:47,598831921-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:21:47,618217248-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T19:21:47,625732755-07:00][RUNNING][ROUND 4/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:21:47,636526517-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T19:21:47,655865891-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40929\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.89986\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 4bb1a208-7025-434a-964b-ebd7cc9e8765\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 4bb1a208-7025-434a-964b-ebd7cc9e8765\nlast_changed 2021-05-15T19:22:17.384322-0700\ncreated 2021-05-15T19:22:17.384322-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40929/0,v1:10.10.1.2:40930/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.89986 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 42dd4051-8ab7-4607-9931-354cf382baaf\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 c3df917f-9c69-4c90-be05-a59707bf5d7f\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 65de3204-029d-4b62-b7c2-549f65c28498\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42929\n  w/ user/pass: admin / 5dfb694b-67b1-43ca-91c6-6bd6d08a609d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 19:22:36 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40929
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.89986
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 4bb1a208-7025-434a-964b-ebd7cc9e8765
setting min_mon_release = octopus
epoch 0
fsid 4bb1a208-7025-434a-964b-ebd7cc9e8765
last_changed 2021-05-15T19:22:17.384322-0700
created 2021-05-15T19:22:17.384322-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40929/0,v1:10.10.1.2:40930/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.89986 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 42dd4051-8ab7-4607-9931-354cf382baaf
0
start osd.0
add osd1 c3df917f-9c69-4c90-be05-a59707bf5d7f
1
start osd.1
add osd2 65de3204-029d-4b62-b7c2-549f65c28498
2
start osd.2


restful urls: https://10.10.1.2:42929
  w/ user/pass: admin / 5dfb694b-67b1-43ca-91c6-6bd6d08a609d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T19:21:49.592-0700 7fa92d95c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:21:49.592-0700 7fa92d95c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:21:49.608-0700 7f0629ea91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:21:49.608-0700 7f0629ea91c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40929,v1:10.10.1.2:40930] --print /tmp/ceph_monmap.89986 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.89986 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.89986 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42929 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x564e54246000 @  0x7fc196c0d680 0x7fc196c2e824 0x7fc1973c9187 0x7fc1973d1355 0x7fc1973c9708 0x7fc1973c9877 0x7fc1973cac24 0x7fc1973e2ec1 0x7fc1973555f3 0x7fc1973b6e97 0x7fc1973beb1a 0x7fc196ac1d84 0x7fc196bdd609 0x7fc1967b1293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.BRlGSu4yki 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 42dd4051-8ab7-4607-9931-354cf382baaf -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDjgaBg43drMhAA9eieho868p0jT0CRoJ8HEg== --osd-uuid 42dd4051-8ab7-4607-9931-354cf382baaf 
2021-05-15T19:22:28.485-0700 7f033255af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T19:22:28.505-0700 7f033255af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T19:22:28.505-0700 7f033255af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55c584172000 @  0x7f0332f23680 0x7f0332f44824 0x55c579744447 0x55c57974c4b5 0x55c5797449c8 0x55c579744b37 0x55c579745ee4 0x55c579516ca1 0x55c5796ee423 0x55c5795072a7 0x55c57950c53a 0x7f0332a76d84 0x7f0332bfb609 0x7f0332764293
2021-05-15T19:22:28.821-0700 7f033255af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c3df917f-9c69-4c90-be05-a59707bf5d7f -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55aca14be000 @  0x7f5406ff6680 0x7f5407017824 0x55ac95ede447 0x55ac95ee64b5 0x55ac95ede9c8 0x55ac95edeb37 0x55ac95edfee4 0x55ac95cb0ca1 0x55ac95e88423 0x55ac95ca12a7 0x55ac95ca653a 0x7f5406b49d84 0x7f5406cce609 0x7f5406837293
2021-05-15T19:22:29.445-0700 7f540662df00 -1 Falling back to public interface
2021-05-15T19:22:29.705-0700 7f540662df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDlgaBgbCN3CBAAz5DntllFFibD/uxOOhVjGg== --osd-uuid c3df917f-9c69-4c90-be05-a59707bf5d7f 
2021-05-15T19:22:29.789-0700 7f86b8491f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T19:22:29.813-0700 7f86b8491f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T19:22:29.813-0700 7f86b8491f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5594c58e8000 @  0x7f86b8e5a680 0x7f86b8e7b824 0x5594bb871447 0x5594bb8794b5 0x5594bb8719c8 0x5594bb871b37 0x5594bb872ee4 0x5594bb643ca1 0x5594bb81b423 0x5594bb6342a7 0x5594bb63953a 0x7f86b89add84 0x7f86b8b32609 0x7f86b869b293
2021-05-15T19:22:30.121-0700 7f86b8491f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 65de3204-029d-4b62-b7c2-549f65c28498 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x560df54e2000 @  0x7f6c555a3680 0x7f6c555c4824 0x560de9b71447 0x560de9b794b5 0x560de9b719c8 0x560de9b71b37 0x560de9b72ee4 0x560de9943ca1 0x560de9b1b423 0x560de99342a7 0x560de993953a 0x7f6c550f6d84 0x7f6c5527b609 0x7f6c54de4293
2021-05-15T19:22:30.749-0700 7f6c54bdaf00 -1 Falling back to public interface
2021-05-15T19:22:31.001-0700 7f6c54bdaf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDmgaBg+h0yGhAASzqS6nncR1kLmQMCHjFMGA== --osd-uuid 65de3204-029d-4b62-b7c2-549f65c28498 
2021-05-15T19:22:31.101-0700 7fbb3836bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T19:22:31.121-0700 7fbb3836bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T19:22:31.121-0700 7fbb3836bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5584d4e52000 @  0x7fbb38d34680 0x7fbb38d55824 0x5584cae21447 0x5584cae294b5 0x5584cae219c8 0x5584cae21b37 0x5584cae22ee4 0x5584cabf3ca1 0x5584cadcb423 0x5584cabe42a7 0x5584cabe953a 0x7fbb38887d84 0x7fbb38a0c609 0x7fbb38575293
2021-05-15T19:22:31.445-0700 7fbb3836bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x561c452c6000 @  0x7fa62ddd3680 0x7fa62ddf4824 0x561c3ac15447 0x561c3ac1d4b5 0x561c3ac159c8 0x561c3ac15b37 0x561c3ac16ee4 0x561c3a9e7ca1 0x561c3abbf423 0x561c3a9d82a7 0x561c3a9dd53a 0x7fa62d926d84 0x7fa62daab609 0x7fa62d614293
2021-05-15T19:22:32.101-0700 7fa62d40af00 -1 Falling back to public interface
2021-05-15T19:22:32.373-0700 7fa62d40af00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T19:22:36,503116889-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T19:22:36,527763766-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T19:22:36,614314878-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:22:36,620708972-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T19:22:40,581034930-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:22:40,587362518-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T19:22:44,462766852-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:22:44,469120799-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T19:22:48,390629220-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:22:48,397073107-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T19:22:56,647812021-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:22:56,654416541-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T19:23:01,561246764-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:23:01,567994453-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T19:23:06,026762465-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:23:06,033327498-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T19:23:10,704212452-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:23:10,710669871-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T19:23:15,235424200-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:23:15,241826890-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T19:23:19,635933753-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:23:19,642252893-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T19:23:24,188386754-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:23:24,194712541-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T19:23:28,092422658-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:23:28,098808485-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  190 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  190 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  0.98  154      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   69 KiB   0 B   0 B   0 B  100 GiB     0  1.09  151      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  0.93  143      up          osd.2  
                       TOTAL  300 GiB  192 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.93/1.09  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T19:23:31,833153772-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:23:55,952182089-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:24:05,034233070-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:24:14,614552625-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:24:23,733885913-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:24:32,840549059-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:24:32,863745229-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:24:42,060255441-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:24:42,083469435-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:24:51,172187365-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:24:51,195735975-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:25:00,311088521-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:25:00,334231943-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:25:09,450795235-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:25:09,474013654-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:25:09,493328526-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:25:09,504417281-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:25:09,530670329-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=1030307
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T19:25:09,551450981-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-15T19:25:09,594419361-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:25:09,600966150-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:25:11.185+0000 ffffa4af3010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:25:11.989+0000 ffffa4af3010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:25:11.989+0000 ffffa4af3010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:25:12.021993+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-16T02:25:12.022106+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T02:25:16.047717+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:25:16.047717+0000     0       0         0         0         0         0           -           0
2021-05-16T02:25:17.047995+0000     1      28        28         0         0         0           -           0
2021-05-16T02:25:18.048248+0000     2      56        56         0         0         0           -           0
2021-05-16T02:25:19.048462+0000     3      84        84         0         0         0           -           0
2021-05-16T02:25:20.048756+0000     4     112       112         0         0         0           -           0
2021-05-16T02:25:21.049030+0000     5     139       139         0         0         0           -           0
2021-05-16T02:25:22.049265+0000     6     165       165         0         0         0           -           0
2021-05-16T02:25:23.049521+0000     7     193       193         0         0         0           -           0
2021-05-16T02:25:24.049757+0000     8     221       221         0         0         0           -           0
2021-05-16T02:25:25.049987+0000     9     248       248         0         0         0           -           0
2021-05-16T02:25:26.050189+0000    10     255       272        17   27.1938      27.2     9.36816     9.34542
2021-05-16T02:25:27.050413+0000    11     255       300        45   65.4395       448     9.44016     9.40669
2021-05-16T02:25:28.050689+0000    12     255       328        73   97.3106       448      9.4544     9.42187
2021-05-16T02:25:29.050936+0000    13     255       354        99   121.818       416     9.46921     9.43096
2021-05-16T02:25:30.051132+0000    14     255       381       126   143.967       432     9.49489     9.44522
2021-05-16T02:25:31.051343+0000    15     255       407       152   162.096       416     9.51459     9.46119
2021-05-16T02:25:32.051567+0000    16     255       435       180   179.959       448     9.52074     9.46913
2021-05-16T02:25:33.051839+0000    17     255       460       205   192.896       400      9.6142     9.47852
2021-05-16T02:25:34.052077+0000    18     255       485       230   204.397       400     9.68035     9.49741
2021-05-16T02:25:35.052297+0000    19     255       512       257   216.371       432     9.70074      9.5203
2021-05-16T02:25:36.052544+0000 min lat: 0.107058 max lat: 9.74988 avg lat: 7.33798
2021-05-16T02:25:36.052544+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:25:36.052544+0000    20       2       540       538     430.3      4496    0.107058     7.33798
2021-05-16T02:25:37.052848+0000 Total time run:         20.0713
Total writes made:      540
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     430.465
Stddev Bandwidth:       982.989
Max bandwidth (MB/sec): 4496
Min bandwidth (MB/sec): 0
Average IOPS:           26
Stddev IOPS:            61.4516
Max IOPS:               281
Min IOPS:               0
Average Latency(s):     7.3112
Stddev Latency(s):      3.03114
Max latency(s):         9.74988
Min latency(s):         0.105543

[1;32mlocalhost.localdomain	[2021-05-15T19:25:38,274456977-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 1030307


[1;33mlocalhost.localdomain	[2021-05-15T19:25:38,286011952-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:26:02,534665656-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 541 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:26:02,559658878-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:26:11,786807915-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 541 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:26:11,809964943-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:26:20,738405855-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 541 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:26:20,761491905-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:26:29,843687453-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 541 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:26:29,866830362-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:26:39,022175355-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 541 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:26:39,045578709-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:26:39,064908552-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:26:39,076019960-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:26:39,102564116-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=1033934
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-15T19:26:39,123799480-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-15T19:26:39,165243087-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:26:39,171732551-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a1fe5d1a-9ef2-4d27-81f2-fa463c962ba3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6QPRot:/tmp/ceph-asok.6QPRot -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:26:40.813+0000 ffff8116c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:26:41.625+0000 ffff8116c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:26:41.625+0000 ffff8116c010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:26:41.660363+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:26:41.660363+0000     0       0         0         0         0         0           -           0
2021-05-16T02:26:42.660607+0000     1      95        95         0         0         0           -           0
2021-05-16T02:26:43.660881+0000     2     184       184         0         0         0           -           0
2021-05-16T02:26:44.664908+0000     3     255       272        17   90.5234   90.6667     2.86768      2.8347
2021-05-16T02:26:45.665275+0000     4     255       327        72   287.632       880     3.25651     3.01041
2021-05-16T02:26:46.667895+0000     5     255       380       125   399.382       848     3.66845     3.20219
2021-05-16T02:26:47.678081+0000     6     255       437       182   483.888       912     4.04022     3.41174
2021-05-16T02:26:48.679067+0000     7     255       493       238   542.535       896       4.455     3.61031
2021-05-16T02:26:49.702935+0000     8     230       540       310   616.703      1152     4.39063     3.82978
2021-05-16T02:26:50.704014+0000     9      57       540       483   854.503      2768     2.19144     3.63193
2021-05-16T02:26:51.704266+0000 Total time run:       9.36628
Total reads made:     540
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   922.458
Average IOPS:         57
Stddev IOPS:          53.2776
Max IOPS:             173
Min IOPS:             0
Average Latency(s):   3.44478
Max latency(s):       4.62608
Min latency(s):       1.52863

[1;32mlocalhost.localdomain	[2021-05-15T19:26:52,931980624-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 1033934


[1;33mlocalhost.localdomain	[2021-05-15T19:26:52,943878977-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:27:17,080252530-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 541 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:27:17,103680653-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:27:26,386565974-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 541 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:27:26,409958000-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:27:35,706249710-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 541 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:27:35,729345331-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:27:44,673719169-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 541 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:27:44,696558456-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:27:53,979514634-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 541 objects, 8.4 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:27:54,005409163-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:27:54,026482813-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T19:27:54,034410017-07:00][RUNNING][ROUND 5/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:27:54,046490651-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T19:27:54,066229568-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40587\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.91088\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b6733e91-48ab-47eb-b34c-1b3924a54e39\nsetting min_mon_release = octopus\nepoch 0\nfsid b6733e91-48ab-47eb-b34c-1b3924a54e39\nlast_changed 2021-05-15T19:28:24.499766-0700\ncreated 2021-05-15T19:28:24.499766-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40587/0,v1:10.10.1.2:40588/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.91088 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 d311bb8d-1755-492e-ab0c-fa0007646148\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 047b05b1-7909-446d-a390-f4aed9a3059d\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 df17c952-7c08-492e-a668-2ab60e96e186\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42587\n  w/ user/pass: admin / 4ea8a299-677c-4fc2-8700-7d138b515563\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 19:28:43 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40587
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.91088
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b6733e91-48ab-47eb-b34c-1b3924a54e39
setting min_mon_release = octopus
epoch 0
fsid b6733e91-48ab-47eb-b34c-1b3924a54e39
last_changed 2021-05-15T19:28:24.499766-0700
created 2021-05-15T19:28:24.499766-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40587/0,v1:10.10.1.2:40588/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.91088 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 d311bb8d-1755-492e-ab0c-fa0007646148
0
start osd.0
add osd1 047b05b1-7909-446d-a390-f4aed9a3059d
1
start osd.1
add osd2 df17c952-7c08-492e-a668-2ab60e96e186
2
start osd.2


restful urls: https://10.10.1.2:42587
  w/ user/pass: admin / 4ea8a299-677c-4fc2-8700-7d138b515563


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T19:27:55.956-0700 7f609960a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:27:55.956-0700 7f609960a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:27:55.972-0700 7ff6124481c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:27:55.972-0700 7ff6124481c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40587,v1:10.10.1.2:40588] --print /tmp/ceph_monmap.91088 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.91088 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.91088 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42587 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5601e9846000 @  0x7f8d1d570680 0x7f8d1d591824 0x7f8d1dd2c187 0x7f8d1dd34355 0x7f8d1dd2c708 0x7f8d1dd2c877 0x7f8d1dd2dc24 0x7f8d1dd45ec1 0x7f8d1dcb85f3 0x7f8d1dd19e97 0x7f8d1dd21b1a 0x7f8d1d424d84 0x7f8d1d540609 0x7f8d1d114293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.e7fcOq8Lsa 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d311bb8d-1755-492e-ab0c-fa0007646148 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBSg6Bgz61VOhAAtckov5KVT/e4KZkIOmO7Og== --osd-uuid d311bb8d-1755-492e-ab0c-fa0007646148 
2021-05-15T19:28:35.616-0700 7fa12a8d3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T19:28:35.640-0700 7fa12a8d3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T19:28:35.640-0700 7fa12a8d3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x563359cb8000 @  0x7fa12b29c680 0x7fa12b2bd824 0x56334f3dd447 0x56334f3e54b5 0x56334f3dd9c8 0x56334f3ddb37 0x56334f3deee4 0x56334f1afca1 0x56334f387423 0x56334f1a02a7 0x56334f1a553a 0x7fa12adefd84 0x7fa12af74609 0x7fa12aadd293
2021-05-15T19:28:35.952-0700 7fa12a8d3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 047b05b1-7909-446d-a390-f4aed9a3059d -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55cd5d5ca000 @  0x7f601c240680 0x7f601c261824 0x55cd522c7447 0x55cd522cf4b5 0x55cd522c79c8 0x55cd522c7b37 0x55cd522c8ee4 0x55cd52099ca1 0x55cd52271423 0x55cd5208a2a7 0x55cd5208f53a 0x7f601bd93d84 0x7f601bf18609 0x7f601ba81293
2021-05-15T19:28:36.576-0700 7f601b877f00 -1 Falling back to public interface
2021-05-15T19:28:36.836-0700 7f601b877f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBUg6BgQZlkEBAAMQmXTkUrxKJznbP+AyQTbg== --osd-uuid 047b05b1-7909-446d-a390-f4aed9a3059d 
2021-05-15T19:28:36.916-0700 7fa100830f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T19:28:36.936-0700 7fa100830f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T19:28:36.936-0700 7fa100830f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x560361d12000 @  0x7fa1011f9680 0x7fa10121a824 0x560356f2d447 0x560356f354b5 0x560356f2d9c8 0x560356f2db37 0x560356f2eee4 0x560356cffca1 0x560356ed7423 0x560356cf02a7 0x560356cf553a 0x7fa100d4cd84 0x7fa100ed1609 0x7fa100a3a293
2021-05-15T19:28:37.244-0700 7fa100830f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new df17c952-7c08-492e-a668-2ab60e96e186 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5624c5d20000 @  0x7f5fd7d66680 0x7f5fd7d87824 0x5624bb442447 0x5624bb44a4b5 0x5624bb4429c8 0x5624bb442b37 0x5624bb443ee4 0x5624bb214ca1 0x5624bb3ec423 0x5624bb2052a7 0x5624bb20a53a 0x7f5fd78b9d84 0x7f5fd7a3e609 0x7f5fd75a7293
2021-05-15T19:28:37.888-0700 7f5fd739df00 -1 Falling back to public interface
2021-05-15T19:28:38.140-0700 7f5fd739df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBVg6BgEZYUIxAAyQ4d8FVScaKFWBMKfIuAAQ== --osd-uuid df17c952-7c08-492e-a668-2ab60e96e186 
2021-05-15T19:28:38.248-0700 7f826037ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T19:28:38.268-0700 7f826037ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T19:28:38.268-0700 7f826037ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5600cdb08000 @  0x7f8260d48680 0x7f8260d69824 0x5600c2749447 0x5600c27514b5 0x5600c27499c8 0x5600c2749b37 0x5600c274aee4 0x5600c251bca1 0x5600c26f3423 0x5600c250c2a7 0x5600c251153a 0x7f826089bd84 0x7f8260a20609 0x7f8260589293
2021-05-15T19:28:38.640-0700 7f826037ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x557616e62000 @  0x7f29024fa680 0x7f290251b824 0x55760b42c447 0x55760b4344b5 0x55760b42c9c8 0x55760b42cb37 0x55760b42dee4 0x55760b1feca1 0x55760b3d6423 0x55760b1ef2a7 0x55760b1f453a 0x7f290204dd84 0x7f29021d2609 0x7f2901d3b293
2021-05-15T19:28:39.264-0700 7f2901b31f00 -1 Falling back to public interface
2021-05-15T19:28:39.540-0700 7f2901b31f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T19:28:43,518217610-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T19:28:43,543342642-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T19:28:43,627942632-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:28:43,634468072-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T19:28:47,750727767-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:28:47,757876638-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T19:28:51,526572081-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:28:51,532971184-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T19:28:55,315563162-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:28:55,321956249-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T19:29:03,145746074-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:29:03,152309635-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T19:29:07,266799528-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:29:07,273049356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T19:29:11,754148905-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:29:11,760619360-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T19:29:16,218424946-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:29:16,224720967-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T19:29:20,524772111-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:29:20,531311023-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T19:29:25,330057243-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:29:25,336388715-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T19:29:29,589570935-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:29:29,595966768-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T19:29:33,394752007-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:29:33,401176855-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  176 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.06   90      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.94   89      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   59 KiB   0 B   0 B   0 B  100 GiB     0  1.01   80      up          osd.2  
                       TOTAL  300 GiB  177 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.94/1.06  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T19:29:37,519822188-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:30:01,568826322-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:30:10,552230004-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:30:19,557124431-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:30:29,047114417-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:30:38,214002040-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:30:38,236906856-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:30:47,442099870-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:30:47,465279833-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:30:56,485879405-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:30:56,509051326-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:31:05,618799702-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:31:05,644438450-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:31:14,671750928-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:31:14,694626669-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:31:14,714061752-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:31:14,725107913-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:31:14,751342458-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=1047030
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T19:31:14,772645765-07:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-15T19:31:14,816241289-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:31:14,822636580-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:31:16.592+0000 ffffbd04a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:31:17.396+0000 ffffbd04a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:31:17.396+0000 ffffbd04a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:31:17.426507+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-16T02:31:17.426624+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-16T02:31:21.329960+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:31:21.329960+0000     0       0         0         0         0         0           -           0
2021-05-16T02:31:22.330203+0000     1      28        28         0         0         0           -           0
2021-05-16T02:31:23.330454+0000     2      56        56         0         0         0           -           0
2021-05-16T02:31:24.330724+0000     3      83        83         0         0         0           -           0
2021-05-16T02:31:25.330984+0000     4     111       111         0         0         0           -           0
2021-05-16T02:31:26.331247+0000     5     137       137         0         0         0           -           0
2021-05-16T02:31:27.331524+0000     6     166       166         0         0         0           -           0
2021-05-16T02:31:28.331738+0000     7     193       193         0         0         0           -           0
2021-05-16T02:31:29.331952+0000     8     222       222         0         0         0           -           0
2021-05-16T02:31:30.332195+0000     9     249       249         0         0         0           -           0
2021-05-16T02:31:31.332447+0000    10     255       274        19    30.393      30.4     9.39286     9.33935
2021-05-16T02:31:32.332676+0000    11     255       302        47   68.3478       448     9.37735      9.3619
2021-05-16T02:31:33.332936+0000    12     255       328        73   97.3106       416     9.41741     9.36945
2021-05-16T02:31:34.333161+0000    13     255       352        97   119.357       384      9.5233      9.3918
2021-05-16T02:31:35.333394+0000    14     255       380       125   142.824       448     9.50569     9.42144
2021-05-16T02:31:36.333719+0000    15     255       405       150   159.962       400     9.57251     9.43874
2021-05-16T02:31:37.333937+0000    16     255       432       177   176.958       432       9.639     9.46589
2021-05-16T02:31:38.334185+0000    17     255       460       205   192.895       448     9.62107     9.48539
2021-05-16T02:31:39.334451+0000    18     255       486       231   205.284       416     9.70447     9.50803
2021-05-16T02:31:40.334727+0000    19     255       513       258   217.211       432     9.69491     9.52696
2021-05-16T02:31:41.334961+0000 min lat: 9.26207 max lat: 9.73051 avg lat: 9.53625
2021-05-16T02:31:41.334961+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:31:41.334961+0000    20     255       541       286   228.745       448     9.61162     9.53625
2021-05-16T02:31:42.335291+0000 Total time run:         20.0951
Total writes made:      542
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     431.548
Stddev Bandwidth:       218.249
Max bandwidth (MB/sec): 448
Min bandwidth (MB/sec): 0
Average IOPS:           26
Stddev IOPS:            13.6821
Max IOPS:               28
Min IOPS:               0
Average Latency(s):     7.28818
Stddev Latency(s):      3.05595
Max latency(s):         9.73051
Min latency(s):         0.102102

[1;32mlocalhost.localdomain	[2021-05-15T19:31:43,498425383-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 1047030


[1;33mlocalhost.localdomain	[2021-05-15T19:31:43,510449411-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:32:07,455428644-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 543 objects, 8.5 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:32:07,478417548-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:32:16,593049787-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 543 objects, 8.5 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:32:16,618820497-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:32:25,842004039-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 543 objects, 8.5 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:32:25,865162337-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:32:34,947483309-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 543 objects, 8.5 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:32:34,971089674-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:32:43,992026860-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 543 objects, 8.5 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:32:44,015311495-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:32:44,035193999-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:32:44,046236559-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:32:44,073176733-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=1050731
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-15T19:32:44,094523382-07:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-15T19:32:44,136647633-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:32:44,142923460-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c325a149-f3be-41ec-a572-9d151b20f800', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c325a149-f3be-41ec-a572-9d151b20f800 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GrEMNL:/tmp/ceph-asok.GrEMNL -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:32:45.631+0000 ffff7f8fe010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:32:46.431+0000 ffff7f8fe010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:32:46.431+0000 ffff7f8fe010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:32:46.497118+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T02:32:46.497118+0000     0       0         0         0         0         0           -           0
2021-05-16T02:32:47.497355+0000     1      92        92         0         0         0           -           0
2021-05-16T02:32:48.497676+0000     2     182       182         0         0         0           -           0
2021-05-16T02:32:49.499820+0000     3     255       267        12   63.9382        64     2.92163     2.88387
2021-05-16T02:32:50.500103+0000     4     255       324        69    275.78       912     3.27792     3.05693
2021-05-16T02:32:51.500400+0000     5     255       378       123   393.326       864     3.63766     3.23674
2021-05-16T02:32:52.502303+0000     6     255       438       183   487.562       960     4.00344     3.43095
2021-05-16T02:32:53.502555+0000     7     255       495       240    548.13       912     4.35181     3.60827
2021-05-16T02:32:54.863514+0000     8     164       542       378   722.875      2208     3.39075     3.79957
2021-05-16T02:32:55.863763+0000 Total time run:       9.29284
Total reads made:     542
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   933.191
Average IOPS:         58
Stddev IOPS:          46.2316
Max IOPS:             138
Min IOPS:             0
Average Latency(s):   3.38724
Max latency(s):       4.51591
Min latency(s):       1.48577

[1;32mlocalhost.localdomain	[2021-05-15T19:32:57,082889799-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 1050731


[1;33mlocalhost.localdomain	[2021-05-15T19:32:57,094896903-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:33:21,348249013-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 543 objects, 8.5 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:33:21,371532433-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:33:30,437310164-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 543 objects, 8.5 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:33:30,460611377-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:33:39,575169941-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 543 objects, 8.5 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:33:39,598725161-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:33:48,725396244-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 543 objects, 8.5 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:33:48,748625639-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:33:57,773249099-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 543 objects, 8.5 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:33:57,797630796-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:33:57,819693232-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-15T19:33:57,837226185-07:00][RUNNING][ROUND 1/8/40] object_size=64MB[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:33:57,849862526-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-15T19:33:57,870716986-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40093\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.92175\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 4cc7f2ce-110b-430c-85df-d195426de131\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 4cc7f2ce-110b-430c-85df-d195426de131\nlast_changed 2021-05-15T19:34:27.387977-0700\ncreated 2021-05-15T19:34:27.387977-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40093/0,v1:10.10.1.2:40094/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.92175 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 10ee0edc-9134-4f04-a410-df7d6d6af983\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 2448676a-ec7d-4b8c-a06e-c08cfc2b58a0\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 7c800e0c-73ad-4dde-8f66-99df3dbd0200\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42093\n'
10.10.1.2: b'  w/ user/pass: admin / c209d0fe-6888-42bb-8dba-4ea212c61a61\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 19:34:46 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40093
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.92175
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 4cc7f2ce-110b-430c-85df-d195426de131
setting min_mon_release = octopus
epoch 0
fsid 4cc7f2ce-110b-430c-85df-d195426de131
last_changed 2021-05-15T19:34:27.387977-0700
created 2021-05-15T19:34:27.387977-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40093/0,v1:10.10.1.2:40094/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.92175 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 10ee0edc-9134-4f04-a410-df7d6d6af983
0
start osd.0
add osd1 2448676a-ec7d-4b8c-a06e-c08cfc2b58a0
1
start osd.1
add osd2 7c800e0c-73ad-4dde-8f66-99df3dbd0200
2
start osd.2


restful urls: https://10.10.1.2:42093
  w/ user/pass: admin / c209d0fe-6888-42bb-8dba-4ea212c61a61


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-15T19:33:59.779-0700 7f392ec7f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:33:59.783-0700 7f392ec7f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:33:59.799-0700 7fbc741cd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-15T19:33:59.799-0700 7fbc741cd1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40093,v1:10.10.1.2:40094] --print /tmp/ceph_monmap.92175 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.92175 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.92175 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42093 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x557d37d94000 @  0x7f8a49cef680 0x7f8a49d10824 0x7f8a4a4ab187 0x7f8a4a4b3355 0x7f8a4a4ab708 0x7f8a4a4ab877 0x7f8a4a4acc24 0x7f8a4a4c4ec1 0x7f8a4a4375f3 0x7f8a4a498e97 0x7f8a4a4a0b1a 0x7f8a49ba3d84 0x7f8a49cbf609 0x7f8a49893293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.SZN9os6oh7 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 10ee0edc-9134-4f04-a410-df7d6d6af983 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC9hKBgFLgXMRAA0xjzLuR0FB8/1oNjdL6VLw== --osd-uuid 10ee0edc-9134-4f04-a410-df7d6d6af983 
2021-05-15T19:34:38.480-0700 7fe487280f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T19:34:38.500-0700 7fe487280f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-15T19:34:38.500-0700 7fe487280f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x559902dfe000 @  0x7fe487c49680 0x7fe487c6a824 0x5598f83c2447 0x5598f83ca4b5 0x5598f83c29c8 0x5598f83c2b37 0x5598f83c3ee4 0x5598f8194ca1 0x5598f836c423 0x5598f81852a7 0x5598f818a53a 0x7fe48779cd84 0x7fe487921609 0x7fe48748a293
2021-05-15T19:34:38.824-0700 7fe487280f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2448676a-ec7d-4b8c-a06e-c08cfc2b58a0 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x561498b48000 @  0x7fe23ad49680 0x7fe23ad6a824 0x56148dde1447 0x56148dde94b5 0x56148dde19c8 0x56148dde1b37 0x56148dde2ee4 0x56148dbb3ca1 0x56148dd8b423 0x56148dba42a7 0x56148dba953a 0x7fe23a89cd84 0x7fe23aa21609 0x7fe23a58a293
2021-05-15T19:34:39.420-0700 7fe23a380f00 -1 Falling back to public interface
2021-05-15T19:34:39.684-0700 7fe23a380f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC/hKBgEldiBxAA0Cp41M47NT9hPKMk4j8b/g== --osd-uuid 2448676a-ec7d-4b8c-a06e-c08cfc2b58a0 
2021-05-15T19:34:39.784-0700 7f8148710f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T19:34:39.804-0700 7f8148710f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-15T19:34:39.804-0700 7f8148710f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x555cf6708000 @  0x7f81490d9680 0x7f81490fa824 0x555cec3dc447 0x555cec3e44b5 0x555cec3dc9c8 0x555cec3dcb37 0x555cec3ddee4 0x555cec1aeca1 0x555cec386423 0x555cec19f2a7 0x555cec1a453a 0x7f8148c2cd84 0x7f8148db1609 0x7f814891a293
2021-05-15T19:34:40.108-0700 7f8148710f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7c800e0c-73ad-4dde-8f66-99df3dbd0200 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55715f6fc000 @  0x7fb6067a4680 0x7fb6067c5824 0x55715576e447 0x5571557764b5 0x55715576e9c8 0x55715576eb37 0x55715576fee4 0x557155540ca1 0x557155718423 0x5571555312a7 0x55715553653a 0x7fb6062f7d84 0x7fb60647c609 0x7fb605fe5293
2021-05-15T19:34:40.760-0700 7fb605ddbf00 -1 Falling back to public interface
2021-05-15T19:34:41.020-0700 7fb605ddbf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDAhKBgOkFiGxAAke+t3qyMIq45r6UkZTxLVA== --osd-uuid 7c800e0c-73ad-4dde-8f66-99df3dbd0200 
2021-05-15T19:34:41.120-0700 7f82353c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T19:34:41.140-0700 7f82353c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-15T19:34:41.140-0700 7f82353c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5596c1816000 @  0x7f8235d8e680 0x7f8235daf824 0x5596b5bbd447 0x5596b5bc54b5 0x5596b5bbd9c8 0x5596b5bbdb37 0x5596b5bbeee4 0x5596b598fca1 0x5596b5b67423 0x5596b59802a7 0x5596b598553a 0x7f82358e1d84 0x7f8235a66609 0x7f82355cf293
2021-05-15T19:34:41.464-0700 7f82353c5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x56421bea4000 @  0x7f33ec6ee680 0x7f33ec70f824 0x5642117cc447 0x5642117d44b5 0x5642117cc9c8 0x5642117ccb37 0x5642117cdee4 0x56421159eca1 0x564211776423 0x56421158f2a7 0x56421159453a 0x7f33ec241d84 0x7f33ec3c6609 0x7f33ebf2f293
2021-05-15T19:34:42.076-0700 7f33ebd25f00 -1 Falling back to public interface
2021-05-15T19:34:42.344-0700 7f33ebd25f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-15T19:34:46,501665291-07:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_0/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0104/g' /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mlocalhost.localdomain	[2021-05-15T19:34:46,526689352-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-15T19:34:46,610684188-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:34:46,616986749-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-15T19:34:50,578307241-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:34:50,584575791-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-15T19:34:54,591834960-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:34:54,598131459-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-15T19:34:58,742938765-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:34:58,749631227-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-15T19:35:06,567800519-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:35:06,574093388-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-15T19:35:11,480628924-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:35:11,486935670-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-15T19:35:15,900215773-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:35:15,906551324-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-15T19:35:20,353884624-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:35:20,360220340-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-15T19:35:25,336497107-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:35:25,342667090-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-15T19:35:29,655234346-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:35:29,661739456-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-15T19:35:34,235985956-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:35:34,242338190-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 64 pgp_num_target 64 autoscale_mode on last_change 21 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-15T19:35:38,086865025-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:35:38,093347916-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  186 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  186 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   62 KiB   0 B   0 B   0 B  100 GiB     0  1.00  151      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   69 KiB   0 B   0 B   0 B  100 GiB     0  1.11  154      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  0.89  143      up          osd.2  
                       TOTAL  300 GiB  188 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.89/1.11  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-15T19:35:42,314488161-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:36:06,429915967-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:36:15,806381582-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:36:24,906566271-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:36:33,915608274-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:36:43,184285632-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:36:43,209315453-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:36:52,467309500-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:36:52,491298699-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:37:01,592767629-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:37:01,616440979-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:37:10,747571410-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:37:10,771433087-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:37:19,890993258-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-15T19:37:19,917144385-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-15T19:37:19,939997275-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-15T19:37:19,954738940-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-15T19:37:19,984475155-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=1063726
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-15T19:37:20,007829445-07:00] INFO: > Run rados bench[0m
# ./bench-rados:191 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64MB_write.log.1
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 67108864 -O 67108864 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mlocalhost.localdomain	[2021-05-15T19:37:20,058256905-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-15T19:37:20,064785794-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '560283fc-e79d-4da0-a9c8-bbb92ff84912', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '67108864', '-O', '67108864', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 560283fc-e79d-4da0-a9c8-bbb92ff84912 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vBKx92:/tmp/ceph-asok.vBKx92 -- rados bench 20 write --pool bench_rados -b 67108864 -O 67108864 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-16T02:37:21.674+0000 ffffb9c98010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:37:22.482+0000 ffffb9c98010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T02:37:22.482+0000 ffffb9c98010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T02:37:22.546582+0000 Maintaining 256 concurrent writes of 67108864 bytes to objects of size 67108864 for up to 20 seconds or 0 objects
2021-05-16T02:37:22.546700+0000 Object prefix: benchmark_data_localhost.localdomain_6
