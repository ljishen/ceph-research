#!/usr/bin/env bash
#
# Require bash version >= 4.4
#
# This script depends on passwordless ssh and passwordless sudo via
# the current user ($USER) on remote hosts.

set -euo pipefail

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )"/.. >/dev/null 2>&1 && pwd )"
readonly SCRIPT_DIR
# shellcheck source=../common.sh
. "$SCRIPT_DIR/common.sh"


usage() {
  cat <<EOF
Usage: $0 -m MON_IP -h OSD_HOST [-h OSD_HOST]... -s SCRIPT_FILE

Options:
  -m :         IP address for the clusterâ€™s monitor daemon.
  -h :         OSD host (e.g., -h host1 -h host2).
  -s :         the remote script (relative path to the remote_script/ceph_volume dir)
               for creating and querying ceph volumes on each host.

Defining the LVS_PROVISION=no can skip the logical volumes provisioning on hosts.

All SSH connections to the cluster hosts will use the user '\$USER' (current: $USER).

EOF
}

if (( $# < 2 )); then
  usage
  exit
fi


OSD_HOSTS=()
while getopts ":m:h:s:" option; do
  case $option in
    m  ) readonly MON_IP=$OPTARG ;;
    h  ) OSD_HOSTS+=("$OPTARG") ;;
    s  ) SCRIPT_FILE=$OPTARG ;;
    \? )
      usage
      common::err $(( ERR_STATUS_START + 1 )) "Invalid option: -$OPTARG"
      ;;
    :  )
      usage
      common::err $(( ERR_STATUS_START + 1 )) "Option -$OPTARG requires an argument."
      ;;
    *  )
      usage
      exit
  esac
done
shift "$(( OPTIND - 1 ))"

if [[ -z "${MON_IP:-}" ]]; then
  usage
  common::err $(( ERR_STATUS_START + 2 )) \
    "Please specify the IP address of the monitor using '-m'"
fi

if ! [[ "$MON_IP" =~ ^[[:digit:].]+$ ]]; then
  common::err $(( ERR_STATUS_START + 3 )) "Invalid IP address: $MON_IP"
fi

if (( "${#OSD_HOSTS[@]}" == 0 )); then
  usage
  common::err $(( ERR_STATUS_START + 2 )) "Please specify OSD_HOSTs using '-h'"
fi
common::debug OSD_HOSTS: "$(common::print_array OSD_HOSTS)"

if [[ -z "${SCRIPT_FILE:-}" ]]; then
  common::err $(( ERR_STATUS_START + 2 )) \
    "Please specify the SCRIPT_FILE using '-s'"
fi

if ! [[ -f "$SCRIPT_DIR/remote_script/ceph_volume/$SCRIPT_FILE" ]]; then
  common::err $(( ERR_STATUS_START + 3 )) \
    "Cannot find the script file '$SCRIPT_FILE' under the remote_script/ceph_volume dir"
fi

prepare_param_hosts() {
  local -n _param_hosts=$1
  local host

  for host in "${OSD_HOSTS[@]}"; do
    _param_hosts+=( --host "$host" )
  done
}


install_dependent_packages() {
  common::stage "Check and install dependent packages..."

  local -a param_hosts=( --host "$MON_IP" )
  prepare_param_hosts param_hosts

  "$SCRIPT_DIR"/parallel-docker-install \
    --env INFO_LEVEL=1 \
    "${param_hosts[@]}"

  CMD_MODE=false "$SCRIPT_DIR"/parallel-exec-script \
    --script=install/general_package \
    --env PACKAGE_NAME=sysstat \
    --env MIN_PACKAGE_VERSION=12.1.5 \
    --env INFO_LEVEL=1 \
    "${param_hosts[@]}"

  CMD_MODE=false "$SCRIPT_DIR"/parallel-exec-script \
    --script=install/general_package \
    --env PACKAGE_NAME=lshw \
    --env MIN_PACKAGE_VERSION=02.17-1 \
    --env INFO_LEVEL=1 \
    "${param_hosts[@]}"
}
install_dependent_packages

# shellcheck source=../remote_script/ceph_volume/interface.sh
. "$SCRIPT_DIR"/remote_script/ceph_volume/interface.sh
create_ceph_lvs() {
  common::stage "Create logical volumes on OSD hosts..."

  local -a param_hosts
  prepare_param_hosts param_hosts

  CMD_MODE=false "$SCRIPT_DIR"/parallel-exec-script \
    --script ceph_volume/"$SCRIPT_FILE" \
    --env INFO_LEVEL=1 \
    --env CEPH_VOLUME_OPERATION="$CEPH_VOLUME_OPERATION_CREATE" \
    "${param_hosts[@]}"
}
if [[ "${LVS_PROVISION:-yes}" != "no" ]]; then
  create_ceph_lvs
fi

query_ceph_lvs() {
  common::stage "Query logical volumes on OSD hosts..."

  CEPH_OSD_ID_TO_HOST=()

  local host host_lvs
  local -a host_id_to_host_repr

  common::create_stdout_dup
  for host in "${OSD_HOSTS[@]}"; do
    host_lvs=("$(
      "$SCRIPT_DIR"/parallel-exec-script \
        --script ceph_volume/"$SCRIPT_FILE" \
        --env CEPH_VOLUME_OPERATION="$CEPH_VOLUME_OPERATION_QUERY" \
        --host "$host" \
        2>&1 | common::to_stdout_dup | common::parse_remote_out
    )")

    local -i idx
    for (( idx = 0; idx < ${#host_lvs[@]}; idx++ )); do
      host_id_to_host_repr+=("osd.${#CEPH_OSD_ID_TO_HOST[@]} => $host")
      CEPH_OSD_ID_TO_HOST+=("$host")
    done
  done

  if (( ${#CEPH_OSD_ID_TO_HOST[@]} < 3 )); then
    common::err $(( ERR_STATUS_START + 4 )) \
      "Need >= 3 OSDs for the test" \
      "(current: $(common::print_array host_id_to_host_repr))"
  fi
  common::debug host_id_to_host_repr: \
    "$(common::print_array host_id_to_host_repr)"
}
query_ceph_lvs

dump_hardware_info() {
  OUTPUT_DIR="$PWD/output/num_osds=${#CEPH_OSD_ID_TO_HOST[@]}"
  mkdir --parents "$OUTPUT_DIR"

  common::stage "Dump hardware information from cluster hosts..."

  local -r hardware_info_dir="$OUTPUT_DIR"/lshw
  mkdir --parents "$hardware_info_dir"

  local -ar all_hosts=( "$MON_IP" "${OSD_HOSTS[@]}" )
  local host local_output_file

  for host in "${all_hosts[@]}"; do
    if [[ "$host" == "$MON_IP" ]]; then
      local_output_file="$hardware_info_dir"/mon.$host.html
    else
      local_output_file="$hardware_info_dir"/osd.$host.html
    fi

    trace_on
    ssh "${SSH_COMM_OPTIONS[@]}" "$USER@$host" \
      sudo lshw -html >"$local_output_file"
    trace_off
  done
}
dump_hardware_info


# shellcheck source=../remote_script/service/interface.sh
. "$SCRIPT_DIR"/remote_script/service/interface.sh
start_sys_activity_collection_service() {
  common::stage "Launch the system activity collection service on cluster hosts..."

  local -a param_hosts=( --host "$MON_IP" )
  prepare_param_hosts param_hosts

  trace_on
  CMD_MODE=false "$SCRIPT_DIR"/parallel-exec-script \
    --script service/sar \
    --env INFO_LEVEL=1 \
    --env SERVICE_OPERATION="$SERVICE_OPERATION_RESTART" \
    "${param_hosts[@]}"
  trace_off
}

stop_sys_activity_collection_service() {
  common::stage "Stop the system activity collection service and gather results..."
  INFO_LEVEL=1

  local -r relative_dirpath="$1"
  local -r benchmark_data_backup_dir="$OUTPUT_DIR"/benchmark/"$relative_dirpath"
  local -r sys_activity_backup_dir="$benchmark_data_backup_dir"/sys_activity
  mkdir --parents "$sys_activity_backup_dir"

  local -ar all_hosts=( "$MON_IP" "${OSD_HOSTS[@]}" )
  local host remote_output_file local_output_file

  common::create_stdout_dup
  for host in "${all_hosts[@]}"; do
    common::info "Query and stop service for $host"
    remote_output_file="$(
      "$SCRIPT_DIR"/parallel-exec-script \
        --script service/sar \
        --env INFO_LEVEL=2 \
        --env SERVICE_OPERATION="$SERVICE_OPERATION_QUERY" \
        --host "$host" \
        2>&1 | common::to_stdout_dup | common::parse_remote_out
    )"

    CMD_MODE=false "$SCRIPT_DIR"/parallel-exec-script \
      --script service/sar \
      --env INFO_LEVEL=2 \
      --env SERVICE_OPERATION="$SERVICE_OPERATION_STOP" \
      --host "$host"

    if [[ "$host" == "$MON_IP" ]]; then
      local_output_file="$sys_activity_backup_dir"/mon.$host.dat
    else
      local_output_file="$sys_activity_backup_dir"/osd.$host.dat
    fi

    common::info "Backing up system activity information from $host"
    trace_on
    scp -C "${SSH_COMM_OPTIONS[@]}" -p \
      "$host":"$remote_output_file" \
      "$local_output_file"
    trace_off
  done

  common::info "Backing up current environment variables"
  # https://askubuntu.com/a/275972
  trace_on
  ( set -o posix; set ) >"$benchmark_data_backup_dir"/env
  trace_off
}

drop_os_cache() {
  common::stage "Drop the OS cache from all cluster hosts..."

  local -a param_hosts=( --host "$MON_IP" )
  prepare_param_hosts param_hosts

  # https://www.kernel.org/doc/Documentation/sysctl/vm.txt
  "$SCRIPT_DIR"/parallel-ssh \
    "${param_hosts[@]}" \
    sudo bash \
    <<< "sync && echo 3 > /proc/sys/vm/drop_caches"
}

deploy_cluster() {
  local -a param_host_osds
  local host host_lvs joined

  for host in "${OSD_HOSTS[@]}"; do
    host_lvs=("$(
      "$SCRIPT_DIR"/parallel-exec-script \
        --script ceph_volume/"$SCRIPT_FILE" \
        --env CEPH_VOLUME_OPERATION="$CEPH_VOLUME_OPERATION_QUERY" \
        --host "$host" | common::parse_remote_out
    )")

    printf -v joined '%s,' "${host_lvs[@]}"
    param_host_osds+=( -o "$host:${joined%,}" )
  done

  local ceph_deploy_output
  common::create_stdout_dup
  ceph_deploy_output="$(
    "$SCRIPT_DIR"/ceph-deploy -m "$MON_IP" "${param_host_osds[@]}" \
      2>&1 | common::to_stdout_dup
  )"

  CEPH_CLUSTER_FSID="$(
    grep --perl-regexp --only-matching "Cluster fsid: \K[[:alnum:]-]+" \
      <<< "$ceph_deploy_output"
  )"

  CEPH_CLUSTER_KEYRING_PATH="$(
      grep --perl-regexp --only-matching "Wrote keyring to \K.+" \
      <<< "$ceph_deploy_output"
  )"
}

shell_cmd() {
  trace_off

  if [[ -z "${CEPH_CLUSTER_KEYRING_PATH}" ]]; then
    common::err $(( ERR_STATUS_START + 4 )) \
      "Fail to read the Ceph cluster's keyring file."
  fi

  echo "$SCRIPT_DIR/parallel-cephadm shell" \
    "--keyring $CEPH_CLUSTER_KEYRING_PATH" \
    "--"

  trace_resume
}

config_cluster() {
  local -ir num_rm=$1

  common::stage "Configure options for the cluster..."
  INFO_LEVEL=1

  local -ir osd_max_backfills=$(( (${#CEPH_OSD_ID_TO_HOST[@]} - num_rm) * 16 ))
  common::info "Set the number of backfill operations to/from an OSD" \
    "(osd_max_backfills) to $osd_max_backfills"
  trace_on
  $(shell_cmd) ceph config set osd osd_max_backfills $osd_max_backfills
  trace_off

  local -ir osd_recovery_max_active=32
  common::info "Set the number of active recovery requests per OSD at one time" \
    "to $osd_recovery_max_active"
  # https://github.com/ceph/ceph/blob/v17.0.0/doc/rados/configuration/osd-config-ref.rst
  trace_on
  $(shell_cmd) ceph config set osd osd_recovery_max_active \
    $osd_recovery_max_active
  trace_off

  local -ir osd_recovery_max_single_start=8
  common::info "Set the newly started recovery operations per OSD when" \
    "recovering to $osd_recovery_max_single_start"
  trace_on
  $(shell_cmd) ceph config set osd osd_recovery_max_single_start \
    $osd_recovery_max_single_start
  trace_off

  local -ir osd_recovery_op_priority=63
  common::info "Increase the priority for recovery operations to" \
    "$osd_recovery_op_priority (same as client operations)"
  trace_on
  $(shell_cmd) ceph config set osd osd_recovery_op_priority \
    $osd_recovery_op_priority
  trace_off

  common::info "Show OSD configuration options"
  trace_on
  $(shell_cmd) ceph config show-with-defaults osd.0 \
    | grep 'osd_max_backfills\|osd_recovery' | column -t -s ' '
  trace_off
}

load_cluster_data() {
  local -ir pool_size=$1 num_pgs=$2 object_size_bytes=$3

  local -r pool_name="bench_failure_management_resource_utilization"
  local -i data_size_per_osd
  data_size_per_osd=$(awk -v object_size_bytes="$object_size_bytes" '
      BEGIN { print int(log(object_size_bytes / 1024) / log(4)) * (1024 ^ 3) }
  ')
  # max is 10GB, min is 2GB
  local -ir data_size_per_osd_max=$(( 10 * ( 1024 ** 3 ) )) \
            data_size_per_osd_min=$(( 2 * ( 1024 ** 3 ) ))

  data_size_per_osd=$(( data_size_per_osd > data_size_per_osd_max \
    ? data_size_per_osd_max : data_size_per_osd ))
  data_size_per_osd=$(( data_size_per_osd < data_size_per_osd_min \
    ? data_size_per_osd_min : data_size_per_osd ))

  common::stage "Prepare data on the cluster..."

  INFO_LEVEL=1 common::info "Set pool attributes"

  INFO_LEVEL=2
  common::info "Create a replicated pool '$pool_name'" \
    "($pool_size replicas, $num_pgs placement groups)"
  trace_on
  $(shell_cmd) ceph osd pool create "$pool_name" "$num_pgs" "$num_pgs" \
    replicated --size "$pool_size" --pg-num-min "$num_pgs"
  trace_off

  common::info "Set the minimum number of replicas required for I/O to 1"
  trace_on
  $(shell_cmd) ceph osd pool set "$pool_name" min_size 1
  trace_off

  common::info "Disable placement groups autoscaling"
  trace_on
  $(shell_cmd) ceph osd pool set "$pool_name" pg_autoscale_mode off
  trace_off

  common::info "Set the application metadata key"
  trace_on
  $(shell_cmd) ceph osd pool application enable "$pool_name" benchmark
  trace_off

  common::info "Set the NOSCRUB flag"
  trace_on
  $(shell_cmd) ceph osd pool set "$pool_name" noscrub 1
  trace_off

  common::info "Set the NODEEP_SCRUB flag"
  trace_on
  $(shell_cmd) ceph osd pool set "$pool_name" nodeep-scrub 1
  trace_off

  common::info "Show details of pools"
  trace_on
  $(shell_cmd) ceph osd pool ls detail
  trace_off

  INFO_LEVEL=1 common::info "Loading objects" \
    "(total data size: $(( data_size_per_osd * ${#CEPH_OSD_ID_TO_HOST[@]} / 1024 / 1024 / 1024 )) GB)"
  # -b op_size
  # -O object_size
  #   https://github.com/ceph/ceph/blob/v17.0.0/src/tools/rados/rados.cc#L184
  trace_on
  $(shell_cmd) rados bench 99999999 write \
    --pool "$pool_name" \
    -b $(( object_size_bytes )) \
    -O $(( object_size_bytes )) \
    --max-objects $(( data_size_per_osd * ${#CEPH_OSD_ID_TO_HOST[@]} / object_size_bytes )) \
    --concurrent-ios $(( ${#CEPH_OSD_ID_TO_HOST[@]} * 256 )) \
    --show-time \
    --write-object \
    --write-omap \
    --write-xattr \
    --no-cleanup
  trace_off

  common::info "Sleep for 60 seconds"
  sleep 60

  common::info "Show OSD utilization"
  trace_on
  $(shell_cmd) ceph osd df tree
  trace_off

  common::info "Show cluster status"
  trace_on
  $(shell_cmd) ceph --status
  trace_off
}

rm_osds() {
  if [[ -z "${CEPH_CLUSTER_FSID:-}" ]]; then
    common::err $(( ERR_STATUS_START + 4 )) \
      "Unable to read the cluster fsid."
  fi

  local -ir num_rm=$1
  local -i idx

  common::stage "Removing $num_rm OSDs from the cluster"
  local confirm
  read -rp "Continue? (y/n): " confirm
  if ! [[ $confirm == [yY] || $confirm == [yY][eE][sS] ]]; then
    common::err 0 "User requests to terminate the program."
  fi

  for (( idx = 0; idx < ${#CEPH_OSD_ID_TO_HOST[@]}; idx++ )); do
    if (( num_rm == idx )); then
      break
    fi

    INFO_LEVEL=1
    common::info "Removing 'osd.$idx' from host '${CEPH_OSD_ID_TO_HOST[$idx]}'"
    "$SCRIPT_DIR"/parallel-cephadm \
      --host "${CEPH_OSD_ID_TO_HOST[$idx]}" \
      rm-daemon \
      --name osd."$idx" \
      --fsid "$CEPH_CLUSTER_FSID" \
      --force \
      --force-delete-data

    trace_on
    $(shell_cmd) ceph osd out osd."$idx"
    trace_off
  done

  common::info "Sleep for 10 seconds"
  sleep 10

  common::info "Show cluster status"
  trace_on
  $(shell_cmd) ceph --status
  trace_off
}

wait_for_data_recovery() {
  common::stage "Wait for data recovery to finish"
  INFO_LEVEL=1

  common::info "Query pool name"
  local -r pool_name="$($(shell_cmd) rados lspools | tail -n1)"
  common::debug pool_name: "$pool_name"

  common::info "Show pool stats"
  local pool_stats
  while true; do
    sleep 20

    pool_stats="$(
      $(shell_cmd) ceph osd pool stats "$pool_name" 2>&1 \
        | sed --quiet "/^pool $pool_name id /,\$p"
    )"
    INFO_LEVEL=2 common::info "$pool_stats"

    if grep "nothing is going on" <<< "$pool_stats" >/dev/null; then
      common::info "Data recovery has finished!!"
      break
    fi
  done
}

tear_down_cluster() {
  if [[ -z "${CEPH_CLUSTER_FSID:-}" ]]; then
    common::err $(( ERR_STATUS_START + 4 )) \
      "Unable to read the cluster fsid."
  fi

  local -a param_hosts=( --host "$MON_IP" )
  prepare_param_hosts param_hosts

  common::stage "Tearing down the Ceph cluster (fsid: $CEPH_CLUSTER_FSID)"
  "$SCRIPT_DIR"/parallel-cephadm "${param_hosts[@]}" \
    rm-cluster \
    --force \
    --fsid "$CEPH_CLUSTER_FSID"

  INFO_LEVEL=1 common::info "Sleep for 20 seconds"
  sleep 20
}

get_suggested_pg_count() {
  # Logic behind PG Count
  # See https://ceph.com/pgcalc/
  #     https://ceph.com/pgcalc_assets/pgcalc.js (function updatePGCount)
  #     https://docs.ceph.com/en/latest/rados/operations/placement-groups/#choosing-number-of-placement-groups
  awk -v pool_size="$1" -v num_osds="$2" -v target_pgs_per_osd="${3:-100}" '
    function nearestpower2(num) {
      tmp = 2 ^ int(log(num) / log(2))
      if (tmp < num * (1 - 0.25))
        tmp *= 2
      return tmp
    }

    BEGIN {
      min_value = nearestpower2(int(num_osds / pool_size) + 1)
      if (min_value < num_osds)
        min_value *= 2

      calc_value = nearestpower2(int(target_pgs_per_osd * num_osds / pool_size))

      print (min_value > calc_value ? min_value : calc_value)
    }
  '
}


bench_replicated_pool() {
  local -ar object_size_bytes_arr=(
    $(( 4 * 1024 ))
    $(( 16 * 1024 ))
    $(( 64 * 1024 ))
    $(( 256 * 1024 ))
    $(( 1024 * 1024 ))
    $(( 4096 * 1024 ))
    $(( 16384 * 1024 ))
  )

  local -i object_size_bytes
  for object_size_bytes in "${object_size_bytes_arr[@]}"; do

    local -a pool_size_arr
    IFS=' ' read -ra pool_size_arr <<< "$(seq -s ' ' 2 $((${#CEPH_OSD_ID_TO_HOST[@]} - 1)))"

    local -i pool_size suggested_pg_count
    for pool_size in "${pool_size_arr[@]}"; do
      suggested_pg_count=$(
        get_suggested_pg_count "$pool_size" ${#CEPH_OSD_ID_TO_HOST[@]})
      common::debug suggested_pg_count: "$suggested_pg_count" \
        "(pool_size=$pool_size, num_osds=${#CEPH_OSD_ID_TO_HOST[@]})"

      local -a num_pgs_arr=(
        $(( suggested_pg_count / 4 ))
        $(( suggested_pg_count / 2 ))
        "$suggested_pg_count"
        $(( suggested_pg_count * 2 ))
        $(( suggested_pg_count * 4 ))
      )

      # PG overdose protection
      #   https://ceph.io/community/new-luminous-pg-overdose-protection/
      # 250 is the default value of mon_max_pg_per_osd
      local -i idx
      for (( idx = ${#num_pgs_arr[@]} - 1; idx >=0; idx-- )); do
        if (( 250 * ${#CEPH_OSD_ID_TO_HOST[@]} / pool_size < ${num_pgs_arr[$idx]} )); then
          unset 'num_pgs_arr[idx]'
        else
          break
        fi
      done
      common::debug num_pgs_arr: "$(common::print_array num_pgs_arr)"

      local -i num_pgs
      for num_pgs in "${num_pgs_arr[@]}"; do

        local -a num_rm_osds_arr
        IFS=' ' read -ra num_rm_osds_arr <<< "$(seq -s ' ' 1 $((${#CEPH_OSD_ID_TO_HOST[@]} - pool_size)))"

        local -i num_rm_osds
        for num_rm_osds in "${num_rm_osds_arr[@]}"; do

          (( BENCHMARK_CUR_ROUND += 1 ))
          printf '\n\n\033[1;7;39;49m[%s][RUNNING][ROUND %d/%d] %s, %s, %s, %s\033[0m\n' \
            "$(common::now)" \
            "$BENCHMARK_CUR_ROUND" \
            $(( ${#object_size_bytes_arr[@]} * ${#num_pgs_arr[@]} * (((${#CEPH_OSD_ID_TO_HOST[@]} - 2) + 1) * (${#CEPH_OSD_ID_TO_HOST[@]} - 2) / 2) )) \
            "object_size_bytes=$object_size_bytes (${object_size_bytes_arr[0]}..${object_size_bytes_arr[-1]})" \
            "pool_size=$pool_size (${pool_size_arr[0]}..${pool_size_arr[-1]})" \
            "num_pgs=$num_pgs (${num_pgs_arr[0]}..${num_pgs_arr[-1]})" \
            "num_rm_osds=$num_rm_osds (${num_rm_osds_arr[0]}..${num_rm_osds_arr[-1]})"

          # TODO: Do we need to precondition the ceph volumes?

          start_sys_activity_collection_service

          deploy_cluster
          config_cluster "$num_rm_osds"

          drop_os_cache
          load_cluster_data "$pool_size" "$num_pgs" "$object_size_bytes"
          rm_osds "$num_rm_osds"
          wait_for_data_recovery

          tear_down_cluster

          stop_sys_activity_collection_service \
            "object_size_bytes=$object_size_bytes/pool_size=$pool_size/num_pgs=$num_pgs/num_rm_osds=$num_rm_osds"
        done
      done
    done
  done
}

bench_erasure_coded_pool() {
  common::err $(( ERR_STATUS_START + 999 )) \
    "Function not implemented yet!"
}

main() {
  BENCHMARK_CUR_ROUND=0

  bench_replicated_pool

  common::stage "All benchmarks have successfully completed."
}
main
